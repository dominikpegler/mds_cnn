{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit CNN in PyTorch\n",
    "===\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">Optimizations to do:<ul><li>Image augmentation: not applied on single images but on a batch for better performance.</li><li>Test newly implemented image augmentation methods: rotation range to 360Â°, shear, random resize crops more balanced, color, blur, contrast</li><li>Image augmentation: Discuss impact of augmentation on the individual MDS dimensions (Dimensions, \"lightness\", \"shape\", \"organisation\" (or a possible dimension \"size\") impaired by harsh crops, resizings, brightness?).</li><li>Learning rate: Discuss usage of lr scheduler (torch.optim.lr_scheduler)</li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device is cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from torchmetrics import R2Score\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import cv2\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from skimage import io, transform\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# number of models\n",
    "n_ensemble = 10\n",
    "\n",
    "# size of test and validation set\n",
    "n_test = 90\n",
    "\n",
    "# Number of dimensions in the dataset\n",
    "n_dim = 8\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size_im = 90\n",
    "batch_size_ft = 30\n",
    "\n",
    "# Number of epochs to train for \n",
    "n_epochs_im = 500\n",
    "n_epochs_ft = 500\n",
    "early_stop_tolerance = 40\n",
    "\n",
    "# where would we use these in pytorch? in keras they are used to create layers for the intermediate model\n",
    "dropout = 0.5 \n",
    "n_dense = 256\n",
    "n_layers = 2\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract_im = True\n",
    "feature_extract_ft = False\n",
    "\n",
    "input_size = 224\n",
    "\n",
    "loglr = -2.2200654426745987\n",
    "lr_im = 1 * 10 ** loglr\n",
    "lr_ft = 1.5e-3 # 2e-3 and 1.5e-3 best so far\n",
    "\n",
    "# create categories of stones for stratified train test split\n",
    "categories = [i for i in range(30) for j in range(12)] # creates 360 list items like so: [0, 0, 0, 0, ... 29, 29, 29, 29]\n",
    "\n",
    "IMG_360 = '../sanders_2018/360 Rocks/'\n",
    "IMG_120 = '../sanders_2018/120 Rocks/'\n",
    "MDS_360 = '../finetuning_torchvision_data/mds_360.csv'\n",
    "CHECKPOINTS = 'CNN_checkpoints/'\n",
    "PATH_IM = CHECKPOINTS + 'intermediate_model.pt'\n",
    "PATH_FT = CHECKPOINTS + 'finetuned_model.pt'\n",
    "\n",
    "\n",
    "print(\"Device is\", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=14, is_inception=False, early_stop_tolerance=20):\n",
    "    \"\"\"\n",
    "    handles the training and validation of a given model. At the end of\n",
    "    training returns the best performing model. After each epoch, the training and validation\n",
    "    accuracies are printed\n",
    "    \"\"\"\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    lrs = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = None\n",
    "    \n",
    "    r2score = R2Score(num_outputs=8).to(device)\n",
    "    \n",
    "    early_stopping = EarlyStopping(early_stop_tolerance)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {str(epoch + 1).rjust(len(str(num_epochs)))}/{num_epochs}', end=\": \")\n",
    "        #print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_r2 = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    score = r2score(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        # scheduler.step()\n",
    "                        lr = optimizer.param_groups[0][\"lr\"]\n",
    "                        lrs.append(lr)\n",
    "\n",
    "                # statistics \n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_r2 += score.item() * inputs.size(0)\n",
    "                \n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset) \n",
    "            epoch_acc = running_r2 / len(dataloaders[phase].dataset) \n",
    "            # print only if phase is validation\n",
    "            if phase == \"val\":\n",
    "                print(f'Loss = {epoch_loss: .4f}, Score = {epoch_acc: .4f}, lr = {lr: .4e}')\n",
    "                early_stopping(epoch_loss)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and best_acc == None:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val' and best_acc < epoch_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "\n",
    "        # early stopping if no improvement in the last 20 epochs\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopped.\")\n",
    "            break\n",
    "                \n",
    "\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history, lrs\n",
    "\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    \"\"\"\n",
    "    This helper function sets the ``.requires_grad`` attribute of the\n",
    "    parameters in the model to False when we are feature extracting.\n",
    "    \"\"\"\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    return None\n",
    "\n",
    "\n",
    "class RocksData(datasets.VisionDataset):\n",
    "    def __init__(self, df, root_dir, transform=None):\n",
    "        super(RocksData).__init__()\n",
    "        self.df = df\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, ix):\n",
    "        img_path = self.root_dir + \"/\" + self.df.iloc[ix,0]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = PIL.Image.fromarray(img)  \n",
    "        label = deepcopy(self.df.iloc[ix,1:].tolist())\n",
    "        label = torch.tensor(label).float()\n",
    "        img = self.transform(img)\n",
    "        return img.to(device), label\n",
    " \n",
    "    \n",
    "class PredictionData(datasets.VisionDataset): # TODO: lots of redundant code. integrate this into above RocksData dataset\n",
    "    def __init__(self, img_paths):\n",
    "        super(PredictionData).__init__()\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                             std=[0.229, 0.224, 0.225])\n",
    "        self.img_paths = img_paths\n",
    "    def __len__(self): return len(self.img_paths)\n",
    "    def __getitem__(self, ix):\n",
    "        img = cv2.imread(self.img_paths[ix])/255.\n",
    "        img = self.preprocess_input(img)\n",
    "        return img\n",
    "    def preprocess_input(self, img):\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        img = torch.tensor(img).permute(2,0,1)\n",
    "        img = self.normalize(img).float()\n",
    "        return img.to(device) \n",
    "    \n",
    "    \n",
    "def get_criterion(loss_name):\n",
    "    \"\"\"\n",
    "    Returns the optimizer\n",
    "    \"\"\"\n",
    "    if loss_name == \"L1\":\n",
    "        return torch.nn.L1Loss()\n",
    "    elif loss_name == \"L2\":\n",
    "        return torch.nn.MSELoss()\n",
    "    elif loss_name == \"smooth_L1\":\n",
    "        return torch.nn.SmoothL1Loss()\n",
    "    elif loss_name == \"huber\":\n",
    "        return torch.nn.HuberLoss()\n",
    "    else:\n",
    "        raise Exception(\"No valid loss_name entered!\")\n",
    "        \n",
    "\n",
    "def load_pretrained_model(stage=\"intermediate\", e=None, model_type=None):\n",
    "    \n",
    "    if (model_type == None) | (model_type == \"resnet\"):\n",
    "        model = models.resnet50()\n",
    "    elif model_type == \"regnet\":\n",
    "        model = models.regnet_y_16gf()\n",
    "    set_parameter_requires_grad(model, feature_extract_ft)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    new_layers = OutputLayers(n_layers, num_ftrs, n_dense, dropout)\n",
    "    model.fc = new_layers # replace last layer\n",
    "    \n",
    "    if model_type == None:\n",
    "        if e == None:\n",
    "            checkpoint = torch.load(CHECKPOINTS + f'{stage}_model.pt', map_location=device)\n",
    "        else:\n",
    "            checkpoint = torch.load(CHECKPOINTS + f'{stage}_model_{e}.pt', map_location=device)\n",
    "    else:\n",
    "        checkpoint = torch.load(CHECKPOINTS + f'{stage}_{model_type}.pt', map_location=device)\n",
    "    \n",
    "    model.load_state_dict(checkpoint)\n",
    "\n",
    "    # Send the model to GPU\n",
    "    return model.to(device)\n",
    "\n",
    "\n",
    "def predict(model, data_loader, unlabeled=False):\n",
    "    \"\"\"Computes predictions for a given mnodel and dataset\"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    outputs = list()\n",
    "    since = time.time()\n",
    "    with torch.no_grad():\n",
    "        if unlabeled:\n",
    "            for inputs in data_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                output = model(inputs)\n",
    "                outputs.append(output.to(\"cpu\").squeeze().numpy())\n",
    "        else:\n",
    "            for inputs, _ in data_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                output = model(inputs)\n",
    "                outputs.append(output.to(\"cpu\").squeeze().numpy())\n",
    "\n",
    "    return np.array(outputs)\n",
    "\n",
    "\n",
    "class EarlyStopping():\n",
    "    def __init__(self, tolerance=20):\n",
    "\n",
    "        self.tolerance = tolerance\n",
    "        self.counter = 0\n",
    "        self.early_stop = False\n",
    "        self.best_loss = np.nan\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if val_loss >= self.best_loss:\n",
    "            self.counter +=1\n",
    "            if self.counter >= self.tolerance:  \n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define image transforms\n",
    "\n",
    "<div class=\"alert alert-warning\">Optimizations to be done here. See dimensions organization and shape</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define image transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.RandomResizedCrop(input_size, scale=(1, 1), ratio=(.33, 2.5)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomAffine(degrees=360, translate=(0.2, 0.2), shear=(-16, 16, -16, 16), scale=(0.8, 1.4)), \n",
    "        transforms.ColorJitter(brightness=(.95, 1.05), contrast=(.9, 1.1), saturation=(0.95, 1.15)), \n",
    "        #transforms.GaussianBlur(kernel_size=(1, 1), sigma=(.01, .5)), # new\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),    \n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torchvision.transforms import InterpolationMode\n",
    "\n",
    "# define image transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "#        transforms.RandomResizedCrop(input_size, scale=(.7, 1.5), ratio=(.66, 5), interpolation=InterpolationMode.NEAREST),\n",
    "        transforms.RandomResizedCrop(input_size, scale=(.08, 1), ratio=(.75, 1.33), interpolation=InterpolationMode.BILINEAR),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomVerticalFlip(),\n",
    "        transforms.RandomAffine(degrees=360, translate=(.2, .2), fill=0, shear=(-16, 16, -16, 16), scale=None, interpolation=InterpolationMode.NEAREST),\n",
    "        transforms.ColorJitter(brightness=(.9, 1.10), contrast=(.85, 1.15), saturation=(0.95, 1.15)), # new\n",
    "        transforms.GaussianBlur(kernel_size=(1, 5), sigma=(.01, .5)), \n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),    \n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(MDS_360)\n",
    "\n",
    "\n",
    "# split data: train vs test\n",
    "(train, test,\n",
    "categories_train_, categories_test) = train_test_split(df,\n",
    "                                                       categories,\n",
    "                                                       test_size=n_test,\n",
    "                                                       stratify=categories,\n",
    "                                                       random_state=0)\n",
    "\n",
    "# split train set again: train vs validate\n",
    "train, val = train_test_split(train,\n",
    "                              test_size=n_test,\n",
    "                              stratify=categories_train_, \n",
    "                              random_state=0)\n",
    "\n",
    "# create image datasets\n",
    "train_dataset = RocksData(train.reset_index(drop=True), root_dir=IMG_360, transform=data_transforms[\"train\"])\n",
    "val_dataset = RocksData(val.reset_index(drop=True), root_dir=IMG_360, transform=data_transforms[\"val\"])\n",
    "test_dataset = RocksData(test.reset_index(drop=True), root_dir=IMG_360, transform=data_transforms[\"val\"])\n",
    "pred_dataset = PredictionData(glob(IMG_120+'*.jpg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adapt model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the final layers look like in Keras:\n",
    "\n",
    "```\n",
    "__________________________________________________________________________________________________\n",
    " Layer (type)                   Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "\n",
    "avg_pool (GlobalAveragePooling  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
    " 2D)                                                                                              \n",
    "                                                                                                  \n",
    " dropout (Dropout)              (None, 2048)         0           ['avg_pool[0][0]']               \n",
    "                                                                                                  \n",
    " dense (Dense)                  (None, 256)          524544      ['dropout[0][0]']                \n",
    "                                                                                                  \n",
    " batch_normalization (BatchNorm  (None, 256)         1024        ['dense[0][0]']                  \n",
    " alization)                                                                                       \n",
    "                                                                                                  \n",
    " dropout_1 (Dropout)            (None, 256)          0           ['batch_normalization[0][0]']    \n",
    "                                                                                                  \n",
    " dense_1 (Dense)                (None, 256)          65792       ['dropout_1[0][0]']              \n",
    "                                                                                                  \n",
    " batch_normalization_1 (BatchNo  (None, 256)         1024        ['dense_1[0][0]']                \n",
    " rmalization)                                                                                     \n",
    "                                                                                                  \n",
    " dropout_2 (Dropout)            (None, 256)          0           ['batch_normalization_1[0][0]']  \n",
    "                                                                                                  \n",
    " dense_2 (Dense)                (None, 8)            2056        ['dropout_2[0][0]']       \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "class OutputLayers(nn.Sequential):\n",
    "    def __init__(self, n_layers, num_ftrs, n_dense, dropout):\n",
    "        super().__init__(self.init_modules(n_layers, num_ftrs, n_dense, dropout))\n",
    "\n",
    "\n",
    "    def init_modules(self, n_layers, num_ftrs, n_dense, dropout):\n",
    "        modules = OrderedDict()\n",
    "        \n",
    "        i = 0\n",
    "        modules[f\"dropout_{i}\"] = nn.Dropout(p=dropout)\n",
    "        modules[f\"fc_{i}\"] = nn.Linear(num_ftrs, n_dense)\n",
    "        \n",
    "        for i in range(1, n_layers):           \n",
    "            modules[f\"relu_{i}\"] = nn.ReLU(inplace=True)\n",
    "            modules[f\"batchnorm_{i}\"] = nn.BatchNorm1d(n_dense)\n",
    "            modules[f\"dropout_{i}\"] = nn.Dropout(p=dropout)\n",
    "            modules[f\"fc_{i}\"] = nn.Linear(n_dense, n_dense)\n",
    "\n",
    "        modules[f\"relu_{i+1}\"] = nn.ReLU(inplace=True)\n",
    "        modules[f\"batchnorm_{i+1}\"] = nn.BatchNorm1d(n_dense)\n",
    "        modules[f\"dropout_{i+1}\"] = nn.Dropout(p=dropout)\n",
    "        modules[f\"fc_{i+1}\"] = nn.Linear(n_dense, n_dim)\n",
    "\n",
    "        return modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train one model ...\n",
    "... for testing purposes, comparing learning rates etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intermediate model"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "n_epochs_one = 500\n",
    "lr_one = 6.0247e-03\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model_type = \"resnet\" # \"resnet\"\n",
    "\n",
    "# Intermediate model\n",
    "\n",
    "if model_type == \"resnet\":\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "elif model_type == \"regnet\":\n",
    "    model = models.regnet_y_16gf(weights=models.RegNet_Y_16GF_Weights.IMAGENET1K_SWAG_E2E_V1)\n",
    "    \n",
    "set_parameter_requires_grad(model, feature_extract_im)\n",
    "num_ftrs = model.fc.in_features\n",
    "new_layers = OutputLayers(n_layers, num_ftrs, n_dense, dropout)\n",
    "model.fc = new_layers # replace last layer\n",
    "\n",
    "# Send the model to GPU\n",
    "model = model.to(device)\n",
    "\n",
    "# create dataloaders with specific batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size_ft)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size_ft)\n",
    "dataloaders_dict = {\"train\":train_loader,\"val\":val_loader}\n",
    "\n",
    "# Create Optimizer and define params to update\n",
    "params_to_update = model.parameters()\n",
    "if feature_extract_im:\n",
    "    params_to_update = []\n",
    "    for name,param in model.named_parameters():\n",
    "        if param.requires_grad == True:\n",
    "            params_to_update.append(param)\n",
    "\n",
    "\n",
    "# Instantiate optimizer for intermediate model\n",
    "optimizer = optim.Adam(params_to_update, lr = lr_one)\n",
    "\n",
    "# learning rate scheduler\n",
    "#scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.95, verbose=True)\n",
    "\n",
    "# Setup the loss fxn\n",
    "criterion = get_criterion(\"L2\")\n",
    "\n",
    "# Train and evaluate intermediate model\n",
    "model, hist, lrs = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=n_epochs_one)\n",
    "\n",
    "# Save intermediate model\n",
    "torch.save(model.state_dict(), CHECKPOINTS + f'state_dict_intermediate_{model_type}.pt')\n",
    "\n",
    "# Plot learning curve\n",
    "plt.plot(hist)\n",
    "plt.title(f'Intermediate model: {model_type}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changes 10.7. 10:00\n",
    "* back to old transforms configuration\n",
    "* lr 1.5e-3\n",
    "* num_early_stop = 40\n",
    "\n",
    "Changes 10.7. 9:32\n",
    "* lr 2e-3 instead of 1.5e-3\n",
    "* poor\n",
    "\n",
    "Changes 9.7. 22:00\n",
    "* RandomResizeCrop scale and ratio to default values (0.08, 1) aund (.75, 1.33)\n",
    "* RandomAffine scale to default value None\n",
    "* r 0.669\n",
    "* 4 wins\n",
    "* dimension 1 could be better\n",
    "\n",
    "Changes 9.7.\n",
    "* Shear from 20 to 16\n",
    "* scale=(.7, 1.5) instead of (1, 1) in RandomResizeCrop\n",
    "* r 0.664\n",
    "\n",
    "Changes 9.7.\n",
    "* Shear change from 24 to 20\n",
    "* r .669\n",
    "* not the best, dimension 1 could have been better\n",
    "\n",
    "Changes 8.7. 23:00\n",
    "* Shear increased from 16 to 24\n",
    "* RÂ²s 62, 73.4\n",
    "* r .662\n",
    "* not bad, but does not come too close to best results\n",
    "\n",
    "Changes 8.7. 22:00\n",
    "* adding gaussian blur again\n",
    "* overall not really better, maybe a little worse, but\n",
    "* seems more stable in the problematic dimensions shape and organization\n",
    "* worth further tests\n",
    "\n",
    "Changes 8.7. 21:19\n",
    "* interpolate method in randomaffine back to nearest\n",
    "* good again\n",
    "\n",
    "Changes 8.7. 20:42\n",
    "* Also set RandomAffine interpolate methode to bilinear\n",
    "* Results a little worse\n",
    "* 63.9, 73\n",
    "\n",
    "Changes 8.7. 19:32\n",
    "* set RandomResizeCrop to interpolate bilinear instead of nearest\n",
    "* results are close to the best\n",
    "\n",
    "Changes 8.7. 18:43\n",
    "* bring back scale in randomaffine\n",
    "* set back scale and ratio in randomresizecrop to previous\n",
    "* no real changes im 61, ft 72\n",
    "\n",
    "Changes 8.7., 18:10:\n",
    "* Shear set back from 9 to 16\n",
    "* better: im: 60, ft: 71\n",
    "\n",
    "Changes 8.7., 17:30\n",
    "* GaussianBlur removed\n",
    "* Poor: RÂ² intermediate: .56, fintetuned: .69\n",
    "\n",
    "Changes 8.7., 17:00\n",
    "* new augmention techniques\n",
    "  * RandomAffine: smaller shear, no scale, translate increased from .16 to .20\n",
    "  * GaussianBlur added again\n",
    "  * ColorJitter: all transforms slightly increased\n",
    "  * RandomResizeCrop: with scale, different ratio, different interpolation (nearest)\n",
    "* lr Adagrad set to 1.5e-3\n",
    "* NEW INTERMEDIATE models generated here\n",
    "* aborted after 1 model with RÂ²=.72 and r=.65\n",
    "\n",
    "\n",
    "optimal learning rate is betweend 1.5e-3 and 2e-3. best looking results so far with 2e-3, best mean correlation at 1.5e-3\n",
    "\n",
    "Changes 8.7., 10:00\n",
    "* lr Adagrad = 1.8e-3 instead of 1.5e-3\n",
    "* r=0.666998\n",
    "* fast\n",
    "\n",
    "Changes 8.7., 6:00\n",
    "* lr Adagrad = 1.5e-3 instead of 2e-3 => slightly better overall, dimension 1 not\n",
    "* r=0.667969\n",
    "\n",
    "Changes 7.7. 19:00\n",
    "* lr Adagrad = 5e-3 => poor, high loss\n",
    "* lr Adagrad = 5e-4 => poor results\n",
    "* lrAdagrad = 2e-3 => best results so far + very fast!\n",
    "\n",
    "Changes 7.7. 16:28:\n",
    "* lr Adagrad = 1e-4 instead of 1e-3\n",
    "=> poor results RÂ² = .67\n",
    "\n",
    "Changes 6.7.:\n",
    "* new INTERMEDIATE model for each finetuned model\n",
    "* no GaussianBlur\n",
    "* early stop at 25 instead of 20 epochs without loss reduction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/500: Loss =  6.6948, Score = -0.0127, lr =  6.0247e-03\n",
      "Epoch   2/500: Loss =  6.2931, Score =  0.0427, lr =  6.0247e-03\n",
      "Epoch   3/500: Loss =  5.6595, Score =  0.1300, lr =  6.0247e-03\n",
      "Epoch   4/500: Loss =  4.9903, Score =  0.2222, lr =  6.0247e-03\n",
      "Epoch   5/500: Loss =  4.4317, Score =  0.3014, lr =  6.0247e-03\n",
      "Epoch   6/500: Loss =  3.9978, Score =  0.3650, lr =  6.0247e-03\n",
      "Epoch   7/500: Loss =  3.7470, Score =  0.4038, lr =  6.0247e-03\n",
      "Epoch   8/500: Loss =  3.5949, Score =  0.4266, lr =  6.0247e-03\n",
      "Epoch   9/500: Loss =  3.5455, Score =  0.4333, lr =  6.0247e-03\n",
      "Epoch  10/500: Loss =  3.4917, Score =  0.4394, lr =  6.0247e-03\n",
      "Epoch  11/500: Loss =  3.3908, Score =  0.4503, lr =  6.0247e-03\n",
      "Epoch  12/500: Loss =  3.3266, Score =  0.4557, lr =  6.0247e-03\n",
      "Epoch  13/500: Loss =  3.2401, Score =  0.4680, lr =  6.0247e-03\n",
      "Epoch  14/500: Loss =  3.1522, Score =  0.4825, lr =  6.0247e-03\n",
      "Epoch  15/500: Loss =  3.0676, Score =  0.4948, lr =  6.0247e-03\n",
      "Epoch  16/500: Loss =  2.9661, Score =  0.5111, lr =  6.0247e-03\n",
      "Epoch  17/500: Loss =  2.8846, Score =  0.5246, lr =  6.0247e-03\n",
      "Epoch  18/500: Loss =  2.8375, Score =  0.5328, lr =  6.0247e-03\n",
      "Epoch  19/500: Loss =  2.8024, Score =  0.5378, lr =  6.0247e-03\n",
      "Epoch  20/500: Loss =  2.7689, Score =  0.5427, lr =  6.0247e-03\n",
      "Epoch  21/500: Loss =  2.7222, Score =  0.5507, lr =  6.0247e-03\n",
      "Epoch  22/500: Loss =  2.6772, Score =  0.5606, lr =  6.0247e-03\n",
      "Epoch  23/500: Loss =  2.6292, Score =  0.5705, lr =  6.0247e-03\n",
      "Epoch  24/500: Loss =  2.6335, Score =  0.5714, lr =  6.0247e-03\n",
      "Epoch  25/500: Loss =  2.6378, Score =  0.5704, lr =  6.0247e-03\n",
      "Epoch  26/500: Loss =  2.6380, Score =  0.5702, lr =  6.0247e-03\n",
      "Epoch  27/500: Loss =  2.6339, Score =  0.5700, lr =  6.0247e-03\n",
      "Epoch  28/500: Loss =  2.6216, Score =  0.5698, lr =  6.0247e-03\n",
      "Epoch  29/500: Loss =  2.6163, Score =  0.5689, lr =  6.0247e-03\n",
      "Epoch  30/500: Loss =  2.6111, Score =  0.5693, lr =  6.0247e-03\n",
      "Epoch  31/500: Loss =  2.5842, Score =  0.5736, lr =  6.0247e-03\n",
      "Epoch  32/500: Loss =  2.5700, Score =  0.5755, lr =  6.0247e-03\n",
      "Epoch  33/500: Loss =  2.5575, Score =  0.5772, lr =  6.0247e-03\n",
      "Epoch  34/500: Loss =  2.5176, Score =  0.5831, lr =  6.0247e-03\n",
      "Epoch  35/500: Loss =  2.4948, Score =  0.5870, lr =  6.0247e-03\n",
      "Epoch  36/500: Loss =  2.4862, Score =  0.5882, lr =  6.0247e-03\n",
      "Epoch  37/500: Loss =  2.4917, Score =  0.5870, lr =  6.0247e-03\n",
      "Epoch  38/500: Loss =  2.5062, Score =  0.5843, lr =  6.0247e-03\n",
      "Epoch  39/500: Loss =  2.5075, Score =  0.5847, lr =  6.0247e-03\n",
      "Epoch  40/500: Loss =  2.5270, Score =  0.5831, lr =  6.0247e-03\n",
      "Epoch  41/500: Loss =  2.5346, Score =  0.5832, lr =  6.0247e-03\n",
      "Epoch  42/500: Loss =  2.5646, Score =  0.5788, lr =  6.0247e-03\n",
      "Epoch  43/500: Loss =  2.5757, Score =  0.5765, lr =  6.0247e-03\n",
      "Epoch  44/500: Loss =  2.5767, Score =  0.5752, lr =  6.0247e-03\n",
      "Epoch  45/500: Loss =  2.5833, Score =  0.5725, lr =  6.0247e-03\n",
      "Epoch  46/500: Loss =  2.5916, Score =  0.5688, lr =  6.0247e-03\n",
      "Epoch  47/500: Loss =  2.6002, Score =  0.5659, lr =  6.0247e-03\n",
      "Epoch  48/500: Loss =  2.5789, Score =  0.5699, lr =  6.0247e-03\n",
      "Epoch  49/500: Loss =  2.5759, Score =  0.5711, lr =  6.0247e-03\n",
      "Epoch  50/500: Loss =  2.5366, Score =  0.5779, lr =  6.0247e-03\n",
      "Epoch  51/500: Loss =  2.5113, Score =  0.5824, lr =  6.0247e-03\n",
      "Epoch  52/500: Loss =  2.4649, Score =  0.5912, lr =  6.0247e-03\n",
      "Epoch  53/500: Loss =  2.4520, Score =  0.5938, lr =  6.0247e-03\n",
      "Epoch  54/500: Loss =  2.4277, Score =  0.5982, lr =  6.0247e-03\n",
      "Epoch  55/500: Loss =  2.4056, Score =  0.6019, lr =  6.0247e-03\n",
      "Epoch  56/500: Loss =  2.4060, Score =  0.6018, lr =  6.0247e-03\n",
      "Epoch  57/500: Loss =  2.4003, Score =  0.6021, lr =  6.0247e-03\n",
      "Epoch  58/500: Loss =  2.3929, Score =  0.6031, lr =  6.0247e-03\n",
      "Epoch  59/500: Loss =  2.3949, Score =  0.6026, lr =  6.0247e-03\n",
      "Epoch  60/500: Loss =  2.4072, Score =  0.6012, lr =  6.0247e-03\n",
      "Epoch  61/500: Loss =  2.3953, Score =  0.6044, lr =  6.0247e-03\n",
      "Epoch  62/500: Loss =  2.3872, Score =  0.6053, lr =  6.0247e-03\n",
      "Epoch  63/500: Loss =  2.3785, Score =  0.6058, lr =  6.0247e-03\n",
      "Epoch  64/500: Loss =  2.3757, Score =  0.6053, lr =  6.0247e-03\n",
      "Epoch  65/500: Loss =  2.3633, Score =  0.6058, lr =  6.0247e-03\n",
      "Epoch  66/500: Loss =  2.3753, Score =  0.6027, lr =  6.0247e-03\n",
      "Epoch  67/500: Loss =  2.3574, Score =  0.6051, lr =  6.0247e-03\n",
      "Epoch  68/500: Loss =  2.3491, Score =  0.6071, lr =  6.0247e-03\n",
      "Epoch  69/500: Loss =  2.3521, Score =  0.6079, lr =  6.0247e-03\n",
      "Epoch  70/500: Loss =  2.3531, Score =  0.6092, lr =  6.0247e-03\n",
      "Epoch  71/500: Loss =  2.3626, Score =  0.6090, lr =  6.0247e-03\n",
      "Epoch  72/500: Loss =  2.3633, Score =  0.6095, lr =  6.0247e-03\n",
      "Epoch  73/500: Loss =  2.3845, Score =  0.6067, lr =  6.0247e-03\n",
      "Epoch  74/500: Loss =  2.4305, Score =  0.5996, lr =  6.0247e-03\n",
      "Epoch  75/500: Loss =  2.4548, Score =  0.5959, lr =  6.0247e-03\n",
      "Epoch  76/500: Loss =  2.4534, Score =  0.5965, lr =  6.0247e-03\n",
      "Epoch  77/500: Loss =  2.4073, Score =  0.6038, lr =  6.0247e-03\n",
      "Epoch  78/500: Loss =  2.3988, Score =  0.6054, lr =  6.0247e-03\n",
      "Epoch  79/500: Loss =  2.3770, Score =  0.6086, lr =  6.0247e-03\n",
      "Epoch  80/500: Loss =  2.3622, Score =  0.6102, lr =  6.0247e-03\n",
      "Epoch  81/500: Loss =  2.3553, Score =  0.6104, lr =  6.0247e-03\n",
      "Epoch  82/500: Loss =  2.3671, Score =  0.6088, lr =  6.0247e-03\n",
      "Epoch  83/500: Loss =  2.3598, Score =  0.6101, lr =  6.0247e-03\n",
      "Epoch  84/500: Loss =  2.3556, Score =  0.6107, lr =  6.0247e-03\n",
      "Epoch  85/500: Loss =  2.3519, Score =  0.6115, lr =  6.0247e-03\n",
      "Epoch  86/500: Loss =  2.3444, Score =  0.6132, lr =  6.0247e-03\n",
      "Epoch  87/500: Loss =  2.3318, Score =  0.6157, lr =  6.0247e-03\n",
      "Epoch  88/500: Loss =  2.3401, Score =  0.6147, lr =  6.0247e-03\n",
      "Epoch  89/500: Loss =  2.3418, Score =  0.6141, lr =  6.0247e-03\n",
      "Epoch  90/500: Loss =  2.3429, Score =  0.6131, lr =  6.0247e-03\n",
      "Epoch  91/500: Loss =  2.3262, Score =  0.6154, lr =  6.0247e-03\n",
      "Epoch  92/500: Loss =  2.3291, Score =  0.6152, lr =  6.0247e-03\n",
      "Epoch  93/500: Loss =  2.3262, Score =  0.6154, lr =  6.0247e-03\n",
      "Epoch  94/500: Loss =  2.3116, Score =  0.6176, lr =  6.0247e-03\n",
      "Epoch  95/500: Loss =  2.3210, Score =  0.6165, lr =  6.0247e-03\n",
      "Epoch  96/500: Loss =  2.3326, Score =  0.6146, lr =  6.0247e-03\n",
      "Epoch  97/500: Loss =  2.3528, Score =  0.6108, lr =  6.0247e-03\n",
      "Epoch  98/500: Loss =  2.3529, Score =  0.6105, lr =  6.0247e-03\n",
      "Epoch  99/500: Loss =  2.3475, Score =  0.6106, lr =  6.0247e-03\n",
      "Epoch 100/500: Loss =  2.3290, Score =  0.6118, lr =  6.0247e-03\n",
      "Epoch 101/500: Loss =  2.2923, Score =  0.6165, lr =  6.0247e-03\n",
      "Epoch 102/500: Loss =  2.2847, Score =  0.6171, lr =  6.0247e-03\n",
      "Epoch 103/500: Loss =  2.2937, Score =  0.6155, lr =  6.0247e-03\n",
      "Epoch 104/500: Loss =  2.3207, Score =  0.6107, lr =  6.0247e-03\n",
      "Epoch 105/500: Loss =  2.3455, Score =  0.6068, lr =  6.0247e-03\n",
      "Epoch 106/500: Loss =  2.3740, Score =  0.6026, lr =  6.0247e-03\n",
      "Epoch 107/500: Loss =  2.3737, Score =  0.6035, lr =  6.0247e-03\n",
      "Epoch 108/500: Loss =  2.3712, Score =  0.6042, lr =  6.0247e-03\n",
      "Epoch 109/500: Loss =  2.3494, Score =  0.6078, lr =  6.0247e-03\n",
      "Epoch 110/500: Loss =  2.3415, Score =  0.6093, lr =  6.0247e-03\n",
      "Epoch 111/500: Loss =  2.3502, Score =  0.6079, lr =  6.0247e-03\n",
      "Epoch 112/500: Loss =  2.3338, Score =  0.6103, lr =  6.0247e-03\n",
      "Epoch 113/500: Loss =  2.3101, Score =  0.6137, lr =  6.0247e-03\n",
      "Epoch 114/500: Loss =  2.3292, Score =  0.6103, lr =  6.0247e-03\n",
      "Epoch 115/500: Loss =  2.3317, Score =  0.6101, lr =  6.0247e-03\n",
      "Epoch 116/500: Loss =  2.3250, Score =  0.6118, lr =  6.0247e-03\n",
      "Epoch 117/500: Loss =  2.3110, Score =  0.6142, lr =  6.0247e-03\n",
      "Epoch 118/500: Loss =  2.3110, Score =  0.6137, lr =  6.0247e-03\n",
      "Epoch 119/500: Loss =  2.3070, Score =  0.6150, lr =  6.0247e-03\n",
      "Epoch 120/500: Loss =  2.3161, Score =  0.6149, lr =  6.0247e-03\n",
      "Epoch 121/500: Loss =  2.3297, Score =  0.6139, lr =  6.0247e-03\n",
      "Epoch 122/500: Loss =  2.3394, Score =  0.6129, lr =  6.0247e-03\n",
      "Epoch 123/500: Loss =  2.3318, Score =  0.6138, lr =  6.0247e-03\n",
      "Epoch 124/500: Loss =  2.3233, Score =  0.6149, lr =  6.0247e-03\n",
      "Epoch 125/500: Loss =  2.3040, Score =  0.6179, lr =  6.0247e-03\n",
      "Epoch 126/500: Loss =  2.2965, Score =  0.6192, lr =  6.0247e-03\n",
      "Epoch 127/500: Loss =  2.2945, Score =  0.6194, lr =  6.0247e-03\n",
      "Epoch 128/500: Loss =  2.2960, Score =  0.6187, lr =  6.0247e-03\n",
      "Epoch 129/500: Loss =  2.2836, Score =  0.6208, lr =  6.0247e-03\n",
      "Epoch 130/500: Loss =  2.2839, Score =  0.6205, lr =  6.0247e-03\n",
      "Epoch 131/500: Loss =  2.2781, Score =  0.6222, lr =  6.0247e-03\n",
      "Epoch 132/500: Loss =  2.2828, Score =  0.6222, lr =  6.0247e-03\n",
      "Epoch 133/500: Loss =  2.2790, Score =  0.6231, lr =  6.0247e-03\n",
      "Epoch 134/500: Loss =  2.2874, Score =  0.6207, lr =  6.0247e-03\n",
      "Epoch 135/500: Loss =  2.2774, Score =  0.6216, lr =  6.0247e-03\n",
      "Epoch 136/500: Loss =  2.2863, Score =  0.6196, lr =  6.0247e-03\n",
      "Epoch 137/500: Loss =  2.3057, Score =  0.6160, lr =  6.0247e-03\n",
      "Epoch 138/500: Loss =  2.3342, Score =  0.6112, lr =  6.0247e-03\n",
      "Epoch 139/500: Loss =  2.3414, Score =  0.6108, lr =  6.0247e-03\n",
      "Epoch 140/500: Loss =  2.3496, Score =  0.6098, lr =  6.0247e-03\n",
      "Epoch 141/500: Loss =  2.3670, Score =  0.6076, lr =  6.0247e-03\n",
      "Epoch 142/500: Loss =  2.3691, Score =  0.6083, lr =  6.0247e-03\n",
      "Epoch 143/500: Loss =  2.3796, Score =  0.6075, lr =  6.0247e-03\n",
      "Epoch 144/500: Loss =  2.3806, Score =  0.6070, lr =  6.0247e-03\n",
      "Epoch 145/500: Loss =  2.3614, Score =  0.6101, lr =  6.0247e-03\n",
      "Epoch 146/500: Loss =  2.3690, Score =  0.6087, lr =  6.0247e-03\n",
      "Epoch 147/500: Loss =  2.3871, Score =  0.6053, lr =  6.0247e-03\n",
      "Epoch 148/500: Loss =  2.3903, Score =  0.6043, lr =  6.0247e-03\n",
      "Epoch 149/500: Loss =  2.4096, Score =  0.6007, lr =  6.0247e-03\n",
      "Epoch 150/500: Loss =  2.3864, Score =  0.6038, lr =  6.0247e-03\n",
      "Epoch 151/500: Loss =  2.3642, Score =  0.6075, lr =  6.0247e-03\n",
      "Epoch 152/500: Loss =  2.3592, Score =  0.6085, lr =  6.0247e-03\n",
      "Epoch 153/500: Loss =  2.3501, Score =  0.6099, lr =  6.0247e-03\n",
      "Epoch 154/500: Loss =  2.3557, Score =  0.6091, lr =  6.0247e-03\n",
      "Epoch 155/500: Loss =  2.3830, Score =  0.6047, lr =  6.0247e-03\n",
      "Epoch 156/500: Loss =  2.3867, Score =  0.6038, lr =  6.0247e-03\n",
      "Epoch 157/500: Loss =  2.3661, Score =  0.6065, lr =  6.0247e-03\n",
      "Epoch 158/500: Loss =  2.3634, Score =  0.6064, lr =  6.0247e-03\n",
      "Epoch 159/500: Loss =  2.3551, Score =  0.6077, lr =  6.0247e-03\n",
      "Epoch 160/500: Loss =  2.3371, Score =  0.6112, lr =  6.0247e-03\n",
      "Epoch 161/500: Loss =  2.3233, Score =  0.6139, lr =  6.0247e-03\n",
      "Epoch 162/500: Loss =  2.3121, Score =  0.6158, lr =  6.0247e-03\n",
      "Epoch 163/500: Loss =  2.3006, Score =  0.6175, lr =  6.0247e-03\n",
      "Epoch 164/500: Loss =  2.3062, Score =  0.6164, lr =  6.0247e-03\n",
      "Epoch 165/500: Loss =  2.3042, Score =  0.6165, lr =  6.0247e-03\n",
      "Epoch 166/500: Loss =  2.2860, Score =  0.6200, lr =  6.0247e-03\n",
      "Epoch 167/500: Loss =  2.2638, Score =  0.6246, lr =  6.0247e-03\n",
      "Epoch 168/500: Loss =  2.2527, Score =  0.6271, lr =  6.0247e-03\n",
      "Epoch 169/500: Loss =  2.2646, Score =  0.6255, lr =  6.0247e-03\n",
      "Epoch 170/500: Loss =  2.2615, Score =  0.6266, lr =  6.0247e-03\n",
      "Epoch 171/500: Loss =  2.2664, Score =  0.6262, lr =  6.0247e-03\n",
      "Epoch 172/500: Loss =  2.2654, Score =  0.6264, lr =  6.0247e-03\n",
      "Epoch 173/500: Loss =  2.2729, Score =  0.6249, lr =  6.0247e-03\n",
      "Epoch 174/500: Loss =  2.2898, Score =  0.6219, lr =  6.0247e-03\n",
      "Epoch 175/500: Loss =  2.3055, Score =  0.6192, lr =  6.0247e-03\n",
      "Epoch 176/500: Loss =  2.3190, Score =  0.6167, lr =  6.0247e-03\n",
      "Epoch 177/500: Loss =  2.3044, Score =  0.6191, lr =  6.0247e-03\n",
      "Epoch 178/500: Loss =  2.2925, Score =  0.6215, lr =  6.0247e-03\n",
      "Epoch 179/500: Loss =  2.2883, Score =  0.6221, lr =  6.0247e-03\n",
      "Epoch 180/500: Loss =  2.2872, Score =  0.6222, lr =  6.0247e-03\n",
      "Epoch 181/500: Loss =  2.2665, Score =  0.6254, lr =  6.0247e-03\n",
      "Epoch 182/500: Loss =  2.2641, Score =  0.6255, lr =  6.0247e-03\n",
      "Epoch 183/500: Loss =  2.2507, Score =  0.6277, lr =  6.0247e-03\n",
      "Epoch 184/500: Loss =  2.2545, Score =  0.6265, lr =  6.0247e-03\n",
      "Epoch 185/500: Loss =  2.2691, Score =  0.6239, lr =  6.0247e-03\n",
      "Epoch 186/500: Loss =  2.3010, Score =  0.6191, lr =  6.0247e-03\n",
      "Epoch 187/500: Loss =  2.3255, Score =  0.6155, lr =  6.0247e-03\n",
      "Epoch 188/500: Loss =  2.3401, Score =  0.6133, lr =  6.0247e-03\n",
      "Epoch 189/500: Loss =  2.3611, Score =  0.6109, lr =  6.0247e-03\n",
      "Epoch 190/500: Loss =  2.3695, Score =  0.6102, lr =  6.0247e-03\n",
      "Epoch 191/500: Loss =  2.3619, Score =  0.6120, lr =  6.0247e-03\n",
      "Epoch 192/500: Loss =  2.3562, Score =  0.6137, lr =  6.0247e-03\n",
      "Epoch 193/500: Loss =  2.3648, Score =  0.6127, lr =  6.0247e-03\n",
      "Epoch 194/500: Loss =  2.3722, Score =  0.6118, lr =  6.0247e-03\n",
      "Epoch 195/500: Loss =  2.3563, Score =  0.6140, lr =  6.0247e-03\n",
      "Epoch 196/500: Loss =  2.3539, Score =  0.6142, lr =  6.0247e-03\n",
      "Epoch 197/500: Loss =  2.3351, Score =  0.6168, lr =  6.0247e-03\n",
      "Epoch 198/500: Loss =  2.3160, Score =  0.6187, lr =  6.0247e-03\n",
      "Epoch 199/500: Loss =  2.3076, Score =  0.6195, lr =  6.0247e-03\n",
      "Epoch 200/500: Loss =  2.2910, Score =  0.6221, lr =  6.0247e-03\n",
      "Epoch 201/500: Loss =  2.2852, Score =  0.6226, lr =  6.0247e-03\n",
      "Epoch 202/500: Loss =  2.3007, Score =  0.6202, lr =  6.0247e-03\n",
      "Epoch 203/500: Loss =  2.3483, Score =  0.6124, lr =  6.0247e-03\n",
      "Epoch 204/500: Loss =  2.3724, Score =  0.6087, lr =  6.0247e-03\n",
      "Epoch 205/500: Loss =  2.3941, Score =  0.6048, lr =  6.0247e-03\n",
      "Epoch 206/500: Loss =  2.3868, Score =  0.6057, lr =  6.0247e-03\n",
      "Epoch 207/500: Loss =  2.3652, Score =  0.6092, lr =  6.0247e-03\n",
      "Epoch 208/500: Loss =  2.3607, Score =  0.6096, lr =  6.0247e-03\n",
      "Epoch 209/500: Loss =  2.3478, Score =  0.6121, lr =  6.0247e-03\n",
      "Epoch 210/500: Loss =  2.3463, Score =  0.6131, lr =  6.0247e-03\n",
      "Epoch 211/500: Loss =  2.3393, Score =  0.6144, lr =  6.0247e-03\n",
      "Epoch 212/500: Loss =  2.3477, Score =  0.6131, lr =  6.0247e-03\n",
      "Epoch 213/500: Loss =  2.3433, Score =  0.6141, lr =  6.0247e-03\n",
      "Epoch 214/500: Loss =  2.3277, Score =  0.6174, lr =  6.0247e-03\n",
      "Epoch 215/500: Loss =  2.3253, Score =  0.6181, lr =  6.0247e-03\n",
      "Epoch 216/500: Loss =  2.3378, Score =  0.6167, lr =  6.0247e-03\n",
      "Epoch 217/500: Loss =  2.3653, Score =  0.6130, lr =  6.0247e-03\n",
      "Epoch 218/500: Loss =  2.3758, Score =  0.6116, lr =  6.0247e-03\n",
      "Epoch 219/500: Loss =  2.3813, Score =  0.6113, lr =  6.0247e-03\n",
      "Epoch 220/500: Loss =  2.3725, Score =  0.6118, lr =  6.0247e-03\n",
      "Epoch 221/500: Loss =  2.3760, Score =  0.6109, lr =  6.0247e-03\n",
      "Epoch 222/500: Loss =  2.3569, Score =  0.6130, lr =  6.0247e-03\n",
      "Epoch 223/500: Loss =  2.3236, Score =  0.6173, lr =  6.0247e-03\n",
      "Early stopped.\n",
      "Training complete in 26m 46s\n",
      "Best val Acc: 0.627685\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/500: Loss =  2.7331, Score =  0.5596, lr =  1.5000e-03\n",
      "Epoch   2/500: Loss =  2.2146, Score =  0.6268, lr =  1.5000e-03\n",
      "Epoch   3/500: Loss =  1.8753, Score =  0.6772, lr =  1.5000e-03\n",
      "Epoch   4/500: Loss =  1.8404, Score =  0.6870, lr =  1.5000e-03\n",
      "Epoch   5/500: Loss =  1.7613, Score =  0.6989, lr =  1.5000e-03\n",
      "Epoch   6/500: Loss =  1.8185, Score =  0.6893, lr =  1.5000e-03\n",
      "Epoch   7/500: Loss =  1.5962, Score =  0.7271, lr =  1.5000e-03\n",
      "Epoch   8/500: Loss =  1.6094, Score =  0.7258, lr =  1.5000e-03\n",
      "Epoch   9/500: Loss =  1.6222, Score =  0.7215, lr =  1.5000e-03\n",
      "Epoch  10/500: Loss =  1.6477, Score =  0.7154, lr =  1.5000e-03\n",
      "Epoch  11/500: Loss =  1.6429, Score =  0.7136, lr =  1.5000e-03\n",
      "Epoch  12/500: Loss =  1.8006, Score =  0.6866, lr =  1.5000e-03\n",
      "Epoch  13/500: Loss =  1.6831, Score =  0.7078, lr =  1.5000e-03\n",
      "Epoch  14/500: Loss =  1.6437, Score =  0.7128, lr =  1.5000e-03\n",
      "Epoch  15/500: Loss =  1.6095, Score =  0.7214, lr =  1.5000e-03\n",
      "Epoch  16/500: Loss =  1.5690, Score =  0.7268, lr =  1.5000e-03\n",
      "Epoch  17/500: Loss =  1.6160, Score =  0.7203, lr =  1.5000e-03\n",
      "Epoch  18/500: Loss =  1.6029, Score =  0.7233, lr =  1.5000e-03\n",
      "Epoch  19/500: Loss =  1.6063, Score =  0.7222, lr =  1.5000e-03\n",
      "Epoch  20/500: Loss =  1.6122, Score =  0.7183, lr =  1.5000e-03\n",
      "Epoch  21/500: Loss =  1.5645, Score =  0.7269, lr =  1.5000e-03\n",
      "Epoch  22/500: Loss =  1.5798, Score =  0.7260, lr =  1.5000e-03\n",
      "Epoch  23/500: Loss =  1.5634, Score =  0.7263, lr =  1.5000e-03\n",
      "Epoch  24/500: Loss =  1.5699, Score =  0.7285, lr =  1.5000e-03\n",
      "Epoch  25/500: Loss =  1.5320, Score =  0.7329, lr =  1.5000e-03\n",
      "Epoch  26/500: Loss =  1.5367, Score =  0.7310, lr =  1.5000e-03\n",
      "Epoch  27/500: Loss =  1.5501, Score =  0.7295, lr =  1.5000e-03\n",
      "Epoch  28/500: Loss =  1.5415, Score =  0.7301, lr =  1.5000e-03\n",
      "Epoch  29/500: Loss =  1.5612, Score =  0.7272, lr =  1.5000e-03\n",
      "Epoch  30/500: Loss =  1.5728, Score =  0.7266, lr =  1.5000e-03\n",
      "Epoch  31/500: Loss =  1.5832, Score =  0.7234, lr =  1.5000e-03\n",
      "Epoch  32/500: Loss =  1.5517, Score =  0.7298, lr =  1.5000e-03\n",
      "Epoch  33/500: Loss =  1.5406, Score =  0.7294, lr =  1.5000e-03\n",
      "Epoch  34/500: Loss =  1.5451, Score =  0.7284, lr =  1.5000e-03\n",
      "Epoch  35/500: Loss =  1.5281, Score =  0.7300, lr =  1.5000e-03\n",
      "Epoch  36/500: Loss =  1.5439, Score =  0.7290, lr =  1.5000e-03\n",
      "Epoch  37/500: Loss =  1.5395, Score =  0.7295, lr =  1.5000e-03\n",
      "Epoch  38/500: Loss =  1.5064, Score =  0.7356, lr =  1.5000e-03\n",
      "Epoch  39/500: Loss =  1.5312, Score =  0.7337, lr =  1.5000e-03\n",
      "Epoch  40/500: Loss =  1.5514, Score =  0.7295, lr =  1.5000e-03\n",
      "Epoch  41/500: Loss =  1.5539, Score =  0.7281, lr =  1.5000e-03\n",
      "Epoch  42/500: Loss =  1.5322, Score =  0.7302, lr =  1.5000e-03\n",
      "Epoch  43/500: Loss =  1.5366, Score =  0.7299, lr =  1.5000e-03\n",
      "Epoch  44/500: Loss =  1.5238, Score =  0.7316, lr =  1.5000e-03\n",
      "Epoch  45/500: Loss =  1.5362, Score =  0.7286, lr =  1.5000e-03\n",
      "Epoch  46/500: Loss =  1.5355, Score =  0.7300, lr =  1.5000e-03\n",
      "Epoch  47/500: Loss =  1.5126, Score =  0.7336, lr =  1.5000e-03\n",
      "Epoch  48/500: Loss =  1.4912, Score =  0.7369, lr =  1.5000e-03\n",
      "Epoch  49/500: Loss =  1.4989, Score =  0.7365, lr =  1.5000e-03\n",
      "Epoch  50/500: Loss =  1.5295, Score =  0.7322, lr =  1.5000e-03\n",
      "Epoch  51/500: Loss =  1.5147, Score =  0.7328, lr =  1.5000e-03\n",
      "Epoch  52/500: Loss =  1.5117, Score =  0.7333, lr =  1.5000e-03\n",
      "Epoch  53/500: Loss =  1.5287, Score =  0.7316, lr =  1.5000e-03\n",
      "Epoch  54/500: Loss =  1.5214, Score =  0.7331, lr =  1.5000e-03\n",
      "Epoch  55/500: Loss =  1.5090, Score =  0.7336, lr =  1.5000e-03\n",
      "Epoch  56/500: Loss =  1.5203, Score =  0.7317, lr =  1.5000e-03\n",
      "Epoch  57/500: Loss =  1.5633, Score =  0.7243, lr =  1.5000e-03\n",
      "Epoch  58/500: Loss =  1.5643, Score =  0.7226, lr =  1.5000e-03\n",
      "Epoch  59/500: Loss =  1.5472, Score =  0.7246, lr =  1.5000e-03\n",
      "Epoch  60/500: Loss =  1.5587, Score =  0.7246, lr =  1.5000e-03\n",
      "Epoch  61/500: Loss =  1.5177, Score =  0.7302, lr =  1.5000e-03\n",
      "Epoch  62/500: Loss =  1.5139, Score =  0.7304, lr =  1.5000e-03\n",
      "Epoch  63/500: Loss =  1.5081, Score =  0.7316, lr =  1.5000e-03\n",
      "Epoch  64/500: Loss =  1.5002, Score =  0.7313, lr =  1.5000e-03\n",
      "Epoch  65/500: Loss =  1.5077, Score =  0.7318, lr =  1.5000e-03\n",
      "Epoch  66/500: Loss =  1.5039, Score =  0.7341, lr =  1.5000e-03\n",
      "Epoch  67/500: Loss =  1.4865, Score =  0.7358, lr =  1.5000e-03\n",
      "Epoch  68/500: Loss =  1.5013, Score =  0.7329, lr =  1.5000e-03\n",
      "Epoch  69/500: Loss =  1.4850, Score =  0.7357, lr =  1.5000e-03\n",
      "Epoch  70/500: Loss =  1.4943, Score =  0.7343, lr =  1.5000e-03\n",
      "Epoch  71/500: Loss =  1.4947, Score =  0.7356, lr =  1.5000e-03\n",
      "Epoch  72/500: Loss =  1.4849, Score =  0.7363, lr =  1.5000e-03\n",
      "Epoch  73/500: Loss =  1.4805, Score =  0.7360, lr =  1.5000e-03\n",
      "Epoch  74/500: Loss =  1.5067, Score =  0.7318, lr =  1.5000e-03\n",
      "Epoch  75/500: Loss =  1.5350, Score =  0.7279, lr =  1.5000e-03\n",
      "Epoch  76/500: Loss =  1.5302, Score =  0.7288, lr =  1.5000e-03\n",
      "Epoch  77/500: Loss =  1.5033, Score =  0.7318, lr =  1.5000e-03\n",
      "Epoch  78/500: Loss =  1.5054, Score =  0.7317, lr =  1.5000e-03\n",
      "Epoch  79/500: Loss =  1.5169, Score =  0.7300, lr =  1.5000e-03\n",
      "Epoch  80/500: Loss =  1.4918, Score =  0.7347, lr =  1.5000e-03\n",
      "Epoch  81/500: Loss =  1.4985, Score =  0.7338, lr =  1.5000e-03\n",
      "Epoch  82/500: Loss =  1.5071, Score =  0.7320, lr =  1.5000e-03\n",
      "Epoch  83/500: Loss =  1.5179, Score =  0.7315, lr =  1.5000e-03\n",
      "Epoch  84/500: Loss =  1.5281, Score =  0.7295, lr =  1.5000e-03\n",
      "Epoch  85/500: Loss =  1.5205, Score =  0.7301, lr =  1.5000e-03\n",
      "Epoch  86/500: Loss =  1.5136, Score =  0.7321, lr =  1.5000e-03\n",
      "Epoch  87/500: Loss =  1.5147, Score =  0.7330, lr =  1.5000e-03\n",
      "Epoch  88/500: Loss =  1.5276, Score =  0.7311, lr =  1.5000e-03\n",
      "Epoch  89/500: Loss =  1.5165, Score =  0.7322, lr =  1.5000e-03\n",
      "Epoch  90/500: Loss =  1.5143, Score =  0.7319, lr =  1.5000e-03\n",
      "Epoch  91/500: Loss =  1.5185, Score =  0.7306, lr =  1.5000e-03\n",
      "Epoch  92/500: Loss =  1.5038, Score =  0.7328, lr =  1.5000e-03\n",
      "Epoch  93/500: Loss =  1.5027, Score =  0.7337, lr =  1.5000e-03\n",
      "Epoch  94/500: Loss =  1.5123, Score =  0.7319, lr =  1.5000e-03\n",
      "Epoch  95/500: Loss =  1.4962, Score =  0.7344, lr =  1.5000e-03\n",
      "Epoch  96/500: Loss =  1.4958, Score =  0.7337, lr =  1.5000e-03\n",
      "Epoch  97/500: Loss =  1.4837, Score =  0.7360, lr =  1.5000e-03\n",
      "Epoch  98/500: Loss =  1.4917, Score =  0.7349, lr =  1.5000e-03\n",
      "Epoch  99/500: Loss =  1.4805, Score =  0.7359, lr =  1.5000e-03\n",
      "Epoch 100/500: Loss =  1.4854, Score =  0.7362, lr =  1.5000e-03\n",
      "Epoch 101/500: Loss =  1.4801, Score =  0.7371, lr =  1.5000e-03\n",
      "Epoch 102/500: Loss =  1.4849, Score =  0.7363, lr =  1.5000e-03\n",
      "Epoch 103/500: Loss =  1.4741, Score =  0.7373, lr =  1.5000e-03\n",
      "Epoch 104/500: Loss =  1.4876, Score =  0.7339, lr =  1.5000e-03\n",
      "Epoch 105/500: Loss =  1.4934, Score =  0.7342, lr =  1.5000e-03\n",
      "Epoch 106/500: Loss =  1.4748, Score =  0.7377, lr =  1.5000e-03\n",
      "Epoch 107/500: Loss =  1.4836, Score =  0.7369, lr =  1.5000e-03\n",
      "Epoch 108/500: Loss =  1.4869, Score =  0.7363, lr =  1.5000e-03\n",
      "Epoch 109/500: Loss =  1.4849, Score =  0.7372, lr =  1.5000e-03\n",
      "Epoch 110/500: Loss =  1.4754, Score =  0.7373, lr =  1.5000e-03\n",
      "Epoch 111/500: Loss =  1.4662, Score =  0.7401, lr =  1.5000e-03\n",
      "Epoch 112/500: Loss =  1.4882, Score =  0.7361, lr =  1.5000e-03\n",
      "Epoch 113/500: Loss =  1.4855, Score =  0.7364, lr =  1.5000e-03\n",
      "Epoch 114/500: Loss =  1.5028, Score =  0.7344, lr =  1.5000e-03\n",
      "Epoch 115/500: Loss =  1.5088, Score =  0.7331, lr =  1.5000e-03\n",
      "Epoch 116/500: Loss =  1.4968, Score =  0.7357, lr =  1.5000e-03\n",
      "Epoch 117/500: Loss =  1.4868, Score =  0.7363, lr =  1.5000e-03\n",
      "Epoch 118/500: Loss =  1.4904, Score =  0.7351, lr =  1.5000e-03\n",
      "Epoch 119/500: Loss =  1.4888, Score =  0.7355, lr =  1.5000e-03\n",
      "Epoch 120/500: Loss =  1.4905, Score =  0.7350, lr =  1.5000e-03\n",
      "Epoch 121/500: Loss =  1.4675, Score =  0.7389, lr =  1.5000e-03\n",
      "Epoch 122/500: Loss =  1.4704, Score =  0.7390, lr =  1.5000e-03\n",
      "Epoch 123/500: Loss =  1.4760, Score =  0.7378, lr =  1.5000e-03\n",
      "Epoch 124/500: Loss =  1.4825, Score =  0.7361, lr =  1.5000e-03\n",
      "Epoch 125/500: Loss =  1.4825, Score =  0.7366, lr =  1.5000e-03\n",
      "Epoch 126/500: Loss =  1.4782, Score =  0.7378, lr =  1.5000e-03\n",
      "Epoch 127/500: Loss =  1.4967, Score =  0.7346, lr =  1.5000e-03\n",
      "Epoch 128/500: Loss =  1.4971, Score =  0.7356, lr =  1.5000e-03\n",
      "Epoch 129/500: Loss =  1.4949, Score =  0.7354, lr =  1.5000e-03\n",
      "Epoch 130/500: Loss =  1.4963, Score =  0.7342, lr =  1.5000e-03\n",
      "Epoch 131/500: Loss =  1.4793, Score =  0.7369, lr =  1.5000e-03\n",
      "Epoch 132/500: Loss =  1.4824, Score =  0.7363, lr =  1.5000e-03\n",
      "Epoch 133/500: Loss =  1.4837, Score =  0.7366, lr =  1.5000e-03\n",
      "Epoch 134/500: Loss =  1.4909, Score =  0.7354, lr =  1.5000e-03\n",
      "Epoch 135/500: Loss =  1.4979, Score =  0.7348, lr =  1.5000e-03\n",
      "Epoch 136/500: Loss =  1.4975, Score =  0.7347, lr =  1.5000e-03\n",
      "Epoch 137/500: Loss =  1.4869, Score =  0.7363, lr =  1.5000e-03\n",
      "Epoch 138/500: Loss =  1.4878, Score =  0.7356, lr =  1.5000e-03\n",
      "Epoch 139/500: Loss =  1.4657, Score =  0.7390, lr =  1.5000e-03\n",
      "Epoch 140/500: Loss =  1.4690, Score =  0.7389, lr =  1.5000e-03\n",
      "Epoch 141/500: Loss =  1.4748, Score =  0.7379, lr =  1.5000e-03\n",
      "Epoch 142/500: Loss =  1.4585, Score =  0.7404, lr =  1.5000e-03\n",
      "Epoch 143/500: Loss =  1.4798, Score =  0.7362, lr =  1.5000e-03\n",
      "Epoch 144/500: Loss =  1.4773, Score =  0.7376, lr =  1.5000e-03\n",
      "Epoch 145/500: Loss =  1.5376, Score =  0.7268, lr =  1.5000e-03\n",
      "Epoch 146/500: Loss =  1.5124, Score =  0.7309, lr =  1.5000e-03\n",
      "Epoch 147/500: Loss =  1.5066, Score =  0.7318, lr =  1.5000e-03\n",
      "Epoch 148/500: Loss =  1.5156, Score =  0.7311, lr =  1.5000e-03\n",
      "Epoch 149/500: Loss =  1.5068, Score =  0.7325, lr =  1.5000e-03\n",
      "Epoch 150/500: Loss =  1.4853, Score =  0.7362, lr =  1.5000e-03\n",
      "Epoch 151/500: Loss =  1.4979, Score =  0.7343, lr =  1.5000e-03\n",
      "Epoch 152/500: Loss =  1.4886, Score =  0.7369, lr =  1.5000e-03\n",
      "Epoch 153/500: Loss =  1.4847, Score =  0.7373, lr =  1.5000e-03\n",
      "Epoch 154/500: Loss =  1.4850, Score =  0.7373, lr =  1.5000e-03\n",
      "Epoch 155/500: Loss =  1.5025, Score =  0.7344, lr =  1.5000e-03\n",
      "Epoch 156/500: Loss =  1.4969, Score =  0.7347, lr =  1.5000e-03\n",
      "Epoch 157/500: Loss =  1.4763, Score =  0.7388, lr =  1.5000e-03\n",
      "Epoch 158/500: Loss =  1.4748, Score =  0.7391, lr =  1.5000e-03\n",
      "Epoch 159/500: Loss =  1.4664, Score =  0.7402, lr =  1.5000e-03\n",
      "Epoch 160/500: Loss =  1.4545, Score =  0.7423, lr =  1.5000e-03\n",
      "Epoch 161/500: Loss =  1.4751, Score =  0.7397, lr =  1.5000e-03\n",
      "Epoch 162/500: Loss =  1.4673, Score =  0.7400, lr =  1.5000e-03\n",
      "Epoch 163/500: Loss =  1.4595, Score =  0.7414, lr =  1.5000e-03\n",
      "Epoch 164/500: Loss =  1.4735, Score =  0.7406, lr =  1.5000e-03\n",
      "Epoch 165/500: Loss =  1.4660, Score =  0.7413, lr =  1.5000e-03\n",
      "Epoch 166/500: Loss =  1.4581, Score =  0.7415, lr =  1.5000e-03\n",
      "Epoch 167/500: Loss =  1.4804, Score =  0.7378, lr =  1.5000e-03\n",
      "Epoch 168/500: Loss =  1.4715, Score =  0.7407, lr =  1.5000e-03\n",
      "Epoch 169/500: Loss =  1.4597, Score =  0.7416, lr =  1.5000e-03\n",
      "Epoch 170/500: Loss =  1.4554, Score =  0.7423, lr =  1.5000e-03\n",
      "Epoch 171/500: Loss =  1.4704, Score =  0.7410, lr =  1.5000e-03\n",
      "Epoch 172/500: Loss =  1.4881, Score =  0.7376, lr =  1.5000e-03\n",
      "Epoch 173/500: Loss =  1.4863, Score =  0.7374, lr =  1.5000e-03\n",
      "Epoch 174/500: Loss =  1.4854, Score =  0.7369, lr =  1.5000e-03\n",
      "Epoch 175/500: Loss =  1.4875, Score =  0.7374, lr =  1.5000e-03\n",
      "Epoch 176/500: Loss =  1.4931, Score =  0.7361, lr =  1.5000e-03\n",
      "Epoch 177/500: Loss =  1.4769, Score =  0.7383, lr =  1.5000e-03\n",
      "Epoch 178/500: Loss =  1.4690, Score =  0.7401, lr =  1.5000e-03\n",
      "Epoch 179/500: Loss =  1.4601, Score =  0.7412, lr =  1.5000e-03\n",
      "Epoch 180/500: Loss =  1.4589, Score =  0.7415, lr =  1.5000e-03\n",
      "Epoch 181/500: Loss =  1.4615, Score =  0.7419, lr =  1.5000e-03\n",
      "Epoch 182/500: Loss =  1.4740, Score =  0.7395, lr =  1.5000e-03\n",
      "Epoch 183/500: Loss =  1.4707, Score =  0.7396, lr =  1.5000e-03\n",
      "Epoch 184/500: Loss =  1.4804, Score =  0.7378, lr =  1.5000e-03\n",
      "Epoch 185/500: Loss =  1.4910, Score =  0.7362, lr =  1.5000e-03\n",
      "Epoch 186/500: Loss =  1.4749, Score =  0.7391, lr =  1.5000e-03\n",
      "Epoch 187/500: Loss =  1.4619, Score =  0.7418, lr =  1.5000e-03\n",
      "Epoch 188/500: Loss =  1.4701, Score =  0.7401, lr =  1.5000e-03\n",
      "Epoch 189/500: Loss =  1.4856, Score =  0.7369, lr =  1.5000e-03\n",
      "Epoch 190/500: Loss =  1.4675, Score =  0.7400, lr =  1.5000e-03\n",
      "Epoch 191/500: Loss =  1.4722, Score =  0.7392, lr =  1.5000e-03\n",
      "Epoch 192/500: Loss =  1.4712, Score =  0.7398, lr =  1.5000e-03\n",
      "Epoch 193/500: Loss =  1.4733, Score =  0.7395, lr =  1.5000e-03\n",
      "Epoch 194/500: Loss =  1.4671, Score =  0.7409, lr =  1.5000e-03\n",
      "Epoch 195/500: Loss =  1.4766, Score =  0.7394, lr =  1.5000e-03\n",
      "Epoch 196/500: Loss =  1.4682, Score =  0.7403, lr =  1.5000e-03\n",
      "Epoch 197/500: Loss =  1.4712, Score =  0.7404, lr =  1.5000e-03\n",
      "Epoch 198/500: Loss =  1.4677, Score =  0.7400, lr =  1.5000e-03\n",
      "Epoch 199/500: Loss =  1.4818, Score =  0.7382, lr =  1.5000e-03\n",
      "Epoch 200/500: Loss =  1.4932, Score =  0.7352, lr =  1.5000e-03\n",
      "Early stopped.\n",
      "Training complete in 40m 49s\n",
      "Best val Acc: 0.742287\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAA350lEQVR4nO3deXwV1d348c83e0hCCJBAgLCDCIIskUVcUIuCqFTrAirSal2r1dra2lr72N/T53GhtYtaEZVHWxeqdaMVQYuCGyI7EhYJSyAQkgCBhKx3+f7+uJNwc3OT3EBIAvf7fr145c6ZMzPnTsL5zpxz5oyoKsYYY8JPRGsXwBhjTOuwAGCMMWHKAoAxxoQpCwDGGBOmLAAYY0yYsgBgjDFhygKAMQ4RmSAiuQ2sf0lEfteSZQqFiCwRkR+GmFdFpP+JLpM5OVgAMC1GRHaKSLmIHPH793Rrl8v4iMi1IvKliJSJyJLWLo858aJauwAm7Fyuqv9p7UKYoA4CfwIGARe2blFMS7A7ANMmiMj3ReRzEfm9iBSJyA4RmRywfruIlDjrbvBbd7OIbHK2WyQivfzWqYjcJSJbnW3/W0T6icgyESkWkTdEJCagLL8Skf3OHcsN1ENELhORtSJyyLlyHtZA3iaVQ0RuFZFsETkoIvNFpJvfuokisllEDjt3UBJwrHrPR0NU9T+q+gawN5T85uRnAcC0JWOALUBn4AngRfFJAP4CTFbVJOBsYC2AiHwX+BVwFZAKfAa8HrDfScAoYCzwc2AOcAOQAZwBTPfL29U5fndgJjBHRE4LLKiIjATmArcDnYDngPkiEtvA9wupHCJyIfAocC2QDuQA85x1nYG3gF875dwGjPcrVyjnwxjAAoBpee86V8zV/271W5ejqs+rqgd4GV/l18VZ5wXOEJF4Vc1T1Swn/XbgUVXdpKpu4H+B4QFXvY+rarGzzQbgQ1XdrqqHgQ+AEQFlfFhVK1V1KfA+voo40K3Ac6q6XFU9qvoyUImvcq9PqOW4AZirqqtVtRL4JTBORHoDlwIbVfWfqurC12Szz+8YoZwPYwALAKblfVdVO/j9e95vXU1FpqplzsdEVS0FrgPuAPJE5H0RGeSs7wX8uTqg4GvHFnxX8NXy/T6XB1lO9Fsuco5XLQfoRl29gJ/6BzN8V/LB8ja1HN2c4wKgqkeAA8536gbs9lun/suEdj6MASwAmJOEqi5S1Yn47go2A9WBYzdwe0BQiVfVL4/xUClOk1O1ngRvE98N/E/AcdupanM0t+zFV5ED4JSnE7AHyMMXaKrXif8yzX8+zCnMAoBp80Ski4hc4VSElcARwOOsng38UkSGOHmTReSa4zzkb0UkRkTOBS4D3gyS53ngDhEZU91PISJTRCTpOI8N8BrwAxEZ7vQp/C+wXFV34muSGiIiV4lIFPBjfP0W1Y75fIhIpIjE4RsdGCEicSIS3Qzfx7RRFgBMS/uX1H4O4J0QtokAforvyvggcD5wF4CqvgM8DswTkWJ8beuT69lPKPYBRc6xXgXuUNXNgZlUdSW+foCnnfzZwPeP47j++14MPIyvszcP6AdMc9btB64BHsPXLDQA+MJv2+M5HzPwNUU9C5zrfH6+wS3MSU3shTDGGBOe7A7AGGPClAUAY4wJUxYAjDEmTIUUAERkkohscR5NfzDI+gecR+LXisgGEfGISEdn3U4R+cZZt9Jvm44i8pHzaPxHIpLSfF/LGGNMYxrtBBaRSOBbYCKQC6wApqvqxnryXw78RFUvdJZ3ApnO6AX/fE8AB1X1MSeopKjqLxoqS+fOnbV3796hfC9jjDGOVatW7VfV1MD0UGYDHQ1kq+p2ABGZB0wFggYAfPOZhPIwzFRggvP5ZWAJ0GAA6N27NytXrmwoizHGmAAikhMsPZQmoO7UftQ8l3oeKxeRdvgmvHrLL1mBD0VklYjc5pfeRVXzAJyfafXs8zYRWSkiKwsLC0MorjHGmFCEEgAkSFp97UaXA1+o6kG/tPGqOhLfwyg/EpHzmlJAVZ2jqpmqmpmaWucOxhhjzDEKJQDkUnuukR7UP1/4NAKaf1R1r/OzAHgHX5MSQL6IpAM4PwtCL7YxxpjjFUoAWAEMEJE+zgsrpgHzAzOJSDK+R/Tf80tLqJ4bxZnH5WJ8j6bj7GOm83mm/3bGGGNOvEY7gVXVLSJ3A4uASHzzlGeJyB3O+tlO1ivxzW/uP5VuF+Ad34SFRAGvqepCZ91jwBsicguwC9/8JsYYY1rISTUXUGZmptooIGOMaRoRWaWqmYHp9iSwMcaEKQsAxpiTxsHSKt5buwevt+ktFwdLq3h1eQ4FJRXHVYZjOXZbFcqDYMYY0+pUlXvnreGzrfsREa440/f2zfnr9hIhMGVoOku/LWTFzoMUl7u5f+JAUhJiqHB5+PPirfzfFzuocHmZvXQbr9wyhl6dEho5ok9ppZu31+xhdU4Ra3YVsfdQBa/fNoZRvToGzV9YUsnNL63g5nN6c+WIHiF9L6eftMVZADCmDSiv8hAfE9naxTgmX20/QK9O7UhPjj+hx5m3Yjefbd1PYmwUv1+0hUlDuvJ/X+zg0Q987+v5bdJGCksqiYoQ3F4lLSmWm8b15prnvuTb/CNcNaI7F53ehV+/+w3XP7+cJQ9MIDqy4UaQnftLuf3vq9iSX0JqUiwje3agwuXloXc28O97ziEqyPZPfbyVb/Yc5oE315OaGMc5AzqjqngVIiNqV/R/XZLN377M4dkbR9IvLZGFG/ZRWukmLSmOs/qkkJYU13wnMAgLAMa0svnr9vKzN9ex8N5z6Zua2PgGbUh2QQk3vLCciwd34dkbR52QY6gqry7fxf/710bO7teJW8/tyw9eWsHZjy1m/5EqLhuWzvj+nZm/di/3fWcA14zKYObcr3lj1W4q3V6+zT/CizMzuej0LgDEREVw699W8mFWPlOGpdd73PziCq6evQy318vLN4/mvAGdEREWbtjHHa+s4q5XV9OzYzvumNCPzomxAOQcKOW15bu4ckR3NuUVc8/rq/niwQt5+cscnv54K3dO6Ed+cSWrdxXRMSGGz7buJzYqgplzvyY+JpL84sqa40dFCHNuGsWFg7qckPMKFgCMaVUuj5dZizZT5fbyzpo9/PTi05q0fZXby61/W8n63EMkxUXz7I0jGdIt+QSVFt5YsZsnFm0mOT6aq0dl8NX2A3i8ysebCyitdJMQ23xViqryq3c2sChrHwdLqzh/YCp/um44HdpFc8OYnuQXV3L+wM5MH92TqMgIpo/uWbPttNEZ3DtvLc8u3cbkM7rWVP4AFw5Ko3uHeP7+1c56A4DL4+Xu11ZTWunm3R+N57SuR1/1fMmQLkwZls6SzQUsdnvZsPcwr9wyhqjICP70n61ER0bwy8mD2HmgjGufW8Zry3cx59NtREdF8PsPvyUmMoKRvTqwreAIPxjfm5vH92H681+RGBvFM9ePpF9qIjkHy/jV29/wk3+s49/3nENGx3bNdl79WQAw5gTweLXW7f6/1++lS/s4zup9tN3Y7fHy6lc57D5YTmpSLO+t3cv9Ewc2qT146beFLP22kCnD0lmTU8R1z33F3O+fxeg+vuOsyinig2/y+PmkQcREHd+YD49XeeqTrSTGRpGWFMfjC31NL5OGdGVh1j4Wby6gc0IM6R3i6dM5tPb1ansPlVPp9tI5MYakON976NfnHub1r3dx0aA0Jp3Rle+N7EGEc07/58qhDe7vkiFdaR8XRXGFmx9fNKDWusgI4caxvXh84Wb+sWIXQ7t34PT0JNxepcrtJSE2iqc/zmbFziL+PG14rcofQER45vqRAPxzVS4/e3Mdf/joW67NzOC9tXu49dy+pLWPIzUpljO6t+exDzbj9ipv3D6OpLgoOifGkpoUW2ufn/xsAlERUvO7T0mI4dkbR3LZU5/zwD/XMe+2cU06n6GyAGB4Z00uX+8o4vJh6Zzdv3NrFwfwVTYRQqt1jjVFQXEF/7NgE/nFFUw7qycvL9vJoTIXH/7kPKIjI8guKOHu19YAMLZvR24/vx8fbyrg9a934fYqI3t2YPronjzwz/Wsyimib2oiHRNi6hzH69WaCrDaO2ty6ZQQw5+uG87+I5Xc8MJybvv7St6+82w25ZVw/xtrqXR7GdK9fdAOybdW5bJjfymXn9mNvMPldGkfx+np7WvlUVVUYcmWAnYfLOeZ60dy6dCuvLNmD8u3H+S3U4dw3hOfMGvRZnKLypkyNJ2nnQqyMR9tzOfRDzaxvdD3/GhaUixLHphAu5go/rVuL9GRwpPXDie5XXRI+6sWFx3Jzy45jYLiyjrfB+DazB4880k2v3jrGwD6pSaw/0gVZVVu7vvOQP66JJupw7sxdXjQeS9rXD2qB6tyinh2yTYWZe0jJiqCH57bF/D97d48vg/3v7GOM3skc1bvlHr/noP1RfTqlMC9Fw3gd+9vYlVOEaN6Nf8rU+xBsFPErgNldEmOJTaqaR2JFS4P4x5dTFGZC4DZN45k0hn1t4s2l+yCEvYfqWJs304UlVaRW1TO0B7JlFW5ef7THTz/2XbioiO5bFg6v7lscJ2Kr7W8syaX4nI3M8b24sON+by5cjfLth/A7VU6JcSQd7iCuOgIKlxenr5+BJcN68Yj87N4bfku7v3OAF7+cicFJZWIwHWZGQztkczkM9KJjhQyf/cfXB4vCjx34yguHtIVgOyCI/xl8VYWZe3jzgn9uPeiARwqcyECo/9nMTeM7cl/XT4E8LVBT33mC4rLXXgVhvVI5kiFm8S4KN6682zW7j5ESrtoendKoPBIJefPWkKV21vz/SIjhP+eegbXj/E1p6gqD/xzPf/ZlE9yfDSVLi+f/eKCOhXWI/OzeOnLnYDvmPPvPqfec/jCZ9vZeaCUBy4ZxIRZn9AxIYYbx/bC41V+9/4mfj3ldG4e34fxj3/MkG7teWHmWc34GzyquMJF7sFyVu8q4v31eXRNjmP3wTJW5hSRHB/N4p+eX9O23xC3x8udr67mo435/GB875rfBUCl28M9r63h+2f3PqaLq9JKN2c/9jGj+3Tk+ZvqPMcVsvoeBLM7gFNAeZWHSX/+lLP7deL5mzKbdNW8cMM+ispcvDgzk4fe2cBbq/ec8ACwrfAI33t2GRUuD5/9/ALu+8dalm0/wEOXns67a/ewYU8xEwd3weNVXvpyJ5efmV7vkLtquUVluDza5KaHpvg2v4QH3lyP26vM/WIHOQfK6JESz5UjuvPDc/uSnhzHRxvzGdOnI9+b/SV/W5bDBael8daqXC4d2pUfXdCfW87pw4cb8+nbOYEzutduq//1ZYPZsq+YlTuL+MVb6xme0YEO7WK4+aUVFJVWcWZGB/70n6288lUO+49UkRgbRZXHy/dGHr2y79UpgRdnZvLq8l1MOC2Niwd34c1VuTz87gYm//kzsguOADA4vT29O7dDVfnnHeP4Nv8IGR3jeeGzHfzqnW/456rdXH5mN3buL+Wfq3IZ2j2Zb/Yc5heTBgW9Wr1zQj86JsSw80Apn2yuf17H0ko3f/zoW0qrPHy94yBFZS5evnk0w3p0AGDxpgKe+3Q76cnx5B2u4BeTBjXDby649nHRDO4WzeBu7blxbC/Ad0E0a9EWzhnQOaTKHyAqMoKnpo/gzVW5TB3erda62KhI5hxHxZ0QG8XMcb34y8fZZBeU0D8tqfGNmsACwClg7e5DlFV5+M+mAl78fEfNLWgoXvkqhz6dE7jgtDQuHZrOK1/lUFLhqmmHDXS43EVSbNQxX5EXV7i4+aUVRAi4vcqPXlvNip1FdGkfy+/e30R8dGTNiI2SChejfvcf3l+/r9EAcNvffEP1bjuvL/dPHNjo8L7GHC53ER8dSXSk8ON5aykqraK4wkViXBR3nt+POZ9u56cTB3LnhH61hgJe7oxNv3FMLx79YDO3/m0lJZVuZozzVTBx0ZE149cDzXAqoeyCEqb85XPuenU15w1MZdfBMl76wVmcPzCVuV/sZFXOQYZ0S2bt7kPER0cypFvtJo5RvTrWOl9XjejOrIWbyT9cwaNXDcXjVR5dsImNecV8/+zeZPbuSKbTNzGubydeXpbDK1/l8Nt/+d75dM2oHjxx9TAOllaR0q5u0xRAl/Zx/PiiAcxeuo23V++p92/o3+v3UlrlYViPZNbnHuayYek1lT/APRf15/rnl/Oj11aTHB/NdwafuBEwwcRFR/LwZYOPabvq319zm3l2b77eeZCyKk+z79sCQCvZvK+Yju1iSGt//ON8V+X4Xr9w7oDOPL5wM98d0T2kq5fN+4pZmVPEQ5eeTkSEMGVYOnO/2MGHWfmkJ8exKGsfAA9fNpjt+0t5YuFmPt5cwHVnZfDoVcOOqax/+mgruw6W8ebt45i3Yjf/XJVL1/ZxLPrJebz4+Q6+c3paTYWQFBfN+QNTWfBNHr+ecnq9QSe3qIyNecUMSEvk2SXbqHB5at2G+6t0e3h3zR4mnJZGlyDnvtLt4dkl25i9dBvDunfghrE9+de6vSTGRnGk0s3j3xvKdWf15Pbz+zX4Pa/NzOCvS7axteAIt53Xl5E9Q2+/7Z+WxJPXDue+f6xhZU4R4/t34vyBqYgIt5zTh1vO6RPyvsB3Ffnuj8aTEBtV851H9OzAy1/u5J4L+9fKGxUZwS3n9OHm8b0pKKnkcLmLAWmJiAidQvib6umMVtl9sJzB3eoGgNe/3s2AtERe/eEYZi/dxoyxvWutH9e3E7OuHkZUpDC2bycSm3FU0cmqU2KsdQKfSqrcXqbN+YpOCTHMv/uc4x46t2JnEQO7JPKrS09n8p8/Y1HWPm4Y0/jVyGvLdxETFcH3RvmaEEZkdKBbchw/f2s9Hq8SExVBldtLUZmLz7P3o6qc3a8zr3+9m7F9OzXaQRZoy74SXl62k+mje5LZuyOdE2N5f30e908cSHJ8NPdPHFhnmylD0/loYz5rdhfVuqotKKkgKiKClHbRLN7ka3J4bsYoXvlqF3O/2MHALklcl5lRK2hk7T3Mj19fw7bCUob1SOaVH47h78tyEIHx/TpzZkYHHl2wmZe+3Mm4vp1Ytv0AK3MOMqhrEm/fdTZb9pUwPKNDSN81JSGGFQ99h+hIOaaO7CnD0unQLponFm3h4csGH3dneODzBUO6JfPE1WfWm19E6NI+LmiQbEhGii8A7DpYxuBu7VFVjlS6SYqL5pMtBazdfYhfTzmdpLhoHrikbvOOiHBNZkaddHNiWABoBcu2H+BQmYtDZS4efm8DT147/Jj35fEqq3OKuHx4NwZ1TaJv5wTeX59XJwBUuDzc9epqBnRJ5JeTT/c93r56D1OGpteMOImIEO68oD+LNuzjmsweXDy4K7//cAsvfr6D1KRY3rj9bDJS4pk25yt+/c4GLhnSlbjo+judD5ZWcf8ba9maf4QeKfGsyz1EUlwUDzhj3Xt3TmDNbyY2uI+LTk8jJiqCf63LqwkA763dw73z1gJw/sBUXB4vfTsn0Dc1kV9eOohv9hzil29/w9MfZ/PcjFGc0T2Zw+UubvvbKtxeL/dc2J+nPs7mnMc+prjC7RxpCzeO7cmry3cxc1wvfjv1DJ5YuJnZS7fxX5cPoV1MFCOacBUPHPewy/H9O/NeGxmVFaqMjr6ngXOLyigoqeDhdzewKCufs/t1YvWuIk5Pb891Z1kF31ZYAGgFCzfkkRATyYxxvZm9dBv9UhO5aVwvNuwpZmzfjnWu9gLnCql0ezhc5iKtfRxb9pVQUumuGWI2ZVg6z3ySzf4jlXROjOXTbws5XO5i8aZ8Pt5cwNJvC7lxTC++yN7PkUo3N47tWetYM8b2qtWW+atLT6d7h3jOPy21poP1h+f24Y5XVrNlXwlnBrkiXrOriP9syudf6/LYV1zBRYPS2HOonGszM5h2Vk9S/IY4NlT5g68ZaOLpXZi/bi8PTTmdSBH+sngr/dMSmTi4C88u2QbAref6mkWiIyN45Ydj+DArn/9dsIk7X13F23eO55F/ZbGvuIK37jyb4RkdqHB5eHNVLi/clMmoXik89O43vPLVLtKT43jA6Xj8+aRB3HJOn5CaPoxPcnw0SXFR7D5Yxs0vreDb/CNMH53Bwg37fA9f3TK63v4l0/IsALQwj1f5MCufCwal8fNLTqOguIJZi7bwzCfZlFV5+Mv0EbU6CfMOl3Pxk5/SJzWBa0b14HujejDjxa/5JvcwP590Gt/mlwCQ6VwdTxmWzlMfZ/Pumj2M69eJm19agduZvXDG2F7MW7GL//73RtblHmJQ16RG26YjI4SbA9qcq580zdpbXCcAqCp3v7aGfcUVDEhL5PVbxx73+OWrRnbn/W/yWLqlkCqPl22FpTw1fQSXn9mNSBGeWZJda+RSbFQkl5/ZjW4d4rj2ua8Y++hiPF7lgUtOq2nCeWjKYH4xaVBNB+5T00eS2Wsnmb1TarU7W+XfNCJCRko7PtlSyK6DZTxy+WC+P75PTZ9MYwHftCwLAC1s+Y4DHCitYvIZ6URECE9cPYz4mEjKXR7W7j7E0x9v5bKh6TVt18u2HaCk0k15lYeH38viyY++5VC5i+EZHfjd+5sAuGxYOj1SfLfep3VJYkyfjjz2wWa6JsfRoV0Mz1w/gqIyFxcP7kKV28s/Vu6mc2IMf5o2/JjalnukxJMUF0XW3sN11u06WMaeQ+X899QhzBjX+9hPlJ/zBqbSKSGG2Uu3kV9SQd/OCVw61Ffh//Tigdw4thddk+u2VY/q1ZHfffcMlm07wLSzMhjXr1Ot9f6jd4IFOnNsMjrGsygr3zdD5zDfxYxV/G2TBQA/lW5Pkx+kagqPV3li4RY6JsQw4bRUwFcJVT/WXt22vShrH5OdCm71riISY6NYeN95vL06l8cXbuG3VwzhxjG9+Cx7Pz1S4unn18EnIjw/M5Pvz/2a1bsO8dyMUYzpe7Ti+/F3BlDp9vDjiwYc88RjIsLg9PZk7S2us27ZtgMAdSrb4xEdGcEVw7vxf1/spEv7WJ68fnjNNAsiErTyrzZ9dM9ac8SYE696JNDZ/TrXmfLAtC0hBQARmQT8Gd87gV9Q1ccC1j8A3OC3z9OBVCAB+BvQFfACc1T1z842jwC3AoXOdr9S1QXH82WOx6Ksfdw7bw1/mTai5gnMUMxft5fthUe47zt1R7CoKrf/fRUZHdtxz4X9+ceK3azdfYg/XTc86Mify4Z148+Lt/LjeWu4fFM+/2/qGazZdYgzM5KJjPCNjrh6VI+aq/bzB6YGLVP7uGhe+eEYNu8rqdPE071DPH+aNiLk71efId2See3rnDpz3izbfoDUpNhaQak53H1Bf7p3iOfaszJob23IbVr1xGWXn3ninyg3x6fRACAikcAzwEQgF1ghIvNVdWN1HlWdBcxy8l8O/ERVD4pILPBTVV0tIknAKhH5yG/bP6rq75v5Ox2T3QfLqHD5Hul+9oaRIQeB2Uu2kV14hDsn9Ktz97CvuIIPN+YD8OLnOwBfc0bg04LVIiOEV24Zw5xPt/PSlztJT45j874S7ppwdMx5qE027WKimjT2vKmGdGtPhcvL9sIjDOjiezpRVfly2wHG9e3U7HP4dEqMbdIDbqb1TBiYxpRhB2ua6UzbFcodwGggW1W3A4jIPGAqsLGe/NOB1wFUNQ/Icz6XiMgmoHsD27aaCpfvKbtendrx9CfZIQWAg6VVbMzzNYNsyqs7Rnxrvu+x+19OHkRplYd+qQlMHNylwcqxW4d4HrliCNsKj/Dc0u14vMqInh3qzd9ahnT3PX2atbeYAV2SWJ97iM+z91NYUtmszT/m5NOzU7ua2TJN2xbKQOXuwG6/5VwnrQ4RaQdMAt4Ksq43MAJY7pd8t4isF5G5IhL0clVEbhORlSKysrCwMFiWZlHh8hLhTNC1Pvcwuw+WNbpNdXs3wNpdRXXWb3XmXfneqB7cP3EgU4d3p11MaN0ut5zTp2b0zvCME3clf6z6pSYSGxXBoqx9bNlXwtXPLuOJhVuIiYzg3AEn19h1Y8JVKAEg2OVqfVOIXg58oaoHa+1AJBFfULhPVat7Dp8F+gHD8d0l/CHYDlV1jqpmqmpmamrwNu/mUO7yEB8dWXPb+sGGvEa3+Tx7vzM3eixrdh9i7uc7uPKvX7Ct0FfxZxeUkNIumk5BpvZtzPkDU+mflkjfzglBpwZubdGREdw1oT8fbNjHtDnLSIqL4uOfns/q30ykR8qJeXmFMaZ5hXI5mgv4P7rXA9hbT95pOM0/1UQkGl/l/6qqvl2drqr5fnmeB/4dYplPiAqX752sGR3bMbR7Mgu+2cdt5zU838uX2/Yzpk9HYqIiWL79IB9vKqCk0s13n/6Cv/9wDFvzjzAgLemY2sNFhBduyqTC3fwTQDWXey7sT9bew3y4MZ85M0addK8zNCbchXIHsAIYICJ9RCQGXyU/PzCTiCQD5wPv+aUJ8CKwSVWfDMjv30N0JbCh6cVvPuWuo0NAJw/tytrdh8g7XF5v/r2Hysk5UMbZ/TszPKMD+4orKKl08/xNmURHRTDnU99EYP27HHul2LtzAoO61n2ZRVsRESE8df0I/n3POU0aOWWMaRsaDQCq6gbuBhYBm4A3VDVLRO4QkTv8sl4JfKiqpX5p44EZwIUistb5d6mz7gkR+UZE1gMXAD9pji90rKrvAADOG+Bravp6x8F6829yOn+HZyTXzBFzwWmpTBzchSvO7MairHwOl7vof4pfFcdGRdaZ194Yc3IIqUfSGZ+/ICBtdsDyS8BLAWmfE7wPAVWd0YRynnAVLi9x0b54OKhrEgkxkazcWVTvjJfVHbz9U5OIj4nkmlE9uPU83zDFq0Z2r3k70oDjuAMwxpgTyZ4EdpRX+TqBwfd07sheKazMqTuyp9rW/COkJcXWvKt01jVHp9Yd2j2Z/mmJZBf4+gCMMaYtOr75ak8hFW5PrflKMnt1ZPO+YoorXEHzZxeU1Ht1LyLcem4fRvbsQJf29ii8MaZtsgDgKK8KCAC9U1CF1UHuAlSVrY1c3V93Vk/evmt8sz8Ra4wxzcUCgKPS7a0VAIZndCAyQli5s24A2Hu4grIqD/3TrH3fGHPysgDg8PUBHD0dCbFRZPZK4dXlOeQW1X4qeKszB/8ACwDGmJOYBQBH9ZPA/h69aihuj3LXq6upcntr0rOrRwBZADDGnMQsADgqXJ46L63om5rIw5cNZn3uYTb4vfwku+AIHRNi7G1RxpiTmgUAwOvVOn0A1fql+d6DW1Lz8nBYlVPEkG5t9wldY4wJhQUAfB3AEPy1ddUvbimt9AWAPYfK2VpwpN6XsRhjzMnCAgC+9n+gVidwteoXhB9x7gCWbvFNSW0BwBhzsrMAwNGXwQS7A6gJAM4dwJItBXTvEG8dwMaYk54FAPzuAGIabgKqcnv5ctsBzhuYag94GWNOehYA8D0DANR5py/4XnwSGxXBkUo3G/OKOVLptjdeGWNOCRYAgEp3/XcA4GsGOlLp5sCRSsD33l5jjDnZWQAAyqt8o4ACHwSrlhjnCwCHy30TwyXHR7dY2Ywx5kSxAIB/J3Dw05EQE0WpBQBjzCnGAgD+w0AbbgKqDgDt4+w1CsaYk58FAI4GgGDDQKF2E1BibBRRkXbajDEnv5BqMhGZJCJbRCRbRB4Msv4Bv3f+bhARj4h0bGhbEekoIh+JyFbnZ0rzfa2mqWwkACTERlFa6eFwucuaf4wxp4xGA4CIRALPAJOBwcB0ERnsn0dVZ6nqcFUdDvwSWKqqBxvZ9kFgsaoOABY7y62ivJE+gMTYSEoq3BSXu2hvAcAYc4oI5Q5gNJCtqttVtQqYB0xtIP904PUQtp0KvOx8fhn4bhPL3mwqXPXPBQS+PoDSSjeHylwkx1v7vzHm1BBKAOgO7PZbznXS6hCRdsAk4K0Qtu2iqnkAzs+0evZ5m4isFJGVhYWFIRQ3dAXFFby3dg/lLg9REUJ0PW37CbFRlLs8HCyrsiYgY8wpI5QAEGzOA60n7+XAF6p68Bi2DUpV56hqpqpmpqY27wRsry7fxb3z1rL3UHm9I4Dg6HxAeYcqLAAYY04ZoQSAXCDDb7kHsLeevNM42vzT2Lb5IpIO4PwsCKXAzSm3qByAjXuLiavnKWA4GgDKXR4LAMaYU0YoAWAFMEBE+ohIDL5Kfn5gJhFJBs4H3gtx2/nATOfzzIDtWsTeQ74AsK3wSL0dwHB0Qjiwh8CMMaeORns0VdUtIncDi4BIYK6qZonIHc762U7WK4EPVbW0sW2d1Y8Bb4jILcAu4Jrm+lKh2uMEAK/W/xAY+J4DqGYBwBhzqghpSIuqLgAWBKTNDlh+CXgplG2d9APARaEXtXl5vUre4fKa5fpGAMHRJiCA5HYxJ7RcxhjTUsL2kdbCI5W4PEf7oxsKAAkxdgdgjDn1hG0AqG7+qX6zV0MBIMmagIwxp6DwDQDOCKBz+vte7hLsfcDVrBPYGHMqCt8AcKh2AGiwCSj26DoLAMaYU0XYBoC9h8ppHxfFsB7JQMOjgGKjIolxnhK2qaCNMaeKsK3N9hSV061DPKlJsXRKiKFTYsOjexJiI4nxRNhU0MaYU0b4BoBD5fRIiUdEePdH40lJaDgAJMZF4fW2UOGMMaYFhO3l7J5D5TUvd8/o2K7WWP9gEmKibCpoY8wpJSzvAA6VVVFS4SYjpV3I23RNjiMqItjcdsYYc3IKywCwY79vtorenRNC3ubJa4efoNIYY0zrCMsAsPOALwD06Rz6HUDHRvoIjDHmZBOWfQA795chAj2a0ARkjDGnmvAMAAdK6ZYc3+DDX8YYc6oLzwCwv5Q+TWj/N8aYU1HYBQBVZcf+Uno3of3fGGNORWEXAIrKXBRXuOndye4AjDHhLewCQM0QUAsAxpgwF3YBIOdA058BMMaYU1FIAUBEJonIFhHJFpEH68kzQUTWikiWiCx10k5z0qr/FYvIfc66R0Rkj9+6S5vtWzUg54BvCGhGx/iWOJwxxrRZjT4IJiKRwDPARCAXWCEi81V1o1+eDsBfgUmquktE0gBUdQsw3G8/e4B3/Hb/R1X9ffN8ldCUVrqJj44kNsqGgBpjwlsodwCjgWxV3a6qVcA8YGpAnuuBt1V1F4CqFgTZz0XANlXNOZ4CH68qj5eYqLBr+TLGmDpCqQm7A7v9lnOdNH8DgRQRWSIiq0TkpiD7mQa8HpB2t4isF5G5IpIS7OAicpuIrBSRlYWFhSEUt2FVbm/Ny12MMSachVITBpsCUwOWo4BRwBTgEuBhERlYswORGOAK4E2/bZ4F+uFrIsoD/hDs4Ko6R1UzVTUzNTU1hOI2rMptdwDGGAOhTQaXC2T4LfcA9gbJs19VS4FSEfkUOBP41lk/GVitqvnVG/h/FpHngX83vfhNV2kBwBhjgNDuAFYAA0Skj3MlPw2YH5DnPeBcEYkSkXbAGGCT3/rpBDT/iEi63+KVwIamFv5YVFoTkDHGACHcAaiqW0TuBhYBkcBcVc0SkTuc9bNVdZOILATWA17gBVXdAOAEhInA7QG7fkJEhuNrTtoZZP0JUeXxEmuTwBljTGjvA1DVBcCCgLTZAcuzgFlBti0DOgVJn9GkkjaTKreHWLsDMMaY8HsS2DqBjTHGJ+xqQusENsYYn7CrCe05AGOM8Qm7mtDXCRx2X9sYY+oIu5rQ7gCMMcYn7GpC6wQ2xhifsKsJLQAYY4xP2NWENgrIGGN8wqomVFVfJ7D1ARhjTHgFgCqPF8CmgjDGGMItALh9AcBGARljTLgGAOsDMMaY8AoAlRYAjDGmRljVhNYEZIwxR4VVTXi0EzisvrYxxgQVVjWh3QEYY8xRYVUTWh+AMcYcFVY1oY0CMsaYo0KqCUVkkohsEZFsEXmwnjwTRGStiGSJyFK/9J0i8o2zbqVfekcR+UhEtjo/U47/6zSs0u0BINYCgDHGNB4ARCQSeAaYDAwGpovI4IA8HYC/Aleo6hDgmoDdXKCqw1U10y/tQWCxqg4AFjvLJ1T1HUBslD0JbIwxoVwKjwayVXW7qlYB84CpAXmuB95W1V0AqloQwn6nAi87n18GvhtSiY9D9SggawIyxpjQAkB3YLffcq6T5m8gkCIiS0RklYjc5LdOgQ+d9Nv80ruoah6A8zMt2MFF5DYRWSkiKwsLC0Mobv1sFJAxxhwVFUIeCZKmQfYzCrgIiAeWichXqvotMF5V94pIGvCRiGxW1U9DLaCqzgHmAGRmZgYet0msE9gYY44KpSbMBTL8lnsAe4PkWaiqpaq6H/gUOBNAVfc6PwuAd/A1KQHki0g6gPMzlGaj42JNQMYYc1QoNeEKYICI9BGRGGAaMD8gz3vAuSISJSLtgDHAJhFJEJEkABFJAC4GNjjbzAdmOp9nOvs4oSpdFgCMMaZao01AquoWkbuBRUAkMFdVs0TkDmf9bFXdJCILgfWAF3hBVTeISF/gHRGpPtZrqrrQ2fVjwBsicguwi7ojh5pdzVQQFgCMMSakPgBUdQGwICBtdsDyLGBWQNp2nKagIPs8gK/PoMVUWiewMcbUCKuasMrtJSYyAueOxBhjwlr4BQBr/jHGGCDMAkCl22MBwBhjHGFVG1a5vdYBbIwxjrCqDas81gRkjDHVwqo2rO4ENsYYE44BwO4AjDEGCLcAYE1AxhhTI6xqw0qXNQEZY0y1sKoNKz1eYqPtZTDGGANhFgCsE9gYY44Kq9qwyu2x5wCMMcYRVrWhdQIbY8xRYVUbWiewMcYcFVa1YZXHS2x0WH1lY4ypV1jVhtYJbIwxR4VVbWhPAhtjzFFhUxt6vYrbqxYAjDHGEVJtKCKTRGSLiGSLyIP15JkgImtFJEtEljppGSLyiYhsctLv9cv/iIjscbZZKyKXNs9XCq76fcAWAIwxxqfRdwKLSCTwDDARyAVWiMh8Vd3ol6cD8FdgkqruEpE0Z5Ub+KmqrhaRJGCViHzkt+0fVfX3zfh96mXvAzbGmNpCqQ1HA9mqul1Vq4B5wNSAPNcDb6vqLgBVLXB+5qnqaudzCbAJ6N5chW8Kt3MHEG0BwBhjgNACQHdgt99yLnUr8YFAiogsEZFVInJT4E5EpDcwAljul3y3iKwXkbkikhLs4CJym4isFJGVhYWFIRQ3OJdHAQsAxhhTLZTaUIKkacByFDAKmAJcAjwsIgNrdiCSCLwF3KeqxU7ys0A/YDiQB/wh2MFVdY6qZqpqZmpqagjFDc7l3AFERQb7OsYYE34a7QPAd8Wf4bfcA9gbJM9+VS0FSkXkU+BM4FsRicZX+b+qqm9Xb6Cq+dWfReR54N/H9hVC4/b6Ypb1ARhjjE8oteEKYICI9BGRGGAaMD8gz3vAuSISJSLtgDHAJhER4EVgk6o+6b+BiKT7LV4JbDjWLxEKuwMwxpjaGr0DUFW3iNwNLAIigbmqmiUidzjrZ6vqJhFZCKwHvMALqrpBRM4BZgDfiMhaZ5e/UtUFwBMiMhxfc9JO4Pbm/Wq11QSACLsDMMYYCK0JCKfCXhCQNjtgeRYwKyDtc4L3IaCqM5pU0uNU3QkcE2V3AMYYA2H0JLDb7gCMMaaWsKkNq+8ArA/AGGN8wigA2JPAxhjjL2xqQ7e3ehRQ2HxlY4xpUNjUhkefBLYmIGOMgbAKADYXkDHG+Aub2tBd3QkcYXcAxhgDYRQAquwOwBhjagmb2tBts4EaY0wtYVMbVo8Csk5gY4zxCZsAUOW2YaDGGOMvbGrD6umg7Q7AGGN8wiYAuNzWCWyMMf7CpjZ0eW0YqDHG+AubAOD2eImOFHzvqDHGGBM2AcDl8dpU0MYY4ydsakSXR20qaGOM8RM2AcDt9dpU0MYY4yekGlFEJonIFhHJFpEH68kzQUTWikiWiCxtbFsR6SgiH4nIVudnyvF/nfq53HYHYIwx/hoNACISCTwDTAYGA9NFZHBAng7AX4ErVHUIcE0I2z4ILFbVAcBiZ/mEcXmtD8AYY/yFUiOOBrJVdbuqVgHzgKkBea4H3lbVXQCqWhDCtlOBl53PLwPfPeZvEQKXR4mJsgBgjDHVQqkRuwO7/ZZznTR/A4EUEVkiIqtE5KYQtu2iqnkAzs+0YAcXkdtEZKWIrCwsLAyhuMG5PV57BsAYY/xEhZAnWK2pQfYzCrgIiAeWichXIW7bIFWdA8wByMzMbNK2/lwetaeAjTHGTygBIBfI8FvuAewNkme/qpYCpSLyKXBmI9vmi0i6quaJSDpQwAnkch4EM8YY4xPKJfEKYICI9BGRGGAaMD8gz3vAuSISJSLtgDHApka2nQ/MdD7PdPZxwri9XpsJ1Bhj/DR6B6CqbhG5G1gERAJzVTVLRO5w1s9W1U0ishBYD3iBF1R1A0CwbZ1dPwa8ISK3ALtwRg6dKC632h2AMcb4CaUJCFVdACwISJsdsDwLmBXKtk76AXx9Bi3C5fWSGB3S1zXGmLAQNm0ibusENsaYWsKmRnTZMFBjjKklrAKA3QEYY8xRYVMjur3WCWyMMf7CJgC43DYM1Bhj/IVNjeiyOwBjjKklfAKA9QEYY0wtYVMjuj1q00EbY4yfsKkRXR4v0VHWBGSMMdXCKwDYHYAxxtQIixrR41W8ir0S0hhj/IRFAHB5vADWCWyMMX7CokZ0e33vkbFhoMYYc1R4BAC7AzDGmDrCokascgKAPQlsjDFHhUWN6PY4TUA2G6gxxtQIrwBgdwDGGFMjLGrEo01AdgdgjDHVQgoAIjJJRLaISLaIPBhk/QQROSwia51/v3HST/NLWysixSJyn7PuERHZ47fu0mb9Zn7cXusENsaYQI2+JFdEIoFngIlALrBCROar6saArJ+p6mX+Caq6BRjut589wDt+Wf6oqr8/9uKHxuW2JiBjjAkUSo04GshW1e2qWgXMA6Yew7EuArapas4xbHtcXF5rAjLGmEChBIDuwG6/5VwnLdA4EVknIh+IyJAg66cBrwek3S0i60VkroikBDu4iNwmIitFZGVhYWEIxa2ruhM4xu4AjDGmRig1YrDLZg1YXg30UtUzgaeAd2vtQCQGuAJ40y/5WaAfviaiPOAPwQ6uqnNUNVNVM1NTU0Mobl3VU0HYS+GNMeaoUAJALpDht9wD2OufQVWLVfWI83kBEC0inf2yTAZWq2q+3zb5qupRVS/wPL6mphPCZQ+CGWNMHaHUiCuAASLSx7mSnwbM988gIl1FRJzPo539HvDLMp2A5h8RSfdbvBLY0PTih8ZlTUDGGFNHo6OAVNUtIncDi4BIYK6qZonIHc762cDVwJ0i4gbKgWmqqgAi0g7fCKLbA3b9hIgMx9ectDPI+mbjtucAjDGmjkYDANQ06ywISJvt9/lp4Ol6ti0DOgVJn9Gkkh4Hl9eGgRpjTKCwqBFd7uoHwewOwBhjqoVFAHB7rRPYGGMChUWNWOWxF8IYY0ygsAgANS+EsZfCG2NMjbCoEaufBLZRQMYYc1RYBIAqeyWkMcbUERY1or0Qxhhj6gqLGtHt9RIhEGlzARljTI2wCABVHq8NATXGmABhUSu6PWovhDfGmABhEQBcHi/RUWHxVY0xJmQhzQV0shuc3p4Kl6e1i2GMMW1KWASAaaN7Mm10z9YuhjHGtCnWLmKMMWHKAoAxxoQpCwDGGBOmLAAYY0yYsgBgjDFhygKAMcaEKQsAxhgTpiwAGGNMmBJVbe0yhExECoGcY9y8M7C/GYvTXNpquaDtls3K1TRttVzQdst2qpWrl6qmBiaeVAHgeIjISlXNbO1yBGqr5YK2WzYrV9O01XJB2y1buJTLmoCMMSZMWQAwxpgwFU4BYE5rF6AebbVc0HbLZuVqmrZaLmi7ZQuLcoVNH4AxxpjawukOwBhjjB8LAMYYE6bCIgCIyCQR2SIi2SLyYCuWI0NEPhGRTSKSJSL3OumPiMgeEVnr/Lu0Fcq2U0S+cY6/0knrKCIfichW52dKC5fpNL9zslZEikXkvtY6XyIyV0QKRGSDX1q950hEfun8zW0RkUtauFyzRGSziKwXkXdEpIOT3ltEyv3O3ewWLle9v7tWPl//8CvTThFZ66S35Pmqr344cX9jqnpK/wMigW1AXyAGWAcMbqWypAMjnc9JwLfAYOAR4GetfJ52Ap0D0p4AHnQ+Pwg83sq/x31Ar9Y6X8B5wEhgQ2PnyPm9rgNigT7O32BkC5brYiDK+fy4X7l6++drhfMV9HfX2ucrYP0fgN+0wvmqr344YX9j4XAHMBrIVtXtqloFzAOmtkZBVDVPVVc7n0uATUD31ihLiKYCLzufXwa+23pF4SJgm6oe65Pgx01VPwUOBiTXd46mAvNUtVJVdwDZ+P4WW6Rcqvqhqrqdxa+AHifi2E0tVwNa9XxVExEBrgVePxHHbkgD9cMJ+xsLhwDQHdjtt5xLG6h0RaQ3MAJY7iTd7dyuz23pphaHAh+KyCoRuc1J66KqeeD74wTSWqFc1aZR+z9la5+vavWdo7b0d3cz8IHfch8RWSMiS0Xk3FYoT7DfXVs5X+cC+aq61S+txc9XQP1wwv7GwiEASJC0Vh37KiKJwFvAfapaDDwL9AOGA3n4bkFb2nhVHQlMBn4kIue1QhmCEpEY4ArgTSepLZyvxrSJvzsReQhwA686SXlAT1UdAdwPvCYi7VuwSPX97trE+QKmU/tCo8XPV5D6od6sQdKadM7CIQDkAhl+yz2Ava1UFkQkGt8v91VVfRtAVfNV1aOqXuB5TtCtb0NUda/zswB4xylDvoikO+VOBwpaulyOycBqVc13ytjq58tPfeeo1f/uRGQmcBlwgzqNxk5zwQHn8yp87cYDW6pMDfzu2sL5igKuAv5RndbS5ytY/cAJ/BsLhwCwAhggIn2cK8lpwPzWKIjTvvgisElVn/RLT/fLdiWwIXDbE1yuBBFJqv6MrwNxA77zNNPJNhN4ryXL5afWVVlrn68A9Z2j+cA0EYkVkT7AAODrliqUiEwCfgFcoaplfumpIhLpfO7rlGt7C5arvt9dq54vx3eAzaqaW53QkuervvqBE/k31hK92639D7gUX4/6NuChVizHOfhu0dYDa51/lwJ/B75x0ucD6S1crr74RhOsA7KqzxHQCVgMbHV+dmyFc9YOOAAk+6W1yvnCF4TyABe+q69bGjpHwEPO39wWYHILlysbX/tw9d/ZbCfv95zf8TpgNXB5C5er3t9da54vJ/0l4I6AvC15vuqrH07Y35hNBWGMMWEqHJqAjDHGBGEBwBhjwpQFAGOMCVMWAIwxJkxZADDGmDBlAcAYY8KUBQBjjAlT/x/+v4FWRkUREAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/500: Loss =  6.7417, Score = -0.0219, lr =  6.0247e-03\n",
      "Epoch   2/500: Loss =  6.3981, Score =  0.0224, lr =  6.0247e-03\n",
      "Epoch   3/500: Loss =  5.9957, Score =  0.0749, lr =  6.0247e-03\n",
      "Epoch   4/500: Loss =  5.4984, Score =  0.1451, lr =  6.0247e-03\n",
      "Epoch   5/500: Loss =  4.9749, Score =  0.2200, lr =  6.0247e-03\n",
      "Epoch   6/500: Loss =  4.5310, Score =  0.2858, lr =  6.0247e-03\n",
      "Epoch   7/500: Loss =  4.1635, Score =  0.3402, lr =  6.0247e-03\n",
      "Epoch   8/500: Loss =  3.8626, Score =  0.3866, lr =  6.0247e-03\n",
      "Epoch   9/500: Loss =  3.5988, Score =  0.4267, lr =  6.0247e-03\n",
      "Epoch  10/500: Loss =  3.3757, Score =  0.4584, lr =  6.0247e-03\n",
      "Epoch  11/500: Loss =  3.2158, Score =  0.4801, lr =  6.0247e-03\n",
      "Epoch  12/500: Loss =  3.0907, Score =  0.4962, lr =  6.0247e-03\n",
      "Epoch  13/500: Loss =  3.0215, Score =  0.5041, lr =  6.0247e-03\n",
      "Epoch  14/500: Loss =  2.9904, Score =  0.5073, lr =  6.0247e-03\n",
      "Epoch  15/500: Loss =  2.9911, Score =  0.5063, lr =  6.0247e-03\n",
      "Epoch  16/500: Loss =  2.9888, Score =  0.5081, lr =  6.0247e-03\n",
      "Epoch  17/500: Loss =  2.9529, Score =  0.5147, lr =  6.0247e-03\n",
      "Epoch  18/500: Loss =  2.9064, Score =  0.5223, lr =  6.0247e-03\n",
      "Epoch  19/500: Loss =  2.8732, Score =  0.5276, lr =  6.0247e-03\n",
      "Epoch  20/500: Loss =  2.8317, Score =  0.5333, lr =  6.0247e-03\n",
      "Epoch  21/500: Loss =  2.8163, Score =  0.5354, lr =  6.0247e-03\n",
      "Epoch  22/500: Loss =  2.7924, Score =  0.5392, lr =  6.0247e-03\n",
      "Epoch  23/500: Loss =  2.7565, Score =  0.5450, lr =  6.0247e-03\n",
      "Epoch  24/500: Loss =  2.7364, Score =  0.5480, lr =  6.0247e-03\n",
      "Epoch  25/500: Loss =  2.6995, Score =  0.5535, lr =  6.0247e-03\n",
      "Epoch  26/500: Loss =  2.6924, Score =  0.5546, lr =  6.0247e-03\n",
      "Epoch  27/500: Loss =  2.6756, Score =  0.5569, lr =  6.0247e-03\n",
      "Epoch  28/500: Loss =  2.6585, Score =  0.5587, lr =  6.0247e-03\n",
      "Epoch  29/500: Loss =  2.6687, Score =  0.5570, lr =  6.0247e-03\n",
      "Epoch  30/500: Loss =  2.6987, Score =  0.5522, lr =  6.0247e-03\n",
      "Epoch  31/500: Loss =  2.7124, Score =  0.5495, lr =  6.0247e-03\n",
      "Epoch  32/500: Loss =  2.7284, Score =  0.5467, lr =  6.0247e-03\n",
      "Epoch  33/500: Loss =  2.7086, Score =  0.5501, lr =  6.0247e-03\n",
      "Epoch  34/500: Loss =  2.6759, Score =  0.5550, lr =  6.0247e-03\n",
      "Epoch  35/500: Loss =  2.6468, Score =  0.5602, lr =  6.0247e-03\n",
      "Epoch  36/500: Loss =  2.6304, Score =  0.5636, lr =  6.0247e-03\n",
      "Epoch  37/500: Loss =  2.6127, Score =  0.5674, lr =  6.0247e-03\n",
      "Epoch  38/500: Loss =  2.6096, Score =  0.5681, lr =  6.0247e-03\n",
      "Epoch  39/500: Loss =  2.6312, Score =  0.5661, lr =  6.0247e-03\n",
      "Epoch  40/500: Loss =  2.6381, Score =  0.5659, lr =  6.0247e-03\n",
      "Epoch  41/500: Loss =  2.6384, Score =  0.5658, lr =  6.0247e-03\n",
      "Epoch  42/500: Loss =  2.6184, Score =  0.5682, lr =  6.0247e-03\n",
      "Epoch  43/500: Loss =  2.5928, Score =  0.5719, lr =  6.0247e-03\n",
      "Epoch  44/500: Loss =  2.5734, Score =  0.5743, lr =  6.0247e-03\n",
      "Epoch  45/500: Loss =  2.5724, Score =  0.5750, lr =  6.0247e-03\n",
      "Epoch  46/500: Loss =  2.5786, Score =  0.5750, lr =  6.0247e-03\n",
      "Epoch  47/500: Loss =  2.5763, Score =  0.5751, lr =  6.0247e-03\n",
      "Epoch  48/500: Loss =  2.5796, Score =  0.5750, lr =  6.0247e-03\n",
      "Epoch  49/500: Loss =  2.5842, Score =  0.5743, lr =  6.0247e-03\n",
      "Epoch  50/500: Loss =  2.5888, Score =  0.5737, lr =  6.0247e-03\n",
      "Epoch  51/500: Loss =  2.5915, Score =  0.5731, lr =  6.0247e-03\n",
      "Epoch  52/500: Loss =  2.5861, Score =  0.5735, lr =  6.0247e-03\n",
      "Epoch  53/500: Loss =  2.5691, Score =  0.5756, lr =  6.0247e-03\n",
      "Epoch  54/500: Loss =  2.5809, Score =  0.5740, lr =  6.0247e-03\n",
      "Epoch  55/500: Loss =  2.5714, Score =  0.5757, lr =  6.0247e-03\n",
      "Epoch  56/500: Loss =  2.5637, Score =  0.5772, lr =  6.0247e-03\n",
      "Epoch  57/500: Loss =  2.5539, Score =  0.5796, lr =  6.0247e-03\n",
      "Epoch  58/500: Loss =  2.5517, Score =  0.5797, lr =  6.0247e-03\n",
      "Epoch  59/500: Loss =  2.5177, Score =  0.5859, lr =  6.0247e-03\n",
      "Epoch  60/500: Loss =  2.5178, Score =  0.5865, lr =  6.0247e-03\n",
      "Epoch  61/500: Loss =  2.5267, Score =  0.5857, lr =  6.0247e-03\n",
      "Epoch  62/500: Loss =  2.5287, Score =  0.5845, lr =  6.0247e-03\n",
      "Epoch  63/500: Loss =  2.5011, Score =  0.5876, lr =  6.0247e-03\n",
      "Epoch  64/500: Loss =  2.4895, Score =  0.5878, lr =  6.0247e-03\n",
      "Epoch  65/500: Loss =  2.4748, Score =  0.5893, lr =  6.0247e-03\n",
      "Epoch  66/500: Loss =  2.4806, Score =  0.5895, lr =  6.0247e-03\n",
      "Epoch  67/500: Loss =  2.4515, Score =  0.5953, lr =  6.0247e-03\n",
      "Epoch  68/500: Loss =  2.4415, Score =  0.5980, lr =  6.0247e-03\n",
      "Epoch  69/500: Loss =  2.4623, Score =  0.5953, lr =  6.0247e-03\n",
      "Epoch  70/500: Loss =  2.4862, Score =  0.5922, lr =  6.0247e-03\n",
      "Epoch  71/500: Loss =  2.5045, Score =  0.5892, lr =  6.0247e-03\n",
      "Epoch  72/500: Loss =  2.5361, Score =  0.5836, lr =  6.0247e-03\n",
      "Epoch  73/500: Loss =  2.5555, Score =  0.5796, lr =  6.0247e-03\n",
      "Epoch  74/500: Loss =  2.5614, Score =  0.5778, lr =  6.0247e-03\n",
      "Epoch  75/500: Loss =  2.5409, Score =  0.5806, lr =  6.0247e-03\n",
      "Epoch  76/500: Loss =  2.5280, Score =  0.5817, lr =  6.0247e-03\n",
      "Epoch  77/500: Loss =  2.5298, Score =  0.5814, lr =  6.0247e-03\n",
      "Epoch  78/500: Loss =  2.5124, Score =  0.5844, lr =  6.0247e-03\n",
      "Epoch  79/500: Loss =  2.5056, Score =  0.5860, lr =  6.0247e-03\n",
      "Epoch  80/500: Loss =  2.5273, Score =  0.5826, lr =  6.0247e-03\n",
      "Epoch  81/500: Loss =  2.4968, Score =  0.5882, lr =  6.0247e-03\n",
      "Epoch  82/500: Loss =  2.4900, Score =  0.5902, lr =  6.0247e-03\n",
      "Epoch  83/500: Loss =  2.4837, Score =  0.5918, lr =  6.0247e-03\n",
      "Epoch  84/500: Loss =  2.4719, Score =  0.5938, lr =  6.0247e-03\n",
      "Epoch  85/500: Loss =  2.4356, Score =  0.5996, lr =  6.0247e-03\n",
      "Epoch  86/500: Loss =  2.4004, Score =  0.6054, lr =  6.0247e-03\n",
      "Epoch  87/500: Loss =  2.3728, Score =  0.6105, lr =  6.0247e-03\n",
      "Epoch  88/500: Loss =  2.3510, Score =  0.6143, lr =  6.0247e-03\n",
      "Epoch  89/500: Loss =  2.3504, Score =  0.6143, lr =  6.0247e-03\n",
      "Epoch  90/500: Loss =  2.3720, Score =  0.6100, lr =  6.0247e-03\n",
      "Epoch  91/500: Loss =  2.3767, Score =  0.6083, lr =  6.0247e-03\n",
      "Epoch  92/500: Loss =  2.3879, Score =  0.6061, lr =  6.0247e-03\n",
      "Epoch  93/500: Loss =  2.3818, Score =  0.6069, lr =  6.0247e-03\n",
      "Epoch  94/500: Loss =  2.3861, Score =  0.6062, lr =  6.0247e-03\n",
      "Epoch  95/500: Loss =  2.3834, Score =  0.6067, lr =  6.0247e-03\n",
      "Epoch  96/500: Loss =  2.3774, Score =  0.6077, lr =  6.0247e-03\n",
      "Epoch  97/500: Loss =  2.3843, Score =  0.6072, lr =  6.0247e-03\n",
      "Epoch  98/500: Loss =  2.4046, Score =  0.6047, lr =  6.0247e-03\n",
      "Epoch  99/500: Loss =  2.4116, Score =  0.6031, lr =  6.0247e-03\n",
      "Epoch 100/500: Loss =  2.4130, Score =  0.6022, lr =  6.0247e-03\n",
      "Epoch 101/500: Loss =  2.4239, Score =  0.5985, lr =  6.0247e-03\n",
      "Epoch 102/500: Loss =  2.4240, Score =  0.5970, lr =  6.0247e-03\n",
      "Epoch 103/500: Loss =  2.4397, Score =  0.5933, lr =  6.0247e-03\n",
      "Epoch 104/500: Loss =  2.4249, Score =  0.5961, lr =  6.0247e-03\n",
      "Epoch 105/500: Loss =  2.4112, Score =  0.5991, lr =  6.0247e-03\n",
      "Epoch 106/500: Loss =  2.3858, Score =  0.6042, lr =  6.0247e-03\n",
      "Epoch 107/500: Loss =  2.3714, Score =  0.6078, lr =  6.0247e-03\n",
      "Epoch 108/500: Loss =  2.3771, Score =  0.6080, lr =  6.0247e-03\n",
      "Epoch 109/500: Loss =  2.3811, Score =  0.6088, lr =  6.0247e-03\n",
      "Epoch 110/500: Loss =  2.3929, Score =  0.6069, lr =  6.0247e-03\n",
      "Epoch 111/500: Loss =  2.4066, Score =  0.6040, lr =  6.0247e-03\n",
      "Epoch 112/500: Loss =  2.4544, Score =  0.5959, lr =  6.0247e-03\n",
      "Epoch 113/500: Loss =  2.4822, Score =  0.5917, lr =  6.0247e-03\n",
      "Epoch 114/500: Loss =  2.5045, Score =  0.5894, lr =  6.0247e-03\n",
      "Epoch 115/500: Loss =  2.4697, Score =  0.5959, lr =  6.0247e-03\n",
      "Epoch 116/500: Loss =  2.4557, Score =  0.5973, lr =  6.0247e-03\n",
      "Epoch 117/500: Loss =  2.4313, Score =  0.5998, lr =  6.0247e-03\n",
      "Epoch 118/500: Loss =  2.3859, Score =  0.6065, lr =  6.0247e-03\n",
      "Epoch 119/500: Loss =  2.3405, Score =  0.6135, lr =  6.0247e-03\n",
      "Epoch 120/500: Loss =  2.3411, Score =  0.6132, lr =  6.0247e-03\n",
      "Epoch 121/500: Loss =  2.3439, Score =  0.6130, lr =  6.0247e-03\n",
      "Epoch 122/500: Loss =  2.3571, Score =  0.6117, lr =  6.0247e-03\n",
      "Epoch 123/500: Loss =  2.3802, Score =  0.6095, lr =  6.0247e-03\n",
      "Epoch 124/500: Loss =  2.3952, Score =  0.6072, lr =  6.0247e-03\n",
      "Epoch 125/500: Loss =  2.4113, Score =  0.6045, lr =  6.0247e-03\n",
      "Epoch 126/500: Loss =  2.4211, Score =  0.6022, lr =  6.0247e-03\n",
      "Epoch 127/500: Loss =  2.4147, Score =  0.6029, lr =  6.0247e-03\n",
      "Epoch 128/500: Loss =  2.3838, Score =  0.6081, lr =  6.0247e-03\n",
      "Epoch 129/500: Loss =  2.3773, Score =  0.6090, lr =  6.0247e-03\n",
      "Epoch 130/500: Loss =  2.3682, Score =  0.6094, lr =  6.0247e-03\n",
      "Epoch 131/500: Loss =  2.3711, Score =  0.6082, lr =  6.0247e-03\n",
      "Epoch 132/500: Loss =  2.3687, Score =  0.6082, lr =  6.0247e-03\n",
      "Epoch 133/500: Loss =  2.3651, Score =  0.6087, lr =  6.0247e-03\n",
      "Epoch 134/500: Loss =  2.3726, Score =  0.6075, lr =  6.0247e-03\n",
      "Epoch 135/500: Loss =  2.3757, Score =  0.6067, lr =  6.0247e-03\n",
      "Epoch 136/500: Loss =  2.3705, Score =  0.6079, lr =  6.0247e-03\n",
      "Epoch 137/500: Loss =  2.3752, Score =  0.6089, lr =  6.0247e-03\n",
      "Epoch 138/500: Loss =  2.3693, Score =  0.6108, lr =  6.0247e-03\n",
      "Epoch 139/500: Loss =  2.3637, Score =  0.6130, lr =  6.0247e-03\n",
      "Epoch 140/500: Loss =  2.3428, Score =  0.6170, lr =  6.0247e-03\n",
      "Epoch 141/500: Loss =  2.3266, Score =  0.6190, lr =  6.0247e-03\n",
      "Epoch 142/500: Loss =  2.3126, Score =  0.6196, lr =  6.0247e-03\n",
      "Epoch 143/500: Loss =  2.3000, Score =  0.6211, lr =  6.0247e-03\n",
      "Epoch 144/500: Loss =  2.2917, Score =  0.6223, lr =  6.0247e-03\n",
      "Epoch 145/500: Loss =  2.3012, Score =  0.6206, lr =  6.0247e-03\n",
      "Epoch 146/500: Loss =  2.3093, Score =  0.6202, lr =  6.0247e-03\n",
      "Epoch 147/500: Loss =  2.3299, Score =  0.6182, lr =  6.0247e-03\n",
      "Epoch 148/500: Loss =  2.3484, Score =  0.6167, lr =  6.0247e-03\n",
      "Epoch 149/500: Loss =  2.3337, Score =  0.6192, lr =  6.0247e-03\n",
      "Epoch 150/500: Loss =  2.3234, Score =  0.6212, lr =  6.0247e-03\n",
      "Epoch 151/500: Loss =  2.3398, Score =  0.6191, lr =  6.0247e-03\n",
      "Epoch 152/500: Loss =  2.3392, Score =  0.6184, lr =  6.0247e-03\n",
      "Epoch 153/500: Loss =  2.3351, Score =  0.6178, lr =  6.0247e-03\n",
      "Epoch 154/500: Loss =  2.3306, Score =  0.6167, lr =  6.0247e-03\n",
      "Epoch 155/500: Loss =  2.3191, Score =  0.6170, lr =  6.0247e-03\n",
      "Epoch 156/500: Loss =  2.3252, Score =  0.6147, lr =  6.0247e-03\n",
      "Epoch 157/500: Loss =  2.3295, Score =  0.6135, lr =  6.0247e-03\n",
      "Epoch 158/500: Loss =  2.3304, Score =  0.6132, lr =  6.0247e-03\n",
      "Epoch 159/500: Loss =  2.3051, Score =  0.6182, lr =  6.0247e-03\n",
      "Epoch 160/500: Loss =  2.2865, Score =  0.6218, lr =  6.0247e-03\n",
      "Epoch 161/500: Loss =  2.2686, Score =  0.6250, lr =  6.0247e-03\n",
      "Epoch 162/500: Loss =  2.2698, Score =  0.6256, lr =  6.0247e-03\n",
      "Epoch 163/500: Loss =  2.2613, Score =  0.6271, lr =  6.0247e-03\n",
      "Epoch 164/500: Loss =  2.2559, Score =  0.6275, lr =  6.0247e-03\n",
      "Epoch 165/500: Loss =  2.2512, Score =  0.6289, lr =  6.0247e-03\n",
      "Epoch 166/500: Loss =  2.2560, Score =  0.6278, lr =  6.0247e-03\n",
      "Epoch 167/500: Loss =  2.2736, Score =  0.6255, lr =  6.0247e-03\n",
      "Epoch 168/500: Loss =  2.2931, Score =  0.6223, lr =  6.0247e-03\n",
      "Epoch 169/500: Loss =  2.3084, Score =  0.6200, lr =  6.0247e-03\n",
      "Epoch 170/500: Loss =  2.3149, Score =  0.6198, lr =  6.0247e-03\n",
      "Epoch 171/500: Loss =  2.3178, Score =  0.6196, lr =  6.0247e-03\n",
      "Epoch 172/500: Loss =  2.3157, Score =  0.6198, lr =  6.0247e-03\n",
      "Epoch 173/500: Loss =  2.3090, Score =  0.6201, lr =  6.0247e-03\n",
      "Epoch 174/500: Loss =  2.2852, Score =  0.6233, lr =  6.0247e-03\n",
      "Epoch 175/500: Loss =  2.2830, Score =  0.6232, lr =  6.0247e-03\n",
      "Epoch 176/500: Loss =  2.2941, Score =  0.6217, lr =  6.0247e-03\n",
      "Epoch 177/500: Loss =  2.2973, Score =  0.6215, lr =  6.0247e-03\n",
      "Epoch 178/500: Loss =  2.3015, Score =  0.6210, lr =  6.0247e-03\n",
      "Epoch 179/500: Loss =  2.3189, Score =  0.6189, lr =  6.0247e-03\n",
      "Epoch 180/500: Loss =  2.3350, Score =  0.6166, lr =  6.0247e-03\n",
      "Epoch 181/500: Loss =  2.3295, Score =  0.6180, lr =  6.0247e-03\n",
      "Epoch 182/500: Loss =  2.3247, Score =  0.6188, lr =  6.0247e-03\n",
      "Epoch 183/500: Loss =  2.3220, Score =  0.6193, lr =  6.0247e-03\n",
      "Epoch 184/500: Loss =  2.3162, Score =  0.6200, lr =  6.0247e-03\n",
      "Epoch 185/500: Loss =  2.3054, Score =  0.6206, lr =  6.0247e-03\n",
      "Epoch 186/500: Loss =  2.3036, Score =  0.6201, lr =  6.0247e-03\n",
      "Epoch 187/500: Loss =  2.3078, Score =  0.6189, lr =  6.0247e-03\n",
      "Epoch 188/500: Loss =  2.3036, Score =  0.6196, lr =  6.0247e-03\n",
      "Epoch 189/500: Loss =  2.3086, Score =  0.6190, lr =  6.0247e-03\n",
      "Epoch 190/500: Loss =  2.2984, Score =  0.6214, lr =  6.0247e-03\n",
      "Epoch 191/500: Loss =  2.2904, Score =  0.6232, lr =  6.0247e-03\n",
      "Epoch 192/500: Loss =  2.2849, Score =  0.6247, lr =  6.0247e-03\n",
      "Epoch 193/500: Loss =  2.2886, Score =  0.6251, lr =  6.0247e-03\n",
      "Epoch 194/500: Loss =  2.2731, Score =  0.6281, lr =  6.0247e-03\n",
      "Epoch 195/500: Loss =  2.2882, Score =  0.6268, lr =  6.0247e-03\n",
      "Epoch 196/500: Loss =  2.2969, Score =  0.6256, lr =  6.0247e-03\n",
      "Epoch 197/500: Loss =  2.3270, Score =  0.6204, lr =  6.0247e-03\n",
      "Epoch 198/500: Loss =  2.3560, Score =  0.6156, lr =  6.0247e-03\n",
      "Epoch 199/500: Loss =  2.3548, Score =  0.6149, lr =  6.0247e-03\n",
      "Epoch 200/500: Loss =  2.3520, Score =  0.6153, lr =  6.0247e-03\n",
      "Epoch 201/500: Loss =  2.3404, Score =  0.6164, lr =  6.0247e-03\n",
      "Epoch 202/500: Loss =  2.3415, Score =  0.6156, lr =  6.0247e-03\n",
      "Epoch 203/500: Loss =  2.3241, Score =  0.6171, lr =  6.0247e-03\n",
      "Epoch 204/500: Loss =  2.3035, Score =  0.6202, lr =  6.0247e-03\n",
      "Epoch 205/500: Loss =  2.2960, Score =  0.6210, lr =  6.0247e-03\n",
      "Early stopped.\n",
      "Training complete in 24m 17s\n",
      "Best val Acc: 0.628852\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/500: Loss =  3.1767, Score =  0.4535, lr =  1.5000e-03\n",
      "Epoch   2/500: Loss =  2.4753, Score =  0.5842, lr =  1.5000e-03\n",
      "Epoch   3/500: Loss =  2.1438, Score =  0.6337, lr =  1.5000e-03\n",
      "Epoch   4/500: Loss =  1.9381, Score =  0.6594, lr =  1.5000e-03\n",
      "Epoch   5/500: Loss =  1.8811, Score =  0.6717, lr =  1.5000e-03\n",
      "Epoch   6/500: Loss =  1.7404, Score =  0.6950, lr =  1.5000e-03\n",
      "Epoch   7/500: Loss =  1.7356, Score =  0.6975, lr =  1.5000e-03\n",
      "Epoch   8/500: Loss =  1.6745, Score =  0.7102, lr =  1.5000e-03\n",
      "Epoch   9/500: Loss =  1.6809, Score =  0.7092, lr =  1.5000e-03\n",
      "Epoch  10/500: Loss =  1.6493, Score =  0.7147, lr =  1.5000e-03\n",
      "Epoch  11/500: Loss =  1.6686, Score =  0.7118, lr =  1.5000e-03\n",
      "Epoch  12/500: Loss =  1.6954, Score =  0.7067, lr =  1.5000e-03\n",
      "Epoch  13/500: Loss =  1.6518, Score =  0.7145, lr =  1.5000e-03\n",
      "Epoch  14/500: Loss =  1.6930, Score =  0.7058, lr =  1.5000e-03\n",
      "Epoch  15/500: Loss =  1.6548, Score =  0.7099, lr =  1.5000e-03\n",
      "Epoch  16/500: Loss =  1.6419, Score =  0.7125, lr =  1.5000e-03\n",
      "Epoch  17/500: Loss =  1.6828, Score =  0.7057, lr =  1.5000e-03\n",
      "Epoch  18/500: Loss =  1.6264, Score =  0.7144, lr =  1.5000e-03\n",
      "Epoch  19/500: Loss =  1.6092, Score =  0.7183, lr =  1.5000e-03\n",
      "Epoch  20/500: Loss =  1.6077, Score =  0.7188, lr =  1.5000e-03\n",
      "Epoch  21/500: Loss =  1.6002, Score =  0.7213, lr =  1.5000e-03\n",
      "Epoch  22/500: Loss =  1.6140, Score =  0.7200, lr =  1.5000e-03\n",
      "Epoch  23/500: Loss =  1.6046, Score =  0.7210, lr =  1.5000e-03\n",
      "Epoch  24/500: Loss =  1.5795, Score =  0.7258, lr =  1.5000e-03\n",
      "Epoch  25/500: Loss =  1.5609, Score =  0.7260, lr =  1.5000e-03\n",
      "Epoch  26/500: Loss =  1.5730, Score =  0.7252, lr =  1.5000e-03\n",
      "Epoch  27/500: Loss =  1.5332, Score =  0.7300, lr =  1.5000e-03\n",
      "Epoch  28/500: Loss =  1.5629, Score =  0.7261, lr =  1.5000e-03\n",
      "Epoch  29/500: Loss =  1.5533, Score =  0.7272, lr =  1.5000e-03\n",
      "Epoch  30/500: Loss =  1.5085, Score =  0.7338, lr =  1.5000e-03\n",
      "Epoch  31/500: Loss =  1.5233, Score =  0.7321, lr =  1.5000e-03\n",
      "Epoch  32/500: Loss =  1.5336, Score =  0.7298, lr =  1.5000e-03\n",
      "Epoch  33/500: Loss =  1.5026, Score =  0.7354, lr =  1.5000e-03\n",
      "Epoch  34/500: Loss =  1.5296, Score =  0.7323, lr =  1.5000e-03\n",
      "Epoch  35/500: Loss =  1.5459, Score =  0.7279, lr =  1.5000e-03\n",
      "Epoch  36/500: Loss =  1.5505, Score =  0.7263, lr =  1.5000e-03\n",
      "Epoch  37/500: Loss =  1.5492, Score =  0.7268, lr =  1.5000e-03\n",
      "Epoch  38/500: Loss =  1.5415, Score =  0.7280, lr =  1.5000e-03\n",
      "Epoch  39/500: Loss =  1.5520, Score =  0.7275, lr =  1.5000e-03\n",
      "Epoch  40/500: Loss =  1.5552, Score =  0.7264, lr =  1.5000e-03\n",
      "Epoch  41/500: Loss =  1.5956, Score =  0.7190, lr =  1.5000e-03\n",
      "Epoch  42/500: Loss =  1.5595, Score =  0.7257, lr =  1.5000e-03\n",
      "Epoch  43/500: Loss =  1.5717, Score =  0.7254, lr =  1.5000e-03\n",
      "Epoch  44/500: Loss =  1.5376, Score =  0.7306, lr =  1.5000e-03\n",
      "Epoch  45/500: Loss =  1.5662, Score =  0.7271, lr =  1.5000e-03\n",
      "Epoch  46/500: Loss =  1.5497, Score =  0.7293, lr =  1.5000e-03\n",
      "Epoch  47/500: Loss =  1.5533, Score =  0.7294, lr =  1.5000e-03\n",
      "Epoch  48/500: Loss =  1.5509, Score =  0.7279, lr =  1.5000e-03\n",
      "Epoch  49/500: Loss =  1.5537, Score =  0.7274, lr =  1.5000e-03\n",
      "Epoch  50/500: Loss =  1.5389, Score =  0.7300, lr =  1.5000e-03\n",
      "Epoch  51/500: Loss =  1.5236, Score =  0.7334, lr =  1.5000e-03\n",
      "Epoch  52/500: Loss =  1.5341, Score =  0.7312, lr =  1.5000e-03\n",
      "Epoch  53/500: Loss =  1.5379, Score =  0.7313, lr =  1.5000e-03\n",
      "Epoch  54/500: Loss =  1.5344, Score =  0.7317, lr =  1.5000e-03\n",
      "Epoch  55/500: Loss =  1.5198, Score =  0.7334, lr =  1.5000e-03\n",
      "Epoch  56/500: Loss =  1.5093, Score =  0.7345, lr =  1.5000e-03\n",
      "Epoch  57/500: Loss =  1.5124, Score =  0.7341, lr =  1.5000e-03\n",
      "Epoch  58/500: Loss =  1.5109, Score =  0.7353, lr =  1.5000e-03\n",
      "Epoch  59/500: Loss =  1.5387, Score =  0.7311, lr =  1.5000e-03\n",
      "Epoch  60/500: Loss =  1.5383, Score =  0.7293, lr =  1.5000e-03\n",
      "Epoch  61/500: Loss =  1.5301, Score =  0.7303, lr =  1.5000e-03\n",
      "Epoch  62/500: Loss =  1.5129, Score =  0.7323, lr =  1.5000e-03\n",
      "Epoch  63/500: Loss =  1.5085, Score =  0.7337, lr =  1.5000e-03\n",
      "Epoch  64/500: Loss =  1.4930, Score =  0.7366, lr =  1.5000e-03\n",
      "Epoch  65/500: Loss =  1.4966, Score =  0.7359, lr =  1.5000e-03\n",
      "Epoch  66/500: Loss =  1.5143, Score =  0.7342, lr =  1.5000e-03\n",
      "Epoch  67/500: Loss =  1.5256, Score =  0.7329, lr =  1.5000e-03\n",
      "Epoch  68/500: Loss =  1.5200, Score =  0.7341, lr =  1.5000e-03\n",
      "Epoch  69/500: Loss =  1.5345, Score =  0.7318, lr =  1.5000e-03\n",
      "Epoch  70/500: Loss =  1.5209, Score =  0.7334, lr =  1.5000e-03\n",
      "Epoch  71/500: Loss =  1.5007, Score =  0.7347, lr =  1.5000e-03\n",
      "Epoch  72/500: Loss =  1.5084, Score =  0.7333, lr =  1.5000e-03\n",
      "Epoch  73/500: Loss =  1.4987, Score =  0.7353, lr =  1.5000e-03\n",
      "Epoch  74/500: Loss =  1.4943, Score =  0.7363, lr =  1.5000e-03\n",
      "Epoch  75/500: Loss =  1.5125, Score =  0.7335, lr =  1.5000e-03\n",
      "Epoch  76/500: Loss =  1.5117, Score =  0.7333, lr =  1.5000e-03\n",
      "Epoch  77/500: Loss =  1.5192, Score =  0.7315, lr =  1.5000e-03\n",
      "Epoch  78/500: Loss =  1.5404, Score =  0.7279, lr =  1.5000e-03\n",
      "Epoch  79/500: Loss =  1.5312, Score =  0.7297, lr =  1.5000e-03\n",
      "Epoch  80/500: Loss =  1.5046, Score =  0.7338, lr =  1.5000e-03\n",
      "Epoch  81/500: Loss =  1.5190, Score =  0.7321, lr =  1.5000e-03\n",
      "Epoch  82/500: Loss =  1.5130, Score =  0.7322, lr =  1.5000e-03\n",
      "Epoch  83/500: Loss =  1.5006, Score =  0.7349, lr =  1.5000e-03\n",
      "Epoch  84/500: Loss =  1.4987, Score =  0.7357, lr =  1.5000e-03\n",
      "Epoch  85/500: Loss =  1.4877, Score =  0.7367, lr =  1.5000e-03\n",
      "Epoch  86/500: Loss =  1.5073, Score =  0.7330, lr =  1.5000e-03\n",
      "Epoch  87/500: Loss =  1.4961, Score =  0.7347, lr =  1.5000e-03\n",
      "Epoch  88/500: Loss =  1.5061, Score =  0.7328, lr =  1.5000e-03\n",
      "Epoch  89/500: Loss =  1.5133, Score =  0.7310, lr =  1.5000e-03\n",
      "Epoch  90/500: Loss =  1.5006, Score =  0.7325, lr =  1.5000e-03\n",
      "Epoch  91/500: Loss =  1.5006, Score =  0.7344, lr =  1.5000e-03\n",
      "Epoch  92/500: Loss =  1.4868, Score =  0.7361, lr =  1.5000e-03\n",
      "Epoch  93/500: Loss =  1.4883, Score =  0.7365, lr =  1.5000e-03\n",
      "Epoch  94/500: Loss =  1.4771, Score =  0.7380, lr =  1.5000e-03\n",
      "Epoch  95/500: Loss =  1.4946, Score =  0.7346, lr =  1.5000e-03\n",
      "Epoch  96/500: Loss =  1.4934, Score =  0.7351, lr =  1.5000e-03\n",
      "Epoch  97/500: Loss =  1.4956, Score =  0.7349, lr =  1.5000e-03\n",
      "Epoch  98/500: Loss =  1.4884, Score =  0.7360, lr =  1.5000e-03\n",
      "Epoch  99/500: Loss =  1.4898, Score =  0.7360, lr =  1.5000e-03\n",
      "Epoch 100/500: Loss =  1.4844, Score =  0.7371, lr =  1.5000e-03\n",
      "Epoch 101/500: Loss =  1.4935, Score =  0.7367, lr =  1.5000e-03\n",
      "Epoch 102/500: Loss =  1.4893, Score =  0.7363, lr =  1.5000e-03\n",
      "Epoch 103/500: Loss =  1.4804, Score =  0.7380, lr =  1.5000e-03\n",
      "Epoch 104/500: Loss =  1.4928, Score =  0.7361, lr =  1.5000e-03\n",
      "Epoch 105/500: Loss =  1.4919, Score =  0.7364, lr =  1.5000e-03\n",
      "Epoch 106/500: Loss =  1.4678, Score =  0.7405, lr =  1.5000e-03\n",
      "Epoch 107/500: Loss =  1.4681, Score =  0.7398, lr =  1.5000e-03\n",
      "Epoch 108/500: Loss =  1.4764, Score =  0.7380, lr =  1.5000e-03\n",
      "Epoch 109/500: Loss =  1.4668, Score =  0.7396, lr =  1.5000e-03\n",
      "Epoch 110/500: Loss =  1.4607, Score =  0.7410, lr =  1.5000e-03\n",
      "Epoch 111/500: Loss =  1.4656, Score =  0.7397, lr =  1.5000e-03\n",
      "Epoch 112/500: Loss =  1.4705, Score =  0.7395, lr =  1.5000e-03\n",
      "Epoch 113/500: Loss =  1.4764, Score =  0.7378, lr =  1.5000e-03\n",
      "Epoch 114/500: Loss =  1.4836, Score =  0.7364, lr =  1.5000e-03\n",
      "Epoch 115/500: Loss =  1.4986, Score =  0.7346, lr =  1.5000e-03\n",
      "Epoch 116/500: Loss =  1.4743, Score =  0.7386, lr =  1.5000e-03\n",
      "Epoch 117/500: Loss =  1.4771, Score =  0.7379, lr =  1.5000e-03\n",
      "Epoch 118/500: Loss =  1.4719, Score =  0.7382, lr =  1.5000e-03\n",
      "Epoch 119/500: Loss =  1.4637, Score =  0.7392, lr =  1.5000e-03\n",
      "Epoch 120/500: Loss =  1.4675, Score =  0.7399, lr =  1.5000e-03\n",
      "Epoch 121/500: Loss =  1.4699, Score =  0.7397, lr =  1.5000e-03\n",
      "Epoch 122/500: Loss =  1.4876, Score =  0.7364, lr =  1.5000e-03\n",
      "Epoch 123/500: Loss =  1.4875, Score =  0.7361, lr =  1.5000e-03\n",
      "Epoch 124/500: Loss =  1.4673, Score =  0.7391, lr =  1.5000e-03\n",
      "Epoch 125/500: Loss =  1.4749, Score =  0.7391, lr =  1.5000e-03\n",
      "Epoch 126/500: Loss =  1.4976, Score =  0.7360, lr =  1.5000e-03\n",
      "Epoch 127/500: Loss =  1.4921, Score =  0.7369, lr =  1.5000e-03\n",
      "Epoch 128/500: Loss =  1.5015, Score =  0.7349, lr =  1.5000e-03\n",
      "Epoch 129/500: Loss =  1.5021, Score =  0.7340, lr =  1.5000e-03\n",
      "Epoch 130/500: Loss =  1.5001, Score =  0.7348, lr =  1.5000e-03\n",
      "Epoch 131/500: Loss =  1.4914, Score =  0.7361, lr =  1.5000e-03\n",
      "Epoch 132/500: Loss =  1.4937, Score =  0.7350, lr =  1.5000e-03\n",
      "Epoch 133/500: Loss =  1.4986, Score =  0.7341, lr =  1.5000e-03\n",
      "Epoch 134/500: Loss =  1.4790, Score =  0.7372, lr =  1.5000e-03\n",
      "Epoch 135/500: Loss =  1.4824, Score =  0.7366, lr =  1.5000e-03\n",
      "Epoch 136/500: Loss =  1.4804, Score =  0.7372, lr =  1.5000e-03\n",
      "Epoch 137/500: Loss =  1.4680, Score =  0.7388, lr =  1.5000e-03\n",
      "Epoch 138/500: Loss =  1.4790, Score =  0.7379, lr =  1.5000e-03\n",
      "Epoch 139/500: Loss =  1.4649, Score =  0.7397, lr =  1.5000e-03\n",
      "Epoch 140/500: Loss =  1.4655, Score =  0.7386, lr =  1.5000e-03\n",
      "Epoch 141/500: Loss =  1.4822, Score =  0.7361, lr =  1.5000e-03\n",
      "Epoch 142/500: Loss =  1.4951, Score =  0.7339, lr =  1.5000e-03\n",
      "Epoch 143/500: Loss =  1.5057, Score =  0.7328, lr =  1.5000e-03\n",
      "Epoch 144/500: Loss =  1.5054, Score =  0.7330, lr =  1.5000e-03\n",
      "Epoch 145/500: Loss =  1.4914, Score =  0.7341, lr =  1.5000e-03\n",
      "Epoch 146/500: Loss =  1.5024, Score =  0.7322, lr =  1.5000e-03\n",
      "Epoch 147/500: Loss =  1.5056, Score =  0.7318, lr =  1.5000e-03\n",
      "Epoch 148/500: Loss =  1.4993, Score =  0.7324, lr =  1.5000e-03\n",
      "Epoch 149/500: Loss =  1.4816, Score =  0.7345, lr =  1.5000e-03\n",
      "Epoch 150/500: Loss =  1.4746, Score =  0.7364, lr =  1.5000e-03\n",
      "Early stopped.\n",
      "Training complete in 30m 37s\n",
      "Best val Acc: 0.741034\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/500: Loss =  6.8099, Score = -0.0350, lr =  6.0247e-03\n",
      "Epoch   2/500: Loss =  6.4870, Score =  0.0106, lr =  6.0247e-03\n",
      "Epoch   3/500: Loss =  5.8753, Score =  0.1012, lr =  6.0247e-03\n",
      "Epoch   4/500: Loss =  5.2111, Score =  0.1981, lr =  6.0247e-03\n",
      "Epoch   5/500: Loss =  4.6241, Score =  0.2813, lr =  6.0247e-03\n",
      "Epoch   6/500: Loss =  4.1814, Score =  0.3434, lr =  6.0247e-03\n",
      "Epoch   7/500: Loss =  3.8879, Score =  0.3854, lr =  6.0247e-03\n",
      "Epoch   8/500: Loss =  3.6571, Score =  0.4186, lr =  6.0247e-03\n",
      "Epoch   9/500: Loss =  3.4483, Score =  0.4481, lr =  6.0247e-03\n",
      "Epoch  10/500: Loss =  3.2598, Score =  0.4739, lr =  6.0247e-03\n",
      "Epoch  11/500: Loss =  3.0893, Score =  0.4972, lr =  6.0247e-03\n",
      "Epoch  12/500: Loss =  2.9730, Score =  0.5126, lr =  6.0247e-03\n",
      "Epoch  13/500: Loss =  2.8841, Score =  0.5252, lr =  6.0247e-03\n",
      "Epoch  14/500: Loss =  2.8166, Score =  0.5354, lr =  6.0247e-03\n",
      "Epoch  15/500: Loss =  2.7720, Score =  0.5435, lr =  6.0247e-03\n",
      "Epoch  16/500: Loss =  2.7530, Score =  0.5483, lr =  6.0247e-03\n",
      "Epoch  17/500: Loss =  2.7442, Score =  0.5507, lr =  6.0247e-03\n",
      "Epoch  18/500: Loss =  2.7305, Score =  0.5532, lr =  6.0247e-03\n",
      "Epoch  19/500: Loss =  2.6969, Score =  0.5586, lr =  6.0247e-03\n",
      "Epoch  20/500: Loss =  2.6905, Score =  0.5590, lr =  6.0247e-03\n",
      "Epoch  21/500: Loss =  2.7025, Score =  0.5564, lr =  6.0247e-03\n",
      "Epoch  22/500: Loss =  2.7123, Score =  0.5548, lr =  6.0247e-03\n",
      "Epoch  23/500: Loss =  2.7060, Score =  0.5557, lr =  6.0247e-03\n",
      "Epoch  24/500: Loss =  2.6963, Score =  0.5564, lr =  6.0247e-03\n",
      "Epoch  25/500: Loss =  2.7054, Score =  0.5535, lr =  6.0247e-03\n",
      "Epoch  26/500: Loss =  2.6964, Score =  0.5543, lr =  6.0247e-03\n",
      "Epoch  27/500: Loss =  2.7145, Score =  0.5512, lr =  6.0247e-03\n",
      "Epoch  28/500: Loss =  2.7430, Score =  0.5471, lr =  6.0247e-03\n",
      "Epoch  29/500: Loss =  2.7690, Score =  0.5438, lr =  6.0247e-03\n",
      "Epoch  30/500: Loss =  2.7713, Score =  0.5439, lr =  6.0247e-03\n",
      "Epoch  31/500: Loss =  2.7648, Score =  0.5451, lr =  6.0247e-03\n",
      "Epoch  32/500: Loss =  2.7673, Score =  0.5436, lr =  6.0247e-03\n",
      "Epoch  33/500: Loss =  2.7392, Score =  0.5482, lr =  6.0247e-03\n",
      "Epoch  34/500: Loss =  2.7066, Score =  0.5532, lr =  6.0247e-03\n",
      "Epoch  35/500: Loss =  2.6822, Score =  0.5580, lr =  6.0247e-03\n",
      "Epoch  36/500: Loss =  2.6975, Score =  0.5549, lr =  6.0247e-03\n",
      "Epoch  37/500: Loss =  2.6698, Score =  0.5589, lr =  6.0247e-03\n",
      "Epoch  38/500: Loss =  2.6455, Score =  0.5627, lr =  6.0247e-03\n",
      "Epoch  39/500: Loss =  2.6013, Score =  0.5700, lr =  6.0247e-03\n",
      "Epoch  40/500: Loss =  2.5716, Score =  0.5741, lr =  6.0247e-03\n",
      "Epoch  41/500: Loss =  2.5597, Score =  0.5751, lr =  6.0247e-03\n",
      "Epoch  42/500: Loss =  2.5413, Score =  0.5770, lr =  6.0247e-03\n",
      "Epoch  43/500: Loss =  2.5208, Score =  0.5806, lr =  6.0247e-03\n",
      "Epoch  44/500: Loss =  2.4838, Score =  0.5881, lr =  6.0247e-03\n",
      "Epoch  45/500: Loss =  2.4841, Score =  0.5889, lr =  6.0247e-03\n",
      "Epoch  46/500: Loss =  2.4761, Score =  0.5907, lr =  6.0247e-03\n",
      "Epoch  47/500: Loss =  2.4838, Score =  0.5901, lr =  6.0247e-03\n",
      "Epoch  48/500: Loss =  2.5150, Score =  0.5851, lr =  6.0247e-03\n",
      "Epoch  49/500: Loss =  2.5284, Score =  0.5823, lr =  6.0247e-03\n",
      "Epoch  50/500: Loss =  2.5390, Score =  0.5791, lr =  6.0247e-03\n",
      "Epoch  51/500: Loss =  2.5366, Score =  0.5790, lr =  6.0247e-03\n",
      "Epoch  52/500: Loss =  2.5341, Score =  0.5788, lr =  6.0247e-03\n",
      "Epoch  53/500: Loss =  2.5447, Score =  0.5775, lr =  6.0247e-03\n",
      "Epoch  54/500: Loss =  2.5489, Score =  0.5775, lr =  6.0247e-03\n",
      "Epoch  55/500: Loss =  2.5189, Score =  0.5833, lr =  6.0247e-03\n",
      "Epoch  56/500: Loss =  2.5028, Score =  0.5870, lr =  6.0247e-03\n",
      "Epoch  57/500: Loss =  2.5129, Score =  0.5862, lr =  6.0247e-03\n",
      "Epoch  58/500: Loss =  2.5091, Score =  0.5872, lr =  6.0247e-03\n",
      "Epoch  59/500: Loss =  2.5068, Score =  0.5876, lr =  6.0247e-03\n",
      "Epoch  60/500: Loss =  2.5108, Score =  0.5864, lr =  6.0247e-03\n",
      "Epoch  61/500: Loss =  2.5185, Score =  0.5826, lr =  6.0247e-03\n",
      "Epoch  62/500: Loss =  2.5327, Score =  0.5783, lr =  6.0247e-03\n",
      "Epoch  63/500: Loss =  2.5060, Score =  0.5835, lr =  6.0247e-03\n",
      "Epoch  64/500: Loss =  2.4923, Score =  0.5862, lr =  6.0247e-03\n",
      "Epoch  65/500: Loss =  2.4703, Score =  0.5904, lr =  6.0247e-03\n",
      "Epoch  66/500: Loss =  2.4681, Score =  0.5905, lr =  6.0247e-03\n",
      "Epoch  67/500: Loss =  2.4711, Score =  0.5905, lr =  6.0247e-03\n",
      "Epoch  68/500: Loss =  2.4660, Score =  0.5929, lr =  6.0247e-03\n",
      "Epoch  69/500: Loss =  2.4350, Score =  0.5991, lr =  6.0247e-03\n",
      "Epoch  70/500: Loss =  2.3863, Score =  0.6070, lr =  6.0247e-03\n",
      "Epoch  71/500: Loss =  2.3914, Score =  0.6057, lr =  6.0247e-03\n",
      "Epoch  72/500: Loss =  2.3921, Score =  0.6056, lr =  6.0247e-03\n",
      "Epoch  73/500: Loss =  2.3901, Score =  0.6061, lr =  6.0247e-03\n",
      "Epoch  74/500: Loss =  2.3721, Score =  0.6096, lr =  6.0247e-03\n",
      "Epoch  75/500: Loss =  2.3803, Score =  0.6091, lr =  6.0247e-03\n",
      "Epoch  76/500: Loss =  2.3813, Score =  0.6091, lr =  6.0247e-03\n",
      "Epoch  77/500: Loss =  2.3997, Score =  0.6060, lr =  6.0247e-03\n",
      "Epoch  78/500: Loss =  2.3822, Score =  0.6092, lr =  6.0247e-03\n",
      "Epoch  79/500: Loss =  2.3881, Score =  0.6082, lr =  6.0247e-03\n",
      "Epoch  80/500: Loss =  2.3992, Score =  0.6064, lr =  6.0247e-03\n",
      "Epoch  81/500: Loss =  2.4175, Score =  0.6027, lr =  6.0247e-03\n",
      "Epoch  82/500: Loss =  2.4230, Score =  0.6019, lr =  6.0247e-03\n",
      "Epoch  83/500: Loss =  2.4249, Score =  0.6016, lr =  6.0247e-03\n",
      "Epoch  84/500: Loss =  2.4080, Score =  0.6039, lr =  6.0247e-03\n",
      "Epoch  85/500: Loss =  2.4022, Score =  0.6055, lr =  6.0247e-03\n",
      "Epoch  86/500: Loss =  2.4134, Score =  0.6041, lr =  6.0247e-03\n",
      "Epoch  87/500: Loss =  2.4272, Score =  0.6027, lr =  6.0247e-03\n",
      "Epoch  88/500: Loss =  2.4248, Score =  0.6036, lr =  6.0247e-03\n",
      "Epoch  89/500: Loss =  2.4088, Score =  0.6073, lr =  6.0247e-03\n",
      "Epoch  90/500: Loss =  2.3896, Score =  0.6103, lr =  6.0247e-03\n",
      "Epoch  91/500: Loss =  2.3840, Score =  0.6103, lr =  6.0247e-03\n",
      "Epoch  92/500: Loss =  2.3677, Score =  0.6117, lr =  6.0247e-03\n",
      "Epoch  93/500: Loss =  2.3417, Score =  0.6149, lr =  6.0247e-03\n",
      "Epoch  94/500: Loss =  2.3286, Score =  0.6164, lr =  6.0247e-03\n",
      "Epoch  95/500: Loss =  2.3463, Score =  0.6133, lr =  6.0247e-03\n",
      "Epoch  96/500: Loss =  2.3700, Score =  0.6093, lr =  6.0247e-03\n",
      "Epoch  97/500: Loss =  2.3840, Score =  0.6066, lr =  6.0247e-03\n",
      "Epoch  98/500: Loss =  2.3847, Score =  0.6060, lr =  6.0247e-03\n",
      "Epoch  99/500: Loss =  2.3645, Score =  0.6092, lr =  6.0247e-03\n",
      "Epoch 100/500: Loss =  2.3600, Score =  0.6096, lr =  6.0247e-03\n",
      "Epoch 101/500: Loss =  2.3647, Score =  0.6082, lr =  6.0247e-03\n",
      "Epoch 102/500: Loss =  2.3660, Score =  0.6071, lr =  6.0247e-03\n",
      "Epoch 103/500: Loss =  2.3508, Score =  0.6096, lr =  6.0247e-03\n",
      "Epoch 104/500: Loss =  2.3356, Score =  0.6127, lr =  6.0247e-03\n",
      "Epoch 105/500: Loss =  2.3274, Score =  0.6140, lr =  6.0247e-03\n",
      "Epoch 106/500: Loss =  2.3271, Score =  0.6144, lr =  6.0247e-03\n",
      "Epoch 107/500: Loss =  2.3599, Score =  0.6093, lr =  6.0247e-03\n",
      "Epoch 108/500: Loss =  2.3951, Score =  0.6035, lr =  6.0247e-03\n",
      "Epoch 109/500: Loss =  2.4519, Score =  0.5950, lr =  6.0247e-03\n",
      "Epoch 110/500: Loss =  2.4939, Score =  0.5878, lr =  6.0247e-03\n",
      "Epoch 111/500: Loss =  2.5167, Score =  0.5840, lr =  6.0247e-03\n",
      "Epoch 112/500: Loss =  2.5272, Score =  0.5820, lr =  6.0247e-03\n",
      "Epoch 113/500: Loss =  2.5209, Score =  0.5824, lr =  6.0247e-03\n",
      "Epoch 114/500: Loss =  2.5153, Score =  0.5821, lr =  6.0247e-03\n",
      "Epoch 115/500: Loss =  2.4894, Score =  0.5862, lr =  6.0247e-03\n",
      "Epoch 116/500: Loss =  2.4643, Score =  0.5906, lr =  6.0247e-03\n",
      "Epoch 117/500: Loss =  2.4377, Score =  0.5951, lr =  6.0247e-03\n",
      "Epoch 118/500: Loss =  2.4433, Score =  0.5951, lr =  6.0247e-03\n",
      "Epoch 119/500: Loss =  2.4413, Score =  0.5954, lr =  6.0247e-03\n",
      "Epoch 120/500: Loss =  2.4323, Score =  0.5969, lr =  6.0247e-03\n",
      "Epoch 121/500: Loss =  2.4199, Score =  0.5978, lr =  6.0247e-03\n",
      "Epoch 122/500: Loss =  2.4096, Score =  0.5984, lr =  6.0247e-03\n",
      "Epoch 123/500: Loss =  2.3869, Score =  0.6016, lr =  6.0247e-03\n",
      "Epoch 124/500: Loss =  2.3661, Score =  0.6048, lr =  6.0247e-03\n",
      "Epoch 125/500: Loss =  2.3522, Score =  0.6074, lr =  6.0247e-03\n",
      "Epoch 126/500: Loss =  2.3445, Score =  0.6089, lr =  6.0247e-03\n",
      "Epoch 127/500: Loss =  2.3557, Score =  0.6068, lr =  6.0247e-03\n",
      "Epoch 128/500: Loss =  2.3824, Score =  0.6019, lr =  6.0247e-03\n",
      "Epoch 129/500: Loss =  2.3918, Score =  0.6009, lr =  6.0247e-03\n",
      "Epoch 130/500: Loss =  2.4286, Score =  0.5949, lr =  6.0247e-03\n",
      "Epoch 131/500: Loss =  2.4513, Score =  0.5921, lr =  6.0247e-03\n",
      "Epoch 132/500: Loss =  2.4484, Score =  0.5935, lr =  6.0247e-03\n",
      "Epoch 133/500: Loss =  2.4446, Score =  0.5947, lr =  6.0247e-03\n",
      "Epoch 134/500: Loss =  2.4621, Score =  0.5925, lr =  6.0247e-03\n",
      "Epoch 135/500: Loss =  2.4583, Score =  0.5936, lr =  6.0247e-03\n",
      "Epoch 136/500: Loss =  2.4651, Score =  0.5928, lr =  6.0247e-03\n",
      "Epoch 137/500: Loss =  2.4850, Score =  0.5895, lr =  6.0247e-03\n",
      "Epoch 138/500: Loss =  2.4486, Score =  0.5958, lr =  6.0247e-03\n",
      "Epoch 139/500: Loss =  2.4331, Score =  0.5984, lr =  6.0247e-03\n",
      "Epoch 140/500: Loss =  2.4120, Score =  0.6011, lr =  6.0247e-03\n",
      "Epoch 141/500: Loss =  2.3726, Score =  0.6075, lr =  6.0247e-03\n",
      "Epoch 142/500: Loss =  2.3297, Score =  0.6149, lr =  6.0247e-03\n",
      "Epoch 143/500: Loss =  2.3150, Score =  0.6183, lr =  6.0247e-03\n",
      "Epoch 144/500: Loss =  2.3087, Score =  0.6201, lr =  6.0247e-03\n",
      "Epoch 145/500: Loss =  2.3046, Score =  0.6216, lr =  6.0247e-03\n",
      "Epoch 146/500: Loss =  2.3204, Score =  0.6191, lr =  6.0247e-03\n",
      "Epoch 147/500: Loss =  2.3472, Score =  0.6143, lr =  6.0247e-03\n",
      "Epoch 148/500: Loss =  2.3583, Score =  0.6117, lr =  6.0247e-03\n",
      "Epoch 149/500: Loss =  2.3806, Score =  0.6066, lr =  6.0247e-03\n",
      "Epoch 150/500: Loss =  2.3682, Score =  0.6082, lr =  6.0247e-03\n",
      "Epoch 151/500: Loss =  2.3685, Score =  0.6087, lr =  6.0247e-03\n",
      "Epoch 152/500: Loss =  2.3758, Score =  0.6083, lr =  6.0247e-03\n",
      "Epoch 153/500: Loss =  2.3997, Score =  0.6051, lr =  6.0247e-03\n",
      "Epoch 154/500: Loss =  2.4048, Score =  0.6048, lr =  6.0247e-03\n",
      "Epoch 155/500: Loss =  2.4115, Score =  0.6038, lr =  6.0247e-03\n",
      "Epoch 156/500: Loss =  2.4107, Score =  0.6031, lr =  6.0247e-03\n",
      "Epoch 157/500: Loss =  2.4253, Score =  0.5998, lr =  6.0247e-03\n",
      "Epoch 158/500: Loss =  2.4262, Score =  0.5986, lr =  6.0247e-03\n",
      "Epoch 159/500: Loss =  2.4226, Score =  0.5989, lr =  6.0247e-03\n",
      "Epoch 160/500: Loss =  2.4224, Score =  0.5974, lr =  6.0247e-03\n",
      "Epoch 161/500: Loss =  2.4195, Score =  0.5973, lr =  6.0247e-03\n",
      "Epoch 162/500: Loss =  2.4082, Score =  0.5992, lr =  6.0247e-03\n",
      "Epoch 163/500: Loss =  2.4215, Score =  0.5971, lr =  6.0247e-03\n",
      "Epoch 164/500: Loss =  2.4413, Score =  0.5945, lr =  6.0247e-03\n",
      "Epoch 165/500: Loss =  2.4857, Score =  0.5879, lr =  6.0247e-03\n",
      "Epoch 166/500: Loss =  2.5089, Score =  0.5855, lr =  6.0247e-03\n",
      "Epoch 167/500: Loss =  2.5176, Score =  0.5853, lr =  6.0247e-03\n",
      "Epoch 168/500: Loss =  2.5357, Score =  0.5827, lr =  6.0247e-03\n",
      "Epoch 169/500: Loss =  2.5291, Score =  0.5839, lr =  6.0247e-03\n",
      "Epoch 170/500: Loss =  2.5157, Score =  0.5858, lr =  6.0247e-03\n",
      "Epoch 171/500: Loss =  2.5113, Score =  0.5854, lr =  6.0247e-03\n",
      "Epoch 172/500: Loss =  2.5223, Score =  0.5825, lr =  6.0247e-03\n",
      "Epoch 173/500: Loss =  2.5224, Score =  0.5820, lr =  6.0247e-03\n",
      "Epoch 174/500: Loss =  2.5249, Score =  0.5814, lr =  6.0247e-03\n",
      "Epoch 175/500: Loss =  2.5187, Score =  0.5827, lr =  6.0247e-03\n",
      "Epoch 176/500: Loss =  2.5016, Score =  0.5856, lr =  6.0247e-03\n",
      "Epoch 177/500: Loss =  2.4842, Score =  0.5886, lr =  6.0247e-03\n",
      "Epoch 178/500: Loss =  2.4847, Score =  0.5887, lr =  6.0247e-03\n",
      "Epoch 179/500: Loss =  2.4743, Score =  0.5904, lr =  6.0247e-03\n",
      "Epoch 180/500: Loss =  2.4884, Score =  0.5880, lr =  6.0247e-03\n",
      "Epoch 181/500: Loss =  2.5009, Score =  0.5855, lr =  6.0247e-03\n",
      "Epoch 182/500: Loss =  2.5098, Score =  0.5831, lr =  6.0247e-03\n",
      "Epoch 183/500: Loss =  2.5204, Score =  0.5801, lr =  6.0247e-03\n",
      "Epoch 184/500: Loss =  2.4972, Score =  0.5837, lr =  6.0247e-03\n",
      "Epoch 185/500: Loss =  2.4663, Score =  0.5881, lr =  6.0247e-03\n",
      "Early stopped.\n",
      "Training complete in 21m 50s\n",
      "Best val Acc: 0.621567\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEICAYAAABPgw/pAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAApQUlEQVR4nO3de5xcdX3/8dcns/dbsslubru5kmAuQCBsuKtYRC4KiAUNaAWpUFS8tL9asbZKa21LrYoWSopIkSqgLahRgih4QQkxNwiQK0tuu9lNstn7fWd2Pr8/5iROlt1kkuxmdmbez8djHjvnMmc+c2b2Pd/5npu5OyIikvrGJLsAEREZHgp0EZE0oUAXEUkTCnQRkTShQBcRSRMKdBGRNKFAl7RmZneZ2feC+9PNrMPMQsmu60SZmZvZnATmu9jMak9GTZJ8CnR5EzPbaWbvTHDe35jZR0e6puHg7rvdvcjd+482b6YEoZn92swazKzNzDaY2TXJrkmOX1ayC5DMZmahRAJWRsyngU3uHjGzc4FnzexUd69PdmFy7NRClyMys5vN7Pdm9u9m1mxmO8zsimDaV4C3AvcGXRn3BuPnmdkvzazJzLaa2fvjlvewmd1vZivMrBN4R/CL4LNm9oqZdZrZd8xskpk9bWbtZvasmZXGLeM8M1tpZi1Bq/LiuGmzzOy3weN+CZTFTZsZdFVkBcMfMbPNwbzbzewvgvGFwNPA1OB1dZjZVDMbY2Z3mtkbZtZoZj80s/FDrLeLzazWzP7GzPabWb2ZvdfMrjSzbcG6+du4+XPN7B4zqwtu95hZbtz0zwbLqDOzWwY8V27w/uw2s31mtszM8hN5f939FXePHBwEsoFpiTxWRiF31023w27ATuCdwf2bgTBwKxACPgbUARZM/w3w0bjHFgI1wEeI/QJcDBwAFgbTHwZagQuJNSjygudbBUwCKoD9wHrgLCAX+BXwpeDxFUAjcGXw+EuD4fJg+ovA14PHvQ1oB74XTJtJLLSyguF3A6cABrwd6AIWB9MuBmoHrJfPBHVWBsv/L+CxIdbhxUAE+CKxkLwVaAAeBYqBhUAPMDuY/x+DZU8EyoGVwJeDaZcD+4DTgvX7aPA65gTT7wGWA+ODZf8U+JehXscgtf4sqMWBnwNjkv0Z1O04/3eTXYBuo+82SKBXx00rCP7xJwfDAwP9A8DvBizvv+IC+WHgkUGe74Nxw08A98cNfxL4cXD/c8D/DHj8M8BNwPQgRAvjpj06VKAP8rp/DHw6uD9YoG8GLokbnkLsy+5Nywse3w2EguHi4LnPjZtnHfDe4P4bwJVx0y4Ddgb3HwL+NW7aqQcDndiXUSdwStz084EdQ72OIV57NnAF8JfJ/vzpdvw39aFLIvYevOPuXWYGUDTEvDOAc82sJW5cFvA/ccM1gzxuX9z97kGGDz7fDOB6M7sqbno28GtgKtDs7p1x03YxRBdC0HX0JWIBOYbYl9Wrg76qPz73j8wsGjeun9gviz2DzN/of9w+0B38Hep1TQ1qja97aty0dQOmHVQe1L0ueF8gFvLHtCePu4eBp83s02b2hrsvP5bHy+igQJcTNfB0nTXAb9390mN4zLGoIdZCv3XgBDObAZSaWWFcqE8f7PmC/ukngA8DP3H3sJn9mFgYDlVjDXCLu79wAvUPpY7YF8bGYHh6MA6gnsO/lKbH3T9A7IthobsP9qVyrLKIdUNJCtJGUTlR+4DZccM/A041sz8zs+zgtsTM5g/T830PuMrMLjOzkJnlBRsgK919F7AW+AczyzGzi4CrhlhODrF+8AYgErTW3zXgdU0ws7Fx45YBXwm+ODCz8mHcze8x4O+CZZYR63v/XjDth8DNZrbAzAqI/aoAwN2jwLeBb5jZxKCuCjO77GhPGGy8vsLM8oP36UPEtjv8dphek5xkCnQ5Ud8Ergv2gPmWu7cTC8alxFqYe4G7iYXnCXP3GuAa4G+JhXEN8Fn++Fm+ETgXaCIWfI8MsZx24FPEwrI5eNzyuOlbiIXs9mBvmqnBa10O/MLM2oltxDx3OF4X8E/EvoxeIdbtsz4Yh7s/TWzD56+A6uBvvM8F41eZWRvwLPCWBJ7TgLuIbYRuILYL4wfcff2JvRRJloN7KoiISIpTC11EJE0o0EVE0oQCXUQkTSjQRUTSRNL2Qy8rK/OZM2cm6+lFRFLSunXrDrh7+WDTkhboM2fOZO3atcl6ehGRlGRmu4aapi4XEZE0oUAXEUkTCnQRkTShQBcRSRMKdBGRNKFAFxFJEwp0EZE0oUAXkREVjTov17Tw4O+2s7GuNdnlpDVdsUhERsTm+jYe+v0Ofr11Pwc6+g6Nf/up5Xz9/YuYUDQsp8iXOAp0kTit3WEeWbmT32xroLU7zMfefgrXnlXBmDF29AcPoqM3wub6Nlq6wsycUMDcScXDXPHoE406X/vlVv7rt9vJzw5x8byJXDJvIounl/KzV+v45rOvc+1/ruTBm6o4NQPWx8mUtAtcVFVVuQ79l2Rr7Q7T3hOmsaOPVdsbeeD57TR19XFG5Tgi/VE21rWxZGYp37rhLKaMzU94uWt3NvHVZ7aydlcz/dHY/1h2yPjm0rO48vQpI/VyRoX7f/MGd/98C+9bXMHfv3sBpYU5h01fv7uZW7+7luauPt59xlT+/j3zmVicl6RqU4+ZrXP3qkGnKdAlWVq7wpTkZxF3tXpauvq4++dbeKG6kdLCHHKzxpCfHeLmC2ZSUZrPt557HXeYP6WYP5k3iflTig97fKL2t/Vwz3Ov84M1NYcCF2DJzFK+dNVCTqsYSzTqPLG+lruWbyQnawyfumQuS5dMJy97DM9u3s+3n9/OvvYeQmOMs6aVUjWzlMkleXxv1S6e27KfSSW5XH/2NBbPGEdpQQ7/9NRmXtrdzL9dt4jrzq4clnWYiGjUWbW9kedfP0BHb5irF1WwZGbpca23o1m7s4kPPLCKyxdO5t4bzxryOQ509PLg73bw8ModTCrJ4xsfOJPN9W1sb+ikpStMYW6IuROL+MCS6eRkaVNfvBMOdDO7nNj1FEPAg+7+r4PMczGx6x5mAwfc/e1HWqYCPbP9dlsDH/nv1YzNz2be5BJyssbQ1Rdh274OOnojXDJvIt3hfvoiUWqbu9nT0g1ASV4WpYU57G7qwh1OrxjLV68/g3mTSw5b/hsNHTy7aR/b9nVw5vRxXHjKBGaVFeIOP3ppD3f9dCM94X5uOGc6p00dS1FeFounlzJ57JtbitsbOvj8k6/yhx1N5ITGML4wh71tPcwuK+T0yrF09vazfnczTZ2xfuLSgmz+/KJZ3HLRLApy/tir2dUX4bZH1rHyjQP8xw2LefcZI9tSd3d++ko99/7qdbbt6yA7ZGSHxtDV18+fzJvIf9xwFoW5w9fruqelm2vve4G87BA/+9RFlORlH/Ux63Y1c8vDa2jtDgOQnx2itCCbzr5+WrvDzC4r5D1nTKEkP5sZEwo5pbyQ6eMLyAplbsifUKCbWQjYBlwK1AJrgBvcfVPcPOOAlcDl7r7bzCa6+/4jLVeBnrmaO/u47J7nKcrL4uzppew40Em4P0p+TojJJXnc+rbZLJw69tD8fZEoj6/ZTUN7Lx+5cBbjC3M40NHL06/t5ZvPbqO1O8z5p5QxfXw+RbnZrN/dzOodTQCMK8impSsWFuXFuXT0ROgO97NkZilfvW4RM8sKE6579Y4mntu8j/rWHs6aPo4PnTeD7CBY3J0dBzrZ2djJebMnHBbk8br6Itz00Gpe2t3Ckx+/gDMqxx3nWjyy1/a0ctfyjazd1cxbJhXzsYtP4dIFkzCD76/azb88vZn5U0q4/uxKLppbzpyJRSf0fI0dvXzwwT+wp7mbJz5+wTH1jW9v6OCFNxo5f/YETikvPNSq/83W/Xzlqc1UN3QQH1PZIePdp0/h81fOZ1JJ5nXVnGignw/c5e6XBcOfB3D3f4mb5+PAVHf/u0SLUqBnpmjU+fj31/Pcln386OMXclrF2KM/6AgaO3r5+i+3saG2hdrmbrp6+5kyLo8bz5nOVYumMmVsHjsbu3ih+gDrdjUzriCbM6eN4z1nTCV0nBs6T1Rrd5h3feO3lBbksPyOi064S8HdqW3upqapi5rmLtbsbOaJ9bWML8jhc5fP47qzK9+0Ufe5zfu488lXaWjvJTdrDI/ccg7nzp5wzM+9v72HB3+3g++t2kVfJMp/f2QJb5076Km6j1s06rT1hNnZ2MUb+zt4pbaFx9bUMMZg8fRSKkvz6ezrZ3ZZIefNnkBxXhZb9rbzq837mV1eyHVnVzK7/MS+sEaTEw3064i1vD8aDP8ZcK673xE3zz3EuloWAsXAN939kUGWdRtwG8D06dPP3rVryNP6Spr6ylOb+PbvdvCFK+dz69tmJ7ucpPnlpn3c+sha/urSU/nUJXMTflz1/g7+d10N3X2x7qi2njCrdzRzoKP30Dw5oTH82fkz+NQlcxmbP3S3x8Evgpv+ezUNbb08eFMV586egLuzq7GLDbUtnDqpmPlTSgj3R+mPOnnZoUOPv/83b/CNZ7cR6Y9y9aKpfOIdc07aXjy7Gjv5zu93sG5X7LUX5GSxq7GTuM0hTCrJ5UBHH/1R553zJ/FXl57KgqklQy80RZxooF8PXDYg0M9x90/GzXMvUAVcAuQDLwLvdvdtQy1XLfSRt7uxi77+KJWl+Yf9I54M7s6+tl76IlHC0SiNHX18b9Uulm+o48Pnz+Afrl44IhvlUsknH3uJFa/W84PbzqNq5vijzv/rLfv55GMv0RvppzA369AG4zMqx3Hu7PHMmlDItPEFTBmbd0x9zHtaunn/shfZ09LNmdPGsaelm4b2P35BzJxQQF1rD32RKBXj8rm+qpLSghy+tHwj71owib+9cv4xdV2NlNbuMK/taaUn3M/E4jxOqyihoaOXR/+wm4dX7qS/3/nRJy5gzsSR+dLpCfeTHRoz4r/8TkaXy51AnrvfFQx/B/i5u//vUMtVoA+/SH+UJ9bXsqmujdU7m9lc3wbEWmz/eM1Clp4z/ZiX2dUXIT87hHvsH39/ew+9kSgLp4xlbMHgrb/mzj7+3/9u4FdbDt+MUpgT4kPnz+BvLpuXtO6O0aStJ8x7vvV7+iJRfvapiygb4kCbaNT5j19Vc89z21gwpYRvf7iKqeMS34UyEZ29Eb774k6eeqWeuROLqJo5nkWV41i9s4mV1QeYXV5IUW42L9c08+utDUDsAKHv3FSVEhso61q6ufreFyjICfH4becNy/rbcaCTR/+wi7buCLuaOlm7s5lJJXncdMEMPrBk+qC/jtyd763axQVzyjjlOLuBTjTQs4htFL0E2ENso+iN7r4xbp75wL3AZUAOsBpY6u6vDbVcBfrw+/dntnLvr6spzs1i3pRiLj9tCuMLs3ly/R5+9/oBPvknc/j4xXPIzzl6a72lq48vLd/IT16uoyjYE6KjN3LYPG87tZyvXnfGoQ1TvZF+nly/h2899zoHOnr5xDvmMK20gKyQkZsV4oI5ExLa8yGTvLanlffdv5LxBTn823Vn8Na5ZYf9cmnq7OMzP3iZ57c18L6zKvina08bcoPrybKy+gDPbdnPX1566qHPRipYt6uZG7+9iv6oc/FbJjKhMIf5U4q5vmoaoTHG6h1NLN9QR0N7L2VFubxvcQUXzil703L2t/fwrede5/HVNYwxo7QwmwmFuVw0t4xXa1t5cXsjBTkhrj+7kpsvnMWs4NdLT7ifv/vxa/zfulpuuXAWX7xqwXG9juHYbfFKYrskhoCH3P0rZnY7gLsvC+b5LPARIEps18Z7jrRMBfrweqH6AB/6zh+4/uxK7v7TMw4LhXB/lDufeJUn1tdSVpTLpy6Zw43nTB+yZbXzQCc3fHsVDe29fOi8GRz8jMybUsLUcfkYsX+OB57fTkFOiK+9fxELppbw4e+sZsvedk6vGMs/vfc0Fk0bdxJeeep7tbaVz/zgJd5o6OQtk4pZWFFCW3eErDHGhtoWGjv6uOvqhdxwzrSM76Y6Ubsbu3h45U6e27KPrr5+Gtp7KcwJ0dcfJdzvFOdlMauskJqmLpq7wiyaNo5TygqZWJJHSX4Wm+vbeXbTPsL9UW48dzqf/JO5lBcf/stqY10rD/1+Jz/dUEc4GuXK06dwzaKpfO0X29i6r51PXzKXT18y97iPPtaBRWksGnW+v3o3/7piM5PH5vHTT140ZAtu9Y4mvv7Lraza3sTsskJmlhWSExpDbvYYZpcVsWjaWMaY8fknX6WrL8Ijt5zL6ZVD74VSvb+dOx59iS172ykryqGrr59vfOBM3rVgkoLnGHX39fN/62r48ct11Ld0M7Ygh2jUKcwN8Q9Xn3bE90GO3/rdzfxwTQ2lhTksnl7KW+eWkZcdoifcz/f/sJunXqljX1sv+9t7CPc7k0vyeOvcMj7xjjlH3W6wv72H767cyUO/30l3uJ/JJXn8y/tO5x3zJp5QzQr0NNPQ3svqHU28XNPMilf3sqelm4vmlHH3dWdQcZS+QXfnmY17eXjlTjp6I/RFonSH+6lt7j60r29xbhaP3XZeQrsU9oT7+cpTm3lm417u/9DZnD2jdDheosioEo06nX0RinKzjrmxsr+9hxeqD3DJ/EnD0uWoQE8jT71Sz+eeeIWO3gjZIeOiOWX86dmVvPv0KSfUKm7tDrOlvo3+qDO7vGjQIyaPxN3VKhc5CY4U6KmzRUP4zu938OWfbeLMaeO46+qFzJtcPGy7I47Nzz6uA0sOUpiLJJ8CPUWs3dnEP6/YzKULJnHfjYt1wiIReROlQgrYVNfGHY++RMW4fL72/kUKcxEZlFroCdpY18oT6/bQ0t3HGRVjef+SaSOyP7C78/zrBzi9Yizj8rN56IUd/NvPtzK2IJv7P7RY+3GLyJAU6AlYs7OJmx9aTSTqjCuIHahzz3Ov8875k5gzsYhdjV3sONBBS1eY66umceXpk9nb2sPcScXHdOBFpD/K3/34NR5fU0NxXhZzJhbx0u4WLl0wiX993+m6ZJeIHJH2cjmCupZuvrtyJ4+8uIsp4/J47NbzmFSSx7pdTTzy4i5+vWU/bT0RxhfmMHNCAQ68tLvl0OPLinL47GVv4f1VRz8gpKM3wh2Pruc3Wxu45cJZVDd0sHZnE3//ngUsXaIDSkQkRnu5HKNdjZ187RfbeOrVegCuOG0yX7pq4aEjws6eMZ6zZ4wn0h+ls6//sHM2rNreyJb6NiYU5fLdlTv53BOv8szGffz79YsYP+BSXAfta+vhlofXsGVvO/987enceG7snCvh/uih822LiByNWugDrN3ZxK2PrCXc7yxdMo2bL5xJZWnBcS3L3XnkxV185anNlORn8YV3z+fcWRNo7upj4542MKgszeevf7iBlu4w931wMe94y4kdRSYi6U0t9AS9tqeVGx/8AxXj8nn4I0uYMeHETglqZtx0wUyWzBzP53/0Kn/5gw2DzjexOJcf/sX5J3yxBxHJbAr0QG+kn7/64cuUFmTzf7efP6wbIBdMLeHJj13ALzftpaUrTFFeFgumlNDXH+WVmlbedmr5MR+ZKSIykAI98M1nYxfS/e+bl4zI3iShMcblp735osADL24sInK8tMUNqGnq4tu/2877Flec8JnQRESSRYEOfO0XWxljxt9cNi/ZpYiIHLeMD/RNdW38ZEMdt1w0S/3YIpLSMj7Q//M31RTlZnH7209JdikiIickowO9vrWbp1/by9Il0wa9oKuISCrJ6EB/5MVduDsfPn9msksRETlhGRvoPeF+Hlu9m3ctmMy08cd3JKiIyGiSsYH+i037aOkK8+HzZyS7FBGRYZFQoJvZ5Wa21cyqzezOQaZfbGatZvZycPvi8Jc6vH7y0h6mjM3jvBO47JqIyGhy1CNFzSwE3AdcCtQCa8xsubtvGjDr79z9PSNQ47Br7Ojlt9sa+PO3zmLMGJ2WVkTSQyIt9HOAanff7u59wOPANSNb1sh66tV6IlHn2rMqkl2KiMiwSSTQK4CauOHaYNxA55vZBjN72swWDrYgM7vNzNaa2dqGhobjKHd4/GxDPfMmF+s8KiKSVhIJ9MH6JAaeRH09MMPdFwH/Afx4sAW5+wPuXuXuVeXl5cdU6HDpCffzUk0zb39Lcp5fRGSkJBLotcC0uOFKoC5+Bndvc/eO4P4KINvMyoatymG0oaaFcL+zZMb4ZJciIjKsEgn0NcBcM5tlZjnAUmB5/AxmNtmCi16a2TnBchuHu9jhsHZXMwBVM0uTXImIyPA66l4u7h4xszuAZ4AQ8JC7bzSz24Ppy4DrgI+ZWQToBpZ6sq5tdxSrdzRx6qQixhUMfn1PEZFUldAFLoJulBUDxi2Lu38vcO/wljb8+qPO+l3NXHXm1GSXIiIy7DLqSNGte9tp742wRN0tIpKGMirQ1+5qAqBKG0RFJA1lVKC/tLuF8uJcKkvzk12KiMiwy6hA31DTwpnTxhHskCMiklYyJtBbu8JsP9DJmdPGJbsUEZERkTGBvqG2BUCBLiJpK2MC/eWaFszg9MqxyS5FRGREZFSgn1JeREmerh0qIukpIwLd3Q9tEBURSVcZEeh7Wrpp7OxjkbpbRCSNZUSgb6prA2BhhQJdRNJXZgR6fRtmMG9ycbJLEREZMRkR6Jvr25g1oZCCnITORSYikpIyItA31bcxf6ouNyci6S3tA72tJ0xNUzcLpijQRSS9pX2gb6lvB1Cgi0jaS/tA31TXCsACdbmISJpL+0DfXN/O+MIcJhbnJrsUEZERlfaBvmVvG/OnFOuUuSKS9tI60KNR5/X9HcydqP3PRST9pXWg17V209XXz9xJRckuRURkxCUU6GZ2uZltNbNqM7vzCPMtMbN+M7tu+Eo8fq/v7wDg1ElqoYtI+jtqoJtZCLgPuAJYANxgZguGmO9u4JnhLvJ4Ve+LBfqccrXQRST9JdJCPweodvft7t4HPA5cM8h8nwSeAPYPY30nZNu+dsqKciktzEl2KSIiIy6RQK8AauKGa4Nxh5hZBXAtsOxICzKz28xsrZmtbWhoONZaj9nr+zs4Vf3nIpIhEgn0wfb38wHD9wCfc/f+Iy3I3R9w9yp3ryovL0+wxOPj7lTv72DuRAW6iGSGRE4/WAtMixuuBOoGzFMFPB7s610GXGlmEXf/8XAUeTzqW3vo6I0wRxtERSRDJBLoa4C5ZjYL2AMsBW6Mn8HdZx28b2YPAz9LZphD3B4uaqGLSIY4aqC7e8TM7iC290oIeMjdN5rZ7cH0I/abJ8sbQaCfokAXkQyR0BUf3H0FsGLAuEGD3N1vPvGyTtzupi4Kc0JM0B4uIpIh0vZI0drmLqaNL9A5XEQkY6RtoO9uigW6iEimSMtAd3dqmrqZrkAXkQySloF+oKOP7nA/00rzk12KiMhJk5aBvrupC4DpE9RCF5HMkZaBXtscBLq6XEQkg6RloO9ujAV6ZakCXUQyR3oGelMXE4tzycsOJbsUEZGTJi0DvaZZuyyKSOZJz0DXLosikoHSLtD7IlHqW7vVQheRjJN2gd7Q0UvUYcrYvGSXIiJyUqVdoDd29AJQVpSb5EpERE6utAv0A0GgTyjSWRZFJLOkYaD3AVCuFrqIZJg0DHS10EUkM6VfoLf3UZAToiAnoWt3iIikjbQL9MbOXm0QFZGMlHaBfqCjlzJ1t4hIBkq/QG/vY4Ja6CKSgdIu0NXlIiKZKqFAN7PLzWyrmVWb2Z2DTL/GzF4xs5fNbK2ZXTT8pR5df9Rp6uyjXF0uIpKBjroriJmFgPuAS4FaYI2ZLXf3TXGzPQcsd3c3szOAHwLzRqLgI2nq7CPqqMtFRDJSIi30c4Bqd9/u7n3A48A18TO4e4e7ezBYCDhJ0Nipw/5FJHMlEugVQE3ccG0w7jBmdq2ZbQGeAm4ZbEFmdlvQJbO2oaHheOo9ogPtsaNEtZeLiGSiRALdBhn3pha4u//I3ecB7wW+PNiC3P0Bd69y96ry8vJjKjQRfzxKVC10Eck8iQR6LTAtbrgSqBtqZnd/HjjFzMpOsLZjdjDQdR4XEclEiQT6GmCumc0ysxxgKbA8fgYzm2NmFtxfDOQAjcNd7NEc6OgjO2SU5OuwfxHJPEdNPnePmNkdwDNACHjI3Tea2e3B9GXAnwIfNrMw0A18IG4j6UlzoKOXCYW5BN8tIiIZJaGmrLuvAFYMGLcs7v7dwN3DW9qxa+zo1VkWRSRjpdWRoi3dYUoLFOgikpnSKtBbu8OMzc9OdhkiIkmRVoHe1h2mRIEuIhkqbQLd3WnrjqiFLiIZK20CvSccpa8/qkAXkYyVNoHe2h0GUKCLSMZKu0DXQUUikqnSLtDVQheRTKVAFxFJEwp0EZE0oUAXEUkTaRPobUGgF+cp0EUkM6VNoLd2hynOyyI0RmdaFJHMlDaB3qbzuIhIhkubQNeJuUQk06VVoJeo/1xEMlhaBbpa6CKSyRToIiJpIr0CvUCBLiKZKy0CvSfcT29Ep84VkcyWFoHe1nPwTIsKdBHJXAkFupldbmZbzazazO4cZPoHzeyV4LbSzBYNf6lDa9Nh/yIiRw90MwsB9wFXAAuAG8xswYDZdgBvd/czgC8DDwx3oUei87iIiCTWQj8HqHb37e7eBzwOXBM/g7uvdPfmYHAVUDm8ZR6ZAl1EJLFArwBq4oZrg3FD+XPg6cEmmNltZrbWzNY2NDQkXuVRHLpaUZ6uViQimSuRQB/sbFc+6Ixm7yAW6J8bbLq7P+DuVe5eVV5enniVR9HeEwG0UVREMlsiTdpaYFrccCVQN3AmMzsDeBC4wt0bh6e8xBwM9KJctdBFJHMl0kJfA8w1s1lmlgMsBZbHz2Bm04EngT9z923DX+aRtfdEyAmNIS87dLKfWkRk1Dhqk9bdI2Z2B/AMEAIecveNZnZ7MH0Z8EVgAvCfZgYQcfeqkSv7cB29YYrUfy4iGS6hFHT3FcCKAeOWxd3/KPDR4S0tce09EYoV6CKS4dLiSNGOnoj6z0Uk46VFoKuFLiKSLoHeG6EoV7ssikhmS49A7wnroCIRyXhpEugR7eUiIhkv5QPd3enoVR+6iEjKB3p3uJ/+qKsPXUQyXsoHekdw2L9a6CKS6VI+0NsU6CIiQBoEekevAl1EBNIg0NuD64mqD11EMl3KB7r60EVEYlI+0HUudBGRmNQP9KAPvSRPXS4iktlSP9CDPvTCXF3cQkQyW8oHekdPhIKcEFmhlH8pIiInJOVTsF3nQhcRAdIg0HUeFxGRmJQP9LaeMEXaICoikvqB3t4T0bnQRURIg0Dv6FUfuogIJBjoZna5mW01s2ozu3OQ6fPM7EUz6zWzvx7+MofW3hNWH7qICHDUJDSzEHAfcClQC6wxs+XuvilutibgU8B7R6LII+no0fVERUQgsRb6OUC1u2939z7gceCa+Bncfb+7rwHCI1DjkPqjTmdfvy4/JyJCYoFeAdTEDdcG446Zmd1mZmvNbG1DQ8PxLOIwB0/MpY2iIiKJBboNMs6P58nc/QF3r3L3qvLy8uNZxGHagsP+S/LV5SIikkig1wLT4oYrgbqRKefYHAp0tdBFRBIK9DXAXDObZWY5wFJg+ciWlZj2Hp1pUUTkoKM2bd09YmZ3AM8AIeAhd99oZrcH05eZ2WRgLVACRM3sM8ACd28budKhrTvWQi9WoIuIHD3QAdx9BbBiwLhlcff3EuuKOakOtdDz1eUiIpLSR4oe7ENXC11EJMUDvV3XExUROSSlA72tO0x+dohsXdxCRCS1A729J6L+cxGRQEoHeltPWP3nIiKBlA50nQtdROSPUjrQ1UIXEfmjlA70WB+6Al1EBFI80Nu6dXELEZGDUjbQ3T3oQ1cLXUQEUjjQeyNR+vqjaqGLiARSNtB1LnQRkcOlbqB362pFIiLxUjbQ2w9d3EItdBERSOFAb9OJuUREDpO6gd6tPnQRkXgpG+g6da6IyOFSNtDb1IcuInKYlA309p4woTFGQU4o2aWIiIwKKRvobd0RivOyMLNklyIiMiqkbKC39+g8LiIi8RIKdDO73My2mlm1md05yHQzs28F018xs8XDX+rhGjv7GF+QM9JPIyKSMo4a6GYWAu4DrgAWADeY2YIBs10BzA1utwH3D3Odb7KnpZup4/JH+mlERFJGIi30c4Bqd9/u7n3A48A1A+a5BnjEY1YB48xsyjDXeoi7U9fSTYUCXUTkkEQCvQKoiRuuDcYd6zyY2W1mttbM1jY0NBxrrYc0dvbRE45SUapAFxE5KJFAH2w3Ej+OeXD3B9y9yt2rysvLE6lvUHuauwHUQhcRiZNIoNcC0+KGK4G645hn2OxpCQJdLXQRkUMSCfQ1wFwzm2VmOcBSYPmAeZYDHw72djkPaHX3+mGu9ZCDLfTKcQUj9RQiIinnqDtyu3vEzO4AngFCwEPuvtHMbg+mLwNWAFcC1UAX8JGRKznWQi/MCVGSr/3QRUQOSigR3X0FsdCOH7cs7r4Dnxje0oZW29xNRWm+jhIVEYmTkkeKapdFEZE3S8lA39PSrQ2iIiIDpFygd/RGaO0OU6ENoiIih0m5QD+0D7pa6CIih0m9QG/pAnRQkYjIQCkX6CV52Vy2cBLTx6vLRUQkXsrtyF01czxVM8cnuwwRkVEn5VroIiIyOAW6iEiaUKCLiKQJBbqISJpQoIuIpAkFuohImlCgi4ikCQW6iEiasNipzJPwxGYNwK7jfHgZcGAYyxkpqVCnahw+qVBnKtQIqVFnsmqc4e6DXpQ5aYF+IsxsrbtXJbuOo0mFOlXj8EmFOlOhRkiNOkdjjepyERFJEwp0EZE0kaqB/kCyC0hQKtSpGodPKtSZCjVCatQ56mpMyT50ERF5s1RtoYuIyAAKdBGRNJFygW5ml5vZVjOrNrM7k10PgJlNM7Nfm9lmM9toZp8Oxt9lZnvM7OXgduUoqHWnmb0a1LM2GDfezH5pZq8Hf0uTWN9b4tbXy2bWZmafSfa6NLOHzGy/mb0WN27I9WZmnw8+o1vN7LIk1/lVM9tiZq+Y2Y/MbFwwfqaZdcet02VJrHHI9zcZ63KIGn8QV99OM3s5GJ+U9Tgod0+ZGxAC3gBmAznABmDBKKhrCrA4uF8MbAMWAHcBf53s+gbUuhMoGzDu34A7g/t3Ancnu86493svMCPZ6xJ4G7AYeO1o6y147zcAucCs4DMbSmKd7wKygvt3x9U5M36+JK/LQd/fZK3LwWocMP1rwBeTuR4Hu6VaC/0coNrdt7t7H/A4cE2Sa8Ld6919fXC/HdgMVCS3qmNyDfDd4P53gfcmr5TDXAK84e7He0TxsHH354GmAaOHWm/XAI+7e6+77wCqiX12k1Knu//C3SPB4Cqg8mTUMpQh1uVQkrIuj1SjmRnwfuCxka7jWKVaoFcANXHDtYyy4DSzmcBZwB+CUXcEP3UfSmZXRhwHfmFm68zstmDcJHevh9iXEzAxadUdbimH/9OMtnU51HobzZ/TW4Cn44ZnmdlLZvZbM3trsooKDPb+jsZ1+VZgn7u/HjduVKzHVAt0G2TcqNnv0syKgCeAz7h7G3A/cApwJlBP7Gdasl3o7ouBK4BPmNnbkl3QYMwsB7ga+N9g1Ghcl0MZlZ9TM/sCEAG+H4yqB6a7+1nAXwGPmllJksob6v0djevyBg5vaIya9ZhqgV4LTIsbrgTqklTLYcwsm1iYf9/dnwRw933u3u/uUeDbnKSf3Ufi7nXB3/3Aj4jVtM/MpgAEf/cnr8JDrgDWu/s+GJ3rkqHX26j7nJrZTcB7gA960PEbdGM0BvfXEeufPjUZ9R3h/R1V69LMsoD3AT84OG40rcdUC/Q1wFwzmxW04JYCy5Nc08E+te8Am93963Hjp8TNdi3w2sDHnkxmVmhmxQfvE9tY9hqxdXhTMNtNwE+SU+FhDmsFjbZ1GRhqvS0HlppZrpnNAuYCq5NQHxDbMwz4HHC1u3fFjS83s1BwfzaxOrcnqcah3t9RtS6BdwJb3L324IjRtB6TvlX2WG/AlcT2InkD+EKy6wlquojYz8BXgJeD25XA/wCvBuOXA1OSXOdsYnsMbAA2Hlx/wATgOeD14O/4JNdZADQCY+PGJXVdEvtyqQfCxFqNf36k9QZ8IfiMbgWuSHKd1cT6oQ9+NpcF8/5p8DnYAKwHrkpijUO+v8lYl4PVGIx/GLh9wLxJWY+D3XTov4hImki1LhcRERmCAl1EJE0o0EVE0oQCXUQkTSjQRUTShAJdRCRNKNBFRNLE/wd5wFTHDBVP9wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/500: Loss =  3.2791, Score =  0.4519, lr =  1.5000e-03\n",
      "Epoch   2/500: Loss =  2.2678, Score =  0.6126, lr =  1.5000e-03\n",
      "Epoch   3/500: Loss =  1.9697, Score =  0.6596, lr =  1.5000e-03\n",
      "Epoch   4/500: Loss =  1.8373, Score =  0.6833, lr =  1.5000e-03\n",
      "Epoch   5/500: Loss =  1.8632, Score =  0.6744, lr =  1.5000e-03\n",
      "Epoch   6/500: Loss =  1.7980, Score =  0.6872, lr =  1.5000e-03\n",
      "Epoch   7/500: Loss =  1.7090, Score =  0.7008, lr =  1.5000e-03\n",
      "Epoch   8/500: Loss =  1.7644, Score =  0.6902, lr =  1.5000e-03\n",
      "Epoch   9/500: Loss =  1.7521, Score =  0.6921, lr =  1.5000e-03\n",
      "Epoch  10/500: Loss =  1.7107, Score =  0.6995, lr =  1.5000e-03\n",
      "Epoch  11/500: Loss =  1.7021, Score =  0.7032, lr =  1.5000e-03\n",
      "Epoch  12/500: Loss =  1.7337, Score =  0.6975, lr =  1.5000e-03\n",
      "Epoch  13/500: Loss =  1.7463, Score =  0.6951, lr =  1.5000e-03\n",
      "Epoch  14/500: Loss =  1.7245, Score =  0.6971, lr =  1.5000e-03\n",
      "Epoch  15/500: Loss =  1.7368, Score =  0.6951, lr =  1.5000e-03\n",
      "Epoch  16/500: Loss =  1.7214, Score =  0.6985, lr =  1.5000e-03\n",
      "Epoch  17/500: Loss =  1.7409, Score =  0.6962, lr =  1.5000e-03\n",
      "Epoch  18/500: Loss =  1.7180, Score =  0.6999, lr =  1.5000e-03\n",
      "Epoch  19/500: Loss =  1.6986, Score =  0.7032, lr =  1.5000e-03\n",
      "Epoch  20/500: Loss =  1.7048, Score =  0.7020, lr =  1.5000e-03\n",
      "Epoch  21/500: Loss =  1.7178, Score =  0.6993, lr =  1.5000e-03\n",
      "Epoch  22/500: Loss =  1.7136, Score =  0.7006, lr =  1.5000e-03\n",
      "Epoch  23/500: Loss =  1.6914, Score =  0.7033, lr =  1.5000e-03\n",
      "Epoch  24/500: Loss =  1.6659, Score =  0.7079, lr =  1.5000e-03\n",
      "Epoch  25/500: Loss =  1.6820, Score =  0.7064, lr =  1.5000e-03\n",
      "Epoch  26/500: Loss =  1.6671, Score =  0.7093, lr =  1.5000e-03\n",
      "Epoch  27/500: Loss =  1.6811, Score =  0.7062, lr =  1.5000e-03\n",
      "Epoch  28/500: Loss =  1.6955, Score =  0.7032, lr =  1.5000e-03\n",
      "Epoch  29/500: Loss =  1.6953, Score =  0.7038, lr =  1.5000e-03\n",
      "Epoch  30/500: Loss =  1.7023, Score =  0.7031, lr =  1.5000e-03\n",
      "Epoch  31/500: Loss =  1.7024, Score =  0.7029, lr =  1.5000e-03\n",
      "Epoch  32/500: Loss =  1.6650, Score =  0.7091, lr =  1.5000e-03\n",
      "Epoch  33/500: Loss =  1.6489, Score =  0.7131, lr =  1.5000e-03\n",
      "Epoch  34/500: Loss =  1.6534, Score =  0.7110, lr =  1.5000e-03\n",
      "Epoch  35/500: Loss =  1.6501, Score =  0.7106, lr =  1.5000e-03\n",
      "Epoch  36/500: Loss =  1.6355, Score =  0.7122, lr =  1.5000e-03\n",
      "Epoch  37/500: Loss =  1.6462, Score =  0.7109, lr =  1.5000e-03\n",
      "Epoch  38/500: Loss =  1.6576, Score =  0.7098, lr =  1.5000e-03\n",
      "Epoch  39/500: Loss =  1.6398, Score =  0.7128, lr =  1.5000e-03\n",
      "Epoch  40/500: Loss =  1.6426, Score =  0.7121, lr =  1.5000e-03\n",
      "Epoch  41/500: Loss =  1.6340, Score =  0.7141, lr =  1.5000e-03\n",
      "Epoch  42/500: Loss =  1.6429, Score =  0.7117, lr =  1.5000e-03\n",
      "Epoch  43/500: Loss =  1.6368, Score =  0.7138, lr =  1.5000e-03\n",
      "Epoch  44/500: Loss =  1.6590, Score =  0.7101, lr =  1.5000e-03\n",
      "Epoch  45/500: Loss =  1.6618, Score =  0.7101, lr =  1.5000e-03\n",
      "Epoch  46/500: Loss =  1.6527, Score =  0.7116, lr =  1.5000e-03\n",
      "Epoch  47/500: Loss =  1.6441, Score =  0.7126, lr =  1.5000e-03\n",
      "Epoch  48/500: Loss =  1.6383, Score =  0.7127, lr =  1.5000e-03\n",
      "Epoch  49/500: Loss =  1.6477, Score =  0.7114, lr =  1.5000e-03\n",
      "Epoch  50/500: Loss =  1.6354, Score =  0.7132, lr =  1.5000e-03\n",
      "Epoch  51/500: Loss =  1.6351, Score =  0.7128, lr =  1.5000e-03\n",
      "Epoch  52/500: Loss =  1.6277, Score =  0.7137, lr =  1.5000e-03\n",
      "Epoch  53/500: Loss =  1.6150, Score =  0.7163, lr =  1.5000e-03\n",
      "Epoch  54/500: Loss =  1.6487, Score =  0.7096, lr =  1.5000e-03\n",
      "Epoch  55/500: Loss =  1.6365, Score =  0.7111, lr =  1.5000e-03\n",
      "Epoch  56/500: Loss =  1.6553, Score =  0.7091, lr =  1.5000e-03\n",
      "Epoch  57/500: Loss =  1.6377, Score =  0.7119, lr =  1.5000e-03\n",
      "Epoch  58/500: Loss =  1.6161, Score =  0.7163, lr =  1.5000e-03\n",
      "Epoch  59/500: Loss =  1.6259, Score =  0.7144, lr =  1.5000e-03\n",
      "Epoch  60/500: Loss =  1.6495, Score =  0.7107, lr =  1.5000e-03\n",
      "Epoch  61/500: Loss =  1.6483, Score =  0.7107, lr =  1.5000e-03\n",
      "Epoch  62/500: Loss =  1.6580, Score =  0.7097, lr =  1.5000e-03\n",
      "Epoch  63/500: Loss =  1.6423, Score =  0.7123, lr =  1.5000e-03\n",
      "Epoch  64/500: Loss =  1.6414, Score =  0.7115, lr =  1.5000e-03\n",
      "Epoch  65/500: Loss =  1.6406, Score =  0.7125, lr =  1.5000e-03\n",
      "Epoch  66/500: Loss =  1.6073, Score =  0.7182, lr =  1.5000e-03\n",
      "Epoch  67/500: Loss =  1.5952, Score =  0.7195, lr =  1.5000e-03\n",
      "Epoch  68/500: Loss =  1.6069, Score =  0.7180, lr =  1.5000e-03\n",
      "Epoch  69/500: Loss =  1.6082, Score =  0.7182, lr =  1.5000e-03\n",
      "Epoch  70/500: Loss =  1.6085, Score =  0.7182, lr =  1.5000e-03\n",
      "Epoch  71/500: Loss =  1.5912, Score =  0.7214, lr =  1.5000e-03\n",
      "Epoch  72/500: Loss =  1.6353, Score =  0.7147, lr =  1.5000e-03\n",
      "Epoch  73/500: Loss =  1.6224, Score =  0.7158, lr =  1.5000e-03\n",
      "Epoch  74/500: Loss =  1.5983, Score =  0.7204, lr =  1.5000e-03\n",
      "Epoch  75/500: Loss =  1.6043, Score =  0.7197, lr =  1.5000e-03\n",
      "Epoch  76/500: Loss =  1.6053, Score =  0.7196, lr =  1.5000e-03\n",
      "Epoch  77/500: Loss =  1.5968, Score =  0.7202, lr =  1.5000e-03\n",
      "Epoch  78/500: Loss =  1.5944, Score =  0.7200, lr =  1.5000e-03\n",
      "Epoch  79/500: Loss =  1.6002, Score =  0.7175, lr =  1.5000e-03\n",
      "Epoch  80/500: Loss =  1.6233, Score =  0.7144, lr =  1.5000e-03\n",
      "Epoch  81/500: Loss =  1.6241, Score =  0.7155, lr =  1.5000e-03\n",
      "Epoch  82/500: Loss =  1.6213, Score =  0.7159, lr =  1.5000e-03\n",
      "Epoch  83/500: Loss =  1.6317, Score =  0.7143, lr =  1.5000e-03\n",
      "Epoch  84/500: Loss =  1.6010, Score =  0.7194, lr =  1.5000e-03\n",
      "Epoch  85/500: Loss =  1.6206, Score =  0.7169, lr =  1.5000e-03\n",
      "Epoch  86/500: Loss =  1.6041, Score =  0.7192, lr =  1.5000e-03\n",
      "Epoch  87/500: Loss =  1.6108, Score =  0.7179, lr =  1.5000e-03\n",
      "Epoch  88/500: Loss =  1.6208, Score =  0.7170, lr =  1.5000e-03\n",
      "Epoch  89/500: Loss =  1.6011, Score =  0.7202, lr =  1.5000e-03\n",
      "Epoch  90/500: Loss =  1.6080, Score =  0.7185, lr =  1.5000e-03\n",
      "Epoch  91/500: Loss =  1.5893, Score =  0.7210, lr =  1.5000e-03\n",
      "Epoch  92/500: Loss =  1.5772, Score =  0.7230, lr =  1.5000e-03\n",
      "Epoch  93/500: Loss =  1.5751, Score =  0.7231, lr =  1.5000e-03\n",
      "Epoch  94/500: Loss =  1.5824, Score =  0.7208, lr =  1.5000e-03\n",
      "Epoch  95/500: Loss =  1.5965, Score =  0.7197, lr =  1.5000e-03\n",
      "Epoch  96/500: Loss =  1.5822, Score =  0.7221, lr =  1.5000e-03\n",
      "Epoch  97/500: Loss =  1.5768, Score =  0.7228, lr =  1.5000e-03\n",
      "Epoch  98/500: Loss =  1.5859, Score =  0.7214, lr =  1.5000e-03\n",
      "Epoch  99/500: Loss =  1.5662, Score =  0.7250, lr =  1.5000e-03\n",
      "Epoch 100/500: Loss =  1.5647, Score =  0.7249, lr =  1.5000e-03\n",
      "Epoch 101/500: Loss =  1.5461, Score =  0.7282, lr =  1.5000e-03\n",
      "Epoch 102/500: Loss =  1.5529, Score =  0.7273, lr =  1.5000e-03\n",
      "Epoch 103/500: Loss =  1.5519, Score =  0.7284, lr =  1.5000e-03\n",
      "Epoch 104/500: Loss =  1.5403, Score =  0.7292, lr =  1.5000e-03\n",
      "Epoch 105/500: Loss =  1.5419, Score =  0.7286, lr =  1.5000e-03\n",
      "Epoch 106/500: Loss =  1.5486, Score =  0.7268, lr =  1.5000e-03\n",
      "Epoch 107/500: Loss =  1.5402, Score =  0.7284, lr =  1.5000e-03\n",
      "Epoch 108/500: Loss =  1.5378, Score =  0.7288, lr =  1.5000e-03\n",
      "Epoch 109/500: Loss =  1.5495, Score =  0.7271, lr =  1.5000e-03\n",
      "Epoch 110/500: Loss =  1.5625, Score =  0.7247, lr =  1.5000e-03\n",
      "Epoch 111/500: Loss =  1.5585, Score =  0.7252, lr =  1.5000e-03\n",
      "Epoch 112/500: Loss =  1.5509, Score =  0.7265, lr =  1.5000e-03\n",
      "Epoch 113/500: Loss =  1.5727, Score =  0.7234, lr =  1.5000e-03\n",
      "Epoch 114/500: Loss =  1.5800, Score =  0.7224, lr =  1.5000e-03\n",
      "Epoch 115/500: Loss =  1.5796, Score =  0.7226, lr =  1.5000e-03\n",
      "Epoch 116/500: Loss =  1.5699, Score =  0.7249, lr =  1.5000e-03\n",
      "Epoch 117/500: Loss =  1.5623, Score =  0.7257, lr =  1.5000e-03\n",
      "Epoch 118/500: Loss =  1.5510, Score =  0.7270, lr =  1.5000e-03\n",
      "Epoch 119/500: Loss =  1.5476, Score =  0.7269, lr =  1.5000e-03\n",
      "Epoch 120/500: Loss =  1.5455, Score =  0.7280, lr =  1.5000e-03\n",
      "Epoch 121/500: Loss =  1.5643, Score =  0.7251, lr =  1.5000e-03\n",
      "Epoch 122/500: Loss =  1.5645, Score =  0.7247, lr =  1.5000e-03\n",
      "Epoch 123/500: Loss =  1.5477, Score =  0.7272, lr =  1.5000e-03\n",
      "Epoch 124/500: Loss =  1.5465, Score =  0.7270, lr =  1.5000e-03\n",
      "Epoch 125/500: Loss =  1.5533, Score =  0.7251, lr =  1.5000e-03\n",
      "Epoch 126/500: Loss =  1.5437, Score =  0.7264, lr =  1.5000e-03\n",
      "Epoch 127/500: Loss =  1.5360, Score =  0.7288, lr =  1.5000e-03\n",
      "Epoch 128/500: Loss =  1.5420, Score =  0.7282, lr =  1.5000e-03\n",
      "Epoch 129/500: Loss =  1.5257, Score =  0.7303, lr =  1.5000e-03\n",
      "Epoch 130/500: Loss =  1.5302, Score =  0.7308, lr =  1.5000e-03\n",
      "Epoch 131/500: Loss =  1.5276, Score =  0.7317, lr =  1.5000e-03\n",
      "Epoch 132/500: Loss =  1.5345, Score =  0.7306, lr =  1.5000e-03\n",
      "Epoch 133/500: Loss =  1.5564, Score =  0.7271, lr =  1.5000e-03\n",
      "Epoch 134/500: Loss =  1.5422, Score =  0.7287, lr =  1.5000e-03\n",
      "Epoch 135/500: Loss =  1.5465, Score =  0.7270, lr =  1.5000e-03\n",
      "Epoch 136/500: Loss =  1.5744, Score =  0.7218, lr =  1.5000e-03\n",
      "Epoch 137/500: Loss =  1.5837, Score =  0.7210, lr =  1.5000e-03\n",
      "Epoch 138/500: Loss =  1.5572, Score =  0.7248, lr =  1.5000e-03\n",
      "Epoch 139/500: Loss =  1.5591, Score =  0.7249, lr =  1.5000e-03\n",
      "Epoch 140/500: Loss =  1.5560, Score =  0.7254, lr =  1.5000e-03\n",
      "Epoch 141/500: Loss =  1.5334, Score =  0.7284, lr =  1.5000e-03\n",
      "Epoch 142/500: "
     ]
    }
   ],
   "source": [
    "new_im_each_epoch = True # False means using the same intermediate model for all ensemble models\n",
    "\n",
    "\n",
    "for e in range(1, n_ensemble + 1):\n",
    "    \n",
    "       \n",
    "    # each epoch a new intermediate model?\n",
    "    if new_im_each_epoch:\n",
    "        \n",
    "        # check if model for this epoch already exists\n",
    "        intermediate_available = os.path.isfile(CHECKPOINTS + f\"intermediate_model_{e}.pt\")\n",
    "        \n",
    "        if intermediate_available == False:\n",
    "\n",
    "            # Intermediate model\n",
    "            model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "            set_parameter_requires_grad(model, feature_extract_im)\n",
    "            num_ftrs = model.fc.in_features\n",
    "            new_layers = OutputLayers(n_layers, num_ftrs, n_dense, dropout)\n",
    "            model.fc = new_layers # replace last layer\n",
    "\n",
    "            # Send the model to GPU\n",
    "            model = model.to(device)\n",
    "\n",
    "            # create datalaoders with specific batch size\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size_im)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size_im)\n",
    "            dataloaders_dict = {\"train\":train_loader,\"val\":val_loader}\n",
    "\n",
    "            # Create Optimizer and define params to update\n",
    "            params_to_update = model.parameters()\n",
    "            if feature_extract_im:\n",
    "                params_to_update = []\n",
    "                for name,param in model.named_parameters():\n",
    "                    if param.requires_grad == True:\n",
    "                        params_to_update.append(param)\n",
    "                        #print(\"\\t\",name)\n",
    "\n",
    "            else:\n",
    "                for name,param in model.named_parameters():\n",
    "                    if param.requires_grad == True:\n",
    "                        #print(\"\\t\",name)\n",
    "                        ...\n",
    "\n",
    "            # Instantiate optimizer for intermediate model\n",
    "            optimizer = optim.Adam(params_to_update, lr=lr_im)\n",
    "\n",
    "            # Setup the loss fxn\n",
    "            criterion = get_criterion('L2')\n",
    "\n",
    "            # Initial training and evaluate\n",
    "            model, hist, lrs = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=n_epochs_im, early_stop_tolerance=early_stop_tolerance)\n",
    "\n",
    "            # Plot learning curve\n",
    "            plt.plot(hist)\n",
    "            plt.title(f'Intermediate model {e}')\n",
    "            plt.show()\n",
    "\n",
    "            # Save intermediate model\n",
    "            PATH_EN = f'CNN_checkpoints/intermediate_model_{e}.pt'\n",
    "            torch.save(model.state_dict(), PATH_EN)\n",
    "            del model\n",
    "        model = load_pretrained_model(\"intermediate\", e=e)\n",
    "        \n",
    "    else: # load a standard intermediate model\n",
    "        model = load_pretrained_model(\"intermediate\")\n",
    "\n",
    "    # fine tuning\n",
    "    \n",
    "    # create dataloaders with specific batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size_ft)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size_ft)\n",
    "    dataloaders_dict = {\"train\":train_loader,\"val\":val_loader}\n",
    "            \n",
    "    # Create Optimizer and define params to update\n",
    "    params_to_update = model.parameters()\n",
    "    if feature_extract_ft:\n",
    "        params_to_update = []\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                #print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                #print(\"\\t\",name)\n",
    "                ...\n",
    "\n",
    "    # Instantiate optimizer for finetuning\n",
    "    # optimizer = optim.SGD(params_to_update, lr=lr_ft, momentum=0.9)\n",
    "    optimizer = optim.Adagrad(params_to_update, lr=lr_ft)\n",
    "   \n",
    "    # Setup the loss fxn\n",
    "    criterion = get_criterion(\"L2\")\n",
    "\n",
    "    # Train and evaluate fine tuned model\n",
    "    model, hist, lrs = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs=n_epochs_ft, early_stop_tolerance=early_stop_tolerance)\n",
    "    \n",
    "    # Plot learning curve\n",
    "    plt.plot(hist)\n",
    "    plt.title(f'Ensemble model {e}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save intermediate model\n",
    "    PATH_EN = f'CNN_checkpoints/ensemble_model_{e}.pt'\n",
    "    torch.save(model.state_dict(), PATH_EN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load checkpoints and get predictions for validation and test sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataloaders for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_loader = DataLoader(pred_dataset, batch_size=1, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run ensemble models (CNNs) ...\n",
    "... to collect their individual predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_ = []\n",
    "rocks_120_pred_ = []\n",
    "\n",
    "#n_ensemble = 4\n",
    "\n",
    "for e in range(1, n_ensemble+1):\n",
    "    model = load_pretrained_model(\"ensemble\", e)\n",
    "    pred = predict(model, test_loader)\n",
    "    test_pred_.append(pred)\n",
    "    rocks_120_pred_.append(predict(model, pred_loader, unlabeled=True))\n",
    "    \n",
    "del model\n",
    "\n",
    "test_pred = np.mean(test_pred_, 0)\n",
    "rocks_120_pred = np.mean(rocks_120_pred_, 0)\n",
    "\n",
    "rocks_120_pred_ = np.array(rocks_120_pred_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels\n",
    "Y_120 = np.loadtxt(\"../sanders_2018/mds_120.txt\")\n",
    "Y_validate = train.iloc[:, 1:].values\n",
    "Y_test = test.iloc[:, 1:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7174001682344249\n",
      "2.1636707358117335\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y_test, test_pred)) # same MDS space\n",
    "print(mean_squared_error(Y_120, rocks_120_pred)) # not part of MDS space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get RÂ²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7077032043118736\n",
      "-0.04115360028683518\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(Y_test, test_pred))\n",
    "print(r2_score(Y_120, rocks_120_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Get Pearson's *r* for invidivual dimensions in Rocks 120 set\n",
    "\n",
    "* Best overall pytorch10 ensemble was from 2022-07-08 with lr=1.5e-3 => 0.667969\n",
    "* seconds best overall pytorch 10 model ensemble was from 2022-07-08 with lr=2e-3 mean correlation 0.667118 (custom model 0.670676)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Keras replicated</th>\n",
       "      <td>0.683814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Keras Sanders &amp; Nosofsky</th>\n",
       "      <td>0.676178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PyTorch ensemble</th>\n",
       "      <td>0.668967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PyTorch 3</th>\n",
       "      <td>0.66366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PyTorch custom</th>\n",
       "      <td>0.662922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PyTorch 2</th>\n",
       "      <td>0.661034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PyTorch 4</th>\n",
       "      <td>0.653443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PyTorch 1</th>\n",
       "      <td>0.647108</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Total\n",
       "Keras replicated          0.683814\n",
       "Keras Sanders & Nosofsky  0.676178\n",
       "PyTorch ensemble          0.668967\n",
       "PyTorch 3                  0.66366\n",
       "PyTorch custom            0.662922\n",
       "PyTorch 2                 0.661034\n",
       "PyTorch 4                 0.653443\n",
       "PyTorch 1                 0.647108"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_i = [1,2] # add models to a custom ensemble\n",
    "\n",
    "###\n",
    "\n",
    "rocks_120_pred_custom = np.mean(rocks_120_pred_[[i-1 for i in m_i], :, :], 0)\n",
    "\n",
    "cnn_pred_file = \"CNN Predictions/MDS Dimensions/cnn_predicted_mds_120.txt\"\n",
    "cnn_pred = np.loadtxt(cnn_pred_file)\n",
    "\n",
    "cnn_keras_pred_file = \"CNN Predictions/MDS Dimensions/cnn_keras_predicted_mds_120.txt\"\n",
    "cnn_keras_pred = np.loadtxt(cnn_keras_pred_file)\n",
    "\n",
    "df_corr = pd.DataFrame(index=range(1,9))\n",
    "\n",
    "for dim in range(1, 9):\n",
    "    df_corr.loc[dim, \"Keras Sanders & Nosofsky\"] = np.corrcoef(cnn_pred[:, dim-1], Y_120[:, dim-1]).min()\n",
    "    df_corr.loc[dim, \"Keras replicated\"] = np.corrcoef(cnn_keras_pred[:, dim-1], Y_120[:, dim-1]).min()\n",
    "    df_corr.loc[dim, \"PyTorch ensemble\"] = np.corrcoef(rocks_120_pred[:, dim-1], Y_120[:, dim-1]).min()\n",
    "    df_corr.loc[dim, \"PyTorch custom\"] = np.corrcoef(rocks_120_pred_custom[:, dim-1], Y_120[:, dim-1]).min()\n",
    "    for i in range(len(rocks_120_pred_)):\n",
    "        df_corr.loc[dim, f\"PyTorch {i+1}\"] = np.corrcoef(rocks_120_pred_[i, :, dim-1], Y_120[:, dim-1]).min()\n",
    "    \n",
    "df_corr.loc[\"Total\", :] = df_corr.mean(axis=0)\n",
    "df_corr[\"Best model\"] = df_corr.columns[df_corr.values.argmax(axis=1)]\n",
    "\n",
    "df_corr.T[[\"Total\"]].drop(\"Best model\", axis=0).sort_values(\"Total\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Keras Sanders &amp; Nosofsky</th>\n",
       "      <th>Keras replicated</th>\n",
       "      <th>PyTorch ensemble</th>\n",
       "      <th>PyTorch custom</th>\n",
       "      <th>PyTorch 1</th>\n",
       "      <th>PyTorch 2</th>\n",
       "      <th>PyTorch 3</th>\n",
       "      <th>PyTorch 4</th>\n",
       "      <th>Best model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.879750</td>\n",
       "      <td>0.877838</td>\n",
       "      <td>0.867953</td>\n",
       "      <td>0.851976</td>\n",
       "      <td>0.856311</td>\n",
       "      <td>0.831148</td>\n",
       "      <td>0.865940</td>\n",
       "      <td>0.865907</td>\n",
       "      <td>Keras Sanders &amp; Nosofsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.850301</td>\n",
       "      <td>0.826120</td>\n",
       "      <td>0.830457</td>\n",
       "      <td>0.822932</td>\n",
       "      <td>0.809643</td>\n",
       "      <td>0.821898</td>\n",
       "      <td>0.823223</td>\n",
       "      <td>0.814836</td>\n",
       "      <td>Keras Sanders &amp; Nosofsky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.378968</td>\n",
       "      <td>0.358630</td>\n",
       "      <td>0.389519</td>\n",
       "      <td>0.397545</td>\n",
       "      <td>0.418049</td>\n",
       "      <td>0.366182</td>\n",
       "      <td>0.340353</td>\n",
       "      <td>0.400332</td>\n",
       "      <td>PyTorch 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.838155</td>\n",
       "      <td>0.831899</td>\n",
       "      <td>0.847006</td>\n",
       "      <td>0.828625</td>\n",
       "      <td>0.819144</td>\n",
       "      <td>0.812707</td>\n",
       "      <td>0.865244</td>\n",
       "      <td>0.828917</td>\n",
       "      <td>PyTorch 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.484210</td>\n",
       "      <td>0.548242</td>\n",
       "      <td>0.467206</td>\n",
       "      <td>0.473253</td>\n",
       "      <td>0.453807</td>\n",
       "      <td>0.478236</td>\n",
       "      <td>0.450872</td>\n",
       "      <td>0.445656</td>\n",
       "      <td>Keras replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.841105</td>\n",
       "      <td>0.857191</td>\n",
       "      <td>0.887730</td>\n",
       "      <td>0.888385</td>\n",
       "      <td>0.874993</td>\n",
       "      <td>0.886919</td>\n",
       "      <td>0.873902</td>\n",
       "      <td>0.872292</td>\n",
       "      <td>PyTorch custom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.477049</td>\n",
       "      <td>0.496715</td>\n",
       "      <td>0.361294</td>\n",
       "      <td>0.379771</td>\n",
       "      <td>0.355650</td>\n",
       "      <td>0.384700</td>\n",
       "      <td>0.358731</td>\n",
       "      <td>0.300468</td>\n",
       "      <td>Keras replicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.659885</td>\n",
       "      <td>0.673881</td>\n",
       "      <td>0.700572</td>\n",
       "      <td>0.660888</td>\n",
       "      <td>0.589269</td>\n",
       "      <td>0.706479</td>\n",
       "      <td>0.731018</td>\n",
       "      <td>0.699133</td>\n",
       "      <td>PyTorch 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>0.676178</td>\n",
       "      <td>0.683814</td>\n",
       "      <td>0.668967</td>\n",
       "      <td>0.662922</td>\n",
       "      <td>0.647108</td>\n",
       "      <td>0.661034</td>\n",
       "      <td>0.663660</td>\n",
       "      <td>0.653443</td>\n",
       "      <td>Keras replicated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Keras Sanders & Nosofsky  Keras replicated  PyTorch ensemble  \\\n",
       "1                      0.879750          0.877838          0.867953   \n",
       "2                      0.850301          0.826120          0.830457   \n",
       "3                      0.378968          0.358630          0.389519   \n",
       "4                      0.838155          0.831899          0.847006   \n",
       "5                      0.484210          0.548242          0.467206   \n",
       "6                      0.841105          0.857191          0.887730   \n",
       "7                      0.477049          0.496715          0.361294   \n",
       "8                      0.659885          0.673881          0.700572   \n",
       "Total                  0.676178          0.683814          0.668967   \n",
       "\n",
       "       PyTorch custom  PyTorch 1  PyTorch 2  PyTorch 3  PyTorch 4  \\\n",
       "1            0.851976   0.856311   0.831148   0.865940   0.865907   \n",
       "2            0.822932   0.809643   0.821898   0.823223   0.814836   \n",
       "3            0.397545   0.418049   0.366182   0.340353   0.400332   \n",
       "4            0.828625   0.819144   0.812707   0.865244   0.828917   \n",
       "5            0.473253   0.453807   0.478236   0.450872   0.445656   \n",
       "6            0.888385   0.874993   0.886919   0.873902   0.872292   \n",
       "7            0.379771   0.355650   0.384700   0.358731   0.300468   \n",
       "8            0.660888   0.589269   0.706479   0.731018   0.699133   \n",
       "Total        0.662922   0.647108   0.661034   0.663660   0.653443   \n",
       "\n",
       "                     Best model  \n",
       "1      Keras Sanders & Nosofsky  \n",
       "2      Keras Sanders & Nosofsky  \n",
       "3                     PyTorch 1  \n",
       "4                     PyTorch 3  \n",
       "5              Keras replicated  \n",
       "6                PyTorch custom  \n",
       "7              Keras replicated  \n",
       "8                     PyTorch 3  \n",
       "Total          Keras replicated  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save predictions to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"CNN Predictions/MDS Dimensions/cnn_torch_ensemble10_22-07-08_predicted_mds_120.txt\", rocks_120_pred_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "### Idea of using a meta learner (LinearRegression, ElasticNet or similar) ...\n",
    "... to find optimally weighted mean of CNN predictions.\n",
    "\n",
    "$Y_{pred} =  b_0 + b_1 Y_{pred_1} + b_2 Y_{pred_2}$\n",
    "\n",
    "<div class=\"alert-warning\"><ul><li>According to their paper Sanders & Nosofsky were not using a meta-learner, they were training an ensemble of 10 models and averaging them.</li><li>Making a custom ensemble by adding only a selection of all trained models and adding some models twice or more times we can quickly see that weights can improve the performance. However, the differences with this simple method were only marginal compared to the differences that we see between models on certain dimensions such as shape or organization. Thus, the latter issue should be tackled first.</ul></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

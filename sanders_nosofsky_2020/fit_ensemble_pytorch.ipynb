{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit CNN in PyTorch\n",
    "===\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "from copy import deepcopy\n",
    "from skimage import io, transform\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "import cv2\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# number of models\n",
    "n_ensemble = 2\n",
    "\n",
    "# Number of classes in the dataset\n",
    "num_classes = 8\n",
    "\n",
    "# Batch size for training (change depending on how much memory you have)\n",
    "batch_size_im = 90\n",
    "batch_size_ft = 30\n",
    "\n",
    "# Number of epochs to train for \n",
    "num_epochs_im = 10\n",
    "num_epochs_ft = 10\n",
    "\n",
    "# where would we use these in pytorch? in keras they are used to create layers for the intermediate model\n",
    "dropout = 0.5   \n",
    "#n_dense = 256\n",
    "#n_layers = 2\n",
    "\n",
    "# Flag for feature extracting. When False, we finetune the whole model, \n",
    "#   when True we only update the reshaped layer params\n",
    "feature_extract_im = True\n",
    "feature_extract_ft = False\n",
    "\n",
    "input_size = 224\n",
    "\n",
    "loglr = -2.2200654426745987\n",
    "lr_im = 10 ** loglr\n",
    "lr_ft = 0.0001\n",
    "\n",
    "img_dir = \"../sanders_2018/360 Rocks\"\n",
    "csv_file = \"../finetuning_torchvision_data/mds_360.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions and classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=14, is_inception=False):\n",
    "    \"\"\"\n",
    "    handles the training and validation of a given model. At the end of\n",
    "    training returns the best performing model. After each epoch, the training and validation\n",
    "    accuracies are printed\n",
    "    \"\"\"\n",
    "    \n",
    "    since = time.time()\n",
    "\n",
    "    val_acc_history = []\n",
    "    \n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = None\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_r2 = 0.0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Get model outputs and calculate loss\n",
    "                    # Special case for inception because in training it has an auxiliary output. In train\n",
    "                    #   mode we calculate the loss by summing the final output and the auxiliary output\n",
    "                    #   but in testing we only consider the final output.\n",
    "                    if is_inception and phase == 'train':\n",
    "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                        outputs, aux_outputs = model(inputs)\n",
    "                        score = r2_score(labels.detach().numpy(), outputs.detach().numpy())\n",
    "                        loss1 = criterion(outputs, labels)\n",
    "                        loss2 = criterion(aux_outputs, labels)\n",
    "                        loss = loss1 + 0.4*loss2\n",
    "                    else:\n",
    "                        outputs = model(inputs)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        global tmp_labels, tmp_outputs\n",
    "                        tmp_labels = labels\n",
    "                        tmp_outputs = outputs\n",
    "                        score = r2_score(labels.detach().numpy(), outputs.detach().numpy())\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics # TODO -> check if this is correct (maybe calculate MSE?)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_r2 += score\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_r2 / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss: .4f} Acc: {epoch_acc: .4f}')\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and best_acc == None:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val' and best_acc < epoch_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            if phase == 'val':\n",
    "                val_acc_history.append(epoch_acc)\n",
    "    \n",
    "    # maybe not necessary, but lets check later if it's a handy feature\n",
    "    \n",
    "   #     # save model after each epoch\n",
    "   #     torch.save({\n",
    "   #         'epoch': epoch,\n",
    "   #         'model_state_dict': model.state_dict(),\n",
    "   #         'optimizer_state_dict': optimizer.state_dict(),\n",
    "   #         'loss': loss,\n",
    "   #         ...\n",
    "   #         }, PATH)\n",
    "    \n",
    "        print() # empty line between epochs\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, val_acc_history\n",
    "\n",
    "\n",
    "def set_parameter_requires_grad(model, feature_extracting):\n",
    "    \"\"\"\n",
    "    This helper function sets the ``.requires_grad`` attribute of the\n",
    "    parameters in the model to False when we are feature extracting.\n",
    "    \"\"\"\n",
    "    if feature_extracting:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "    else:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "    return None\n",
    "\n",
    "\n",
    "class RocksData(Dataset):\n",
    "    def __init__(self, df, root_dir):\n",
    "        super(RocksData).__init__()\n",
    "        self.df = df\n",
    "        self.normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                              std=[0.229, 0.224, 0.225])\n",
    "        self.root_dir = root_dir\n",
    "    def __len__(self): return len(self.df)\n",
    "    def __getitem__(self, ix):\n",
    "        img_path = self.root_dir + \"/\" + self.df.iloc[ix,0]\n",
    "        img = cv2.imread(img_path)/255.\n",
    "        kp = deepcopy(self.df.iloc[ix,1:].tolist())\n",
    "        kp_x = (np.array(kp[0::2])/img.shape[1]).tolist()\n",
    "        kp_y = (np.array(kp[1::2])/img.shape[0]).tolist()\n",
    "        kp2 = kp_x + kp_y\n",
    "        kp2 = torch.tensor(kp2) \n",
    "        img = self.preprocess_input(img)\n",
    "        return img, kp2\n",
    "    def preprocess_input(self, img):\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        img = torch.tensor(img).permute(2,0,1)\n",
    "        img = self.normalize(img).float()\n",
    "        return img.to(device)\n",
    "    def load_img(self, ix):\n",
    "        img_path = self.root_dir + \"/\" + self.df.iloc[ix,0]        \n",
    "        img = cv2.imread(img_path)\n",
    "        img =cv2.cvtColor(img, cv2.COLOR_BGR2RGB)/255.\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        return img\n",
    "    \n",
    "\n",
    "def get_criterion(loss_name):\n",
    "    \"\"\"\n",
    "    Returns the optimizer\n",
    "    \"\"\"\n",
    "    if loss_name == \"L1\":\n",
    "        return torch.nn.L1Loss()\n",
    "    elif loss_name == \"L2\":\n",
    "        return torch.nn.MSELoss()\n",
    "    elif loss_name == \"smooth_L1\":\n",
    "        return torch.nn.SmoothL1Loss()\n",
    "    elif loss_name == \"huber\":\n",
    "        return torch.nn.HuberLoss()\n",
    "    else:\n",
    "        raise Exception(\"No valid loss_name entered!\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "data_transforms = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize(input_size),\n",
    "        transforms.CenterCrop(input_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "train, test = train_test_split(df, test_size=90, random_state=0)\n",
    "train_dataset = RocksData(train.reset_index(drop=True), img_dir)\n",
    "test_dataset = RocksData(test.reset_index(drop=True), img_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define final layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the final layers look like in Keras:\n",
    "\n",
    "```\n",
    "__________________________________________________________________________________________________\n",
    " Layer (type)                   Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "\n",
    "avg_pool (GlobalAveragePooling  (None, 2048)        0           ['conv5_block3_out[0][0]']       \n",
    " 2D)                                                                                              \n",
    "                                                                                                  \n",
    " dropout (Dropout)              (None, 2048)         0           ['avg_pool[0][0]']               \n",
    "                                                                                                  \n",
    " dense (Dense)                  (None, 256)          524544      ['dropout[0][0]']                \n",
    "                                                                                                  \n",
    " batch_normalization (BatchNorm  (None, 256)         1024        ['dense[0][0]']                  \n",
    " alization)                                                                                       \n",
    "                                                                                                  \n",
    " dropout_1 (Dropout)            (None, 256)          0           ['batch_normalization[0][0]']    \n",
    "                                                                                                  \n",
    " dense_1 (Dense)                (None, 256)          65792       ['dropout_1[0][0]']              \n",
    "                                                                                                  \n",
    " batch_normalization_1 (BatchNo  (None, 256)         1024        ['dense_1[0][0]']                \n",
    " rmalization)                                                                                     \n",
    "                                                                                                  \n",
    " dropout_2 (Dropout)            (None, 256)          0           ['batch_normalization_1[0][0]']  \n",
    "                                                                                                  \n",
    " dense_2 (Dense)                (None, 8)            2056        ['dropout_2[0][0]']       \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_output_layers(dropout, num_ftrs, num_classes):\n",
    "    \"\"\"\n",
    "    Returns the output layers.\n",
    "    \"\"\"\n",
    "\n",
    "    return nn.Sequential(\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(num_ftrs, 256),\n",
    "            nn.ReLU(inplace = False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=dropout),\n",
    "            #nn.Linear(num_ftrs, 256),\n",
    "            nn.ReLU(inplace = False),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn for intermediate training:\n",
      "\t fc.1.weight\n",
      "\t fc.1.bias\n",
      "\t fc.3.weight\n",
      "\t fc.3.bias\n",
      "\t fc.6.weight\n",
      "\t fc.6.bias\n",
      "\t fc.8.weight\n",
      "\t fc.8.bias\n",
      "Epoch 1/10\n",
      "----------\n",
      "train Loss:  0.5406 Acc: -607.5319\n",
      "val Loss:  0.3665 Acc: -436.3427\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "train Loss:  0.2868 Acc: -327.9669\n",
      "val Loss:  0.0552 Acc: -68.6895\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "train Loss:  0.1540 Acc: -180.8577\n",
      "val Loss:  0.2184 Acc: -252.9600\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "train Loss:  0.0845 Acc: -97.2194\n",
      "val Loss:  0.1455 Acc: -161.2828\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "train Loss:  0.0597 Acc: -69.0448\n",
      "val Loss:  0.0335 Acc: -38.4465\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "train Loss:  0.0434 Acc: -50.0588\n",
      "val Loss:  0.0176 Acc: -21.4973\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "train Loss:  0.0435 Acc: -51.2795\n",
      "val Loss:  0.0195 Acc: -21.2697\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "train Loss:  0.0385 Acc: -45.1347\n",
      "val Loss:  0.0121 Acc: -13.7092\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "train Loss:  0.0377 Acc: -43.1582\n",
      "val Loss:  0.0073 Acc: -9.0751\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "train Loss:  0.0267 Acc: -31.1191\n",
      "val Loss:  0.0052 Acc: -6.5339\n",
      "\n",
      "Training complete in 6m 47s\n",
      "Best val Acc: -6.533872\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params to learn for finetuning:\n",
      "\t conv1.weight\n",
      "\t bn1.weight\n",
      "\t bn1.bias\n",
      "\t layer1.0.conv1.weight\n",
      "\t layer1.0.bn1.weight\n",
      "\t layer1.0.bn1.bias\n",
      "\t layer1.0.conv2.weight\n",
      "\t layer1.0.bn2.weight\n",
      "\t layer1.0.bn2.bias\n",
      "\t layer1.0.conv3.weight\n",
      "\t layer1.0.bn3.weight\n",
      "\t layer1.0.bn3.bias\n",
      "\t layer1.0.downsample.0.weight\n",
      "\t layer1.0.downsample.1.weight\n",
      "\t layer1.0.downsample.1.bias\n",
      "\t layer1.1.conv1.weight\n",
      "\t layer1.1.bn1.weight\n",
      "\t layer1.1.bn1.bias\n",
      "\t layer1.1.conv2.weight\n",
      "\t layer1.1.bn2.weight\n",
      "\t layer1.1.bn2.bias\n",
      "\t layer1.1.conv3.weight\n",
      "\t layer1.1.bn3.weight\n",
      "\t layer1.1.bn3.bias\n",
      "\t layer1.2.conv1.weight\n",
      "\t layer1.2.bn1.weight\n",
      "\t layer1.2.bn1.bias\n",
      "\t layer1.2.conv2.weight\n",
      "\t layer1.2.bn2.weight\n",
      "\t layer1.2.bn2.bias\n",
      "\t layer1.2.conv3.weight\n",
      "\t layer1.2.bn3.weight\n",
      "\t layer1.2.bn3.bias\n",
      "\t layer2.0.conv1.weight\n",
      "\t layer2.0.bn1.weight\n",
      "\t layer2.0.bn1.bias\n",
      "\t layer2.0.conv2.weight\n",
      "\t layer2.0.bn2.weight\n",
      "\t layer2.0.bn2.bias\n",
      "\t layer2.0.conv3.weight\n",
      "\t layer2.0.bn3.weight\n",
      "\t layer2.0.bn3.bias\n",
      "\t layer2.0.downsample.0.weight\n",
      "\t layer2.0.downsample.1.weight\n",
      "\t layer2.0.downsample.1.bias\n",
      "\t layer2.1.conv1.weight\n",
      "\t layer2.1.bn1.weight\n",
      "\t layer2.1.bn1.bias\n",
      "\t layer2.1.conv2.weight\n",
      "\t layer2.1.bn2.weight\n",
      "\t layer2.1.bn2.bias\n",
      "\t layer2.1.conv3.weight\n",
      "\t layer2.1.bn3.weight\n",
      "\t layer2.1.bn3.bias\n",
      "\t layer2.2.conv1.weight\n",
      "\t layer2.2.bn1.weight\n",
      "\t layer2.2.bn1.bias\n",
      "\t layer2.2.conv2.weight\n",
      "\t layer2.2.bn2.weight\n",
      "\t layer2.2.bn2.bias\n",
      "\t layer2.2.conv3.weight\n",
      "\t layer2.2.bn3.weight\n",
      "\t layer2.2.bn3.bias\n",
      "\t layer2.3.conv1.weight\n",
      "\t layer2.3.bn1.weight\n",
      "\t layer2.3.bn1.bias\n",
      "\t layer2.3.conv2.weight\n",
      "\t layer2.3.bn2.weight\n",
      "\t layer2.3.bn2.bias\n",
      "\t layer2.3.conv3.weight\n",
      "\t layer2.3.bn3.weight\n",
      "\t layer2.3.bn3.bias\n",
      "\t layer3.0.conv1.weight\n",
      "\t layer3.0.bn1.weight\n",
      "\t layer3.0.bn1.bias\n",
      "\t layer3.0.conv2.weight\n",
      "\t layer3.0.bn2.weight\n",
      "\t layer3.0.bn2.bias\n",
      "\t layer3.0.conv3.weight\n",
      "\t layer3.0.bn3.weight\n",
      "\t layer3.0.bn3.bias\n",
      "\t layer3.0.downsample.0.weight\n",
      "\t layer3.0.downsample.1.weight\n",
      "\t layer3.0.downsample.1.bias\n",
      "\t layer3.1.conv1.weight\n",
      "\t layer3.1.bn1.weight\n",
      "\t layer3.1.bn1.bias\n",
      "\t layer3.1.conv2.weight\n",
      "\t layer3.1.bn2.weight\n",
      "\t layer3.1.bn2.bias\n",
      "\t layer3.1.conv3.weight\n",
      "\t layer3.1.bn3.weight\n",
      "\t layer3.1.bn3.bias\n",
      "\t layer3.2.conv1.weight\n",
      "\t layer3.2.bn1.weight\n",
      "\t layer3.2.bn1.bias\n",
      "\t layer3.2.conv2.weight\n",
      "\t layer3.2.bn2.weight\n",
      "\t layer3.2.bn2.bias\n",
      "\t layer3.2.conv3.weight\n",
      "\t layer3.2.bn3.weight\n",
      "\t layer3.2.bn3.bias\n",
      "\t layer3.3.conv1.weight\n",
      "\t layer3.3.bn1.weight\n",
      "\t layer3.3.bn1.bias\n",
      "\t layer3.3.conv2.weight\n",
      "\t layer3.3.bn2.weight\n",
      "\t layer3.3.bn2.bias\n",
      "\t layer3.3.conv3.weight\n",
      "\t layer3.3.bn3.weight\n",
      "\t layer3.3.bn3.bias\n",
      "\t layer3.4.conv1.weight\n",
      "\t layer3.4.bn1.weight\n",
      "\t layer3.4.bn1.bias\n",
      "\t layer3.4.conv2.weight\n",
      "\t layer3.4.bn2.weight\n",
      "\t layer3.4.bn2.bias\n",
      "\t layer3.4.conv3.weight\n",
      "\t layer3.4.bn3.weight\n",
      "\t layer3.4.bn3.bias\n",
      "\t layer3.5.conv1.weight\n",
      "\t layer3.5.bn1.weight\n",
      "\t layer3.5.bn1.bias\n",
      "\t layer3.5.conv2.weight\n",
      "\t layer3.5.bn2.weight\n",
      "\t layer3.5.bn2.bias\n",
      "\t layer3.5.conv3.weight\n",
      "\t layer3.5.bn3.weight\n",
      "\t layer3.5.bn3.bias\n",
      "\t layer4.0.conv1.weight\n",
      "\t layer4.0.bn1.weight\n",
      "\t layer4.0.bn1.bias\n",
      "\t layer4.0.conv2.weight\n",
      "\t layer4.0.bn2.weight\n",
      "\t layer4.0.bn2.bias\n",
      "\t layer4.0.conv3.weight\n",
      "\t layer4.0.bn3.weight\n",
      "\t layer4.0.bn3.bias\n",
      "\t layer4.0.downsample.0.weight\n",
      "\t layer4.0.downsample.1.weight\n",
      "\t layer4.0.downsample.1.bias\n",
      "\t layer4.1.conv1.weight\n",
      "\t layer4.1.bn1.weight\n",
      "\t layer4.1.bn1.bias\n",
      "\t layer4.1.conv2.weight\n",
      "\t layer4.1.bn2.weight\n",
      "\t layer4.1.bn2.bias\n",
      "\t layer4.1.conv3.weight\n",
      "\t layer4.1.bn3.weight\n",
      "\t layer4.1.bn3.bias\n",
      "\t layer4.2.conv1.weight\n",
      "\t layer4.2.bn1.weight\n",
      "\t layer4.2.bn1.bias\n",
      "\t layer4.2.conv2.weight\n",
      "\t layer4.2.bn2.weight\n",
      "\t layer4.2.bn2.bias\n",
      "\t layer4.2.conv3.weight\n",
      "\t layer4.2.bn3.weight\n",
      "\t layer4.2.bn3.bias\n",
      "\t fc.1.weight\n",
      "\t fc.1.bias\n",
      "\t fc.3.weight\n",
      "\t fc.3.bias\n",
      "\t fc.6.weight\n",
      "\t fc.6.bias\n",
      "\t fc.8.weight\n",
      "\t fc.8.bias\n",
      "Epoch 1/10\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "for e in range(1, n_ensemble + 1):\n",
    "       \n",
    "    # Intermediate model\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    set_parameter_requires_grad(model, feature_extract_im)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    new_layers = get_output_layers(dropout, num_ftrs, num_classes)\n",
    "    model.fc = new_layers # last fully connected layer\n",
    "    \n",
    "    # Send the model to GPU\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # create datalaoders with specific batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size_im)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size_im)\n",
    "    dataloaders_dict = {\"train\":train_loader,\"val\":test_loader}\n",
    "    \n",
    "    # Create Optimizer and define params to update\n",
    "    params_to_update = model.parameters()\n",
    "    print('Params to learn for intermediate training:')\n",
    "    if feature_extract_im:\n",
    "        params_to_update = []\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "\n",
    "    # Instantiate optimizer for intermediate model\n",
    "    optimizer = optim.Adam(params_to_update, lr = lr_im)\n",
    "    \n",
    "    # Setup the loss fxn\n",
    "    criterion = get_criterion('L2')\n",
    "    \n",
    "    # Initial training and evaluate\n",
    "    model, hist = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs = num_epochs_im)\n",
    "    \n",
    "    # Plot learning curve\n",
    "    plt.plot(hist)\n",
    "    plt.title('Intermediate model')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save intermediate model\n",
    "    PATH = 'CNN_checkpoints/state_dict_intermediate_model.pt'\n",
    "    torch.save(model.state_dict(), PATH)\n",
    "\n",
    "    \n",
    "    # Fine tuned model\n",
    "    model = models.resnet50()\n",
    "    set_parameter_requires_grad(model, feature_extract_ft)\n",
    "    num_ftrs = model.fc.in_features\n",
    "    new_layers = get_output_layers(dropout, num_ftrs, num_classes)\n",
    "    model.fc = new_layers # last fully connected layer\n",
    "    \n",
    "    # Load pre-trained intermediate model\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "       \n",
    "    # Send the model to GPU\n",
    "    model = model.to(device)\n",
    "\n",
    "    # create datalaoders with specific batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size_ft)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size_ft)\n",
    "    dataloaders_dict = {\"train\":train_loader,\"val\":test_loader}\n",
    "            \n",
    "    # Create Optimizer and define params to update\n",
    "    params_to_update = model.parameters()\n",
    "    print('Params to learn for finetuning:')\n",
    "    if feature_extract_ft:\n",
    "        params_to_update = []\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                params_to_update.append(param)\n",
    "                print(\"\\t\",name)\n",
    "    else:\n",
    "        for name,param in model.named_parameters():\n",
    "            if param.requires_grad == True:\n",
    "                print(\"\\t\",name)\n",
    "\n",
    "    # Instantiate optimizer for finetuning\n",
    "    optimizer = optim.SGD(params_to_update, lr = lr_ft, momentum = 0.9)\n",
    "    \n",
    "    # Setup the loss fxn\n",
    "    criterion = get_criterion(\"L2\")\n",
    "\n",
    "    # Train and evaluate fine tuned model\n",
    "    model, hist = train_model(model, dataloaders_dict, criterion, optimizer, num_epochs = num_epochs_ft)\n",
    "    \n",
    "    # Plot learning curve\n",
    "    plt.plot(hist)\n",
    "    plt.title(f'Ensemble model {e}')\n",
    "    plt.show()\n",
    "    \n",
    "    # Save intermediate model\n",
    "    PATH = f'CNN_checkpoints/state_dict_ensemble_model_{e}.pt'\n",
    "    torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac78307-c58c-4f51-9394-72a35b8dc8e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Notebook equivalent of `fit_ensemble.py`\n",
    "---\n",
    "\n",
    "* This program trains the ensemble of CNN models reported in https://link.springer.com/article/10.1007/s42113-020-00073-z (https://osf.io/efjmq)\n",
    "    * It trains a model/ensemble on a 180 images training set of a 360 images dataset\n",
    "    * Makes predictions on\n",
    "        1. the 90 images validation set (part of the same 360 images set)\n",
    "        2. the 90 images test set (part of the same 360 images set)\n",
    "        3. the 120 images set (a different set)\n",
    "* Data required\n",
    "    * File `mds_360.txt` with labels (in `../sanders_2018`)\n",
    "    * Directory `360 Rocks/` with `*.jpg` images (in `../sanders_2018`)\n",
    "    * File `mds_120.txt` with labels (in `../sanders_2018`)\n",
    "    * Directory `120 Rocks/` with `*.jpg` images  (in `../sanders_2018`)\n",
    "* other available data\n",
    "    * Directory `120 Rock Images/` with 120 `*.png` images\n",
    "    * Directory `Similarity Judgements Data/` with similarity labels for the \"120 Rocks\" set as individual textfiles for each of the 85 participants: `rocks_similarity_120_*.txt`\n",
    "    * Directory `Categorization Data/` with category labels (1 = Igneous, 2 = Metamorphic, 4 = Mixed) for the \"120 Rocks\" set as individual textfiles for each of the 85 participants: `rocks_similarity_120_*_*.txt`\n",
    "    * File `MDS/mds_120_supplemental_dims.txt`\n",
    "    \n",
    "    \n",
    "#### **Update 2022/05/31: Additional necessary data added to `../sanders_2018` from here: https://osf.io/d6b9y/**\n",
    "\n",
    "   * Rocks dataset was created in 2017 here: https://link.springer.com/article/10.3758/s13428-017-0884-8 (https://osf.io/w64fv)\n",
    "   * Further work in Sanders' 2018 doctoral thesis https://scholarworks.iu.edu/dspace/handle/2022/22415 (https://osf.io/d6b9y)\n",
    "        * includes the relevant additional data such as the 360 rocks images set\n",
    "        * includes the same script `fit_ensemble.py` (identical version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9918b7-6593-4b18-a223-605d525501ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "nPixels = 224\n",
    "\n",
    "nTest = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575f4ca-9a76-4c7f-9528-987a98d634f8",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3398881-bcc4-46b5-8699-6cb1f478826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [i for i in range(30) for j in range(12)] # creates 360 list items like so: [0, 0, 0, 0, ... 29, 29, 29, 29]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc824d1-e3b2-4fe8-9b31-507fd2c326e5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b97c899-da5c-4399-ac69-362eae2c303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(directory, nPixels, preprocesser):\n",
    "    \"\"\"\n",
    "    Creates array-like data from a directory with image files for usage with Keras.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                img = load_img(os.path.join(subdir, file), target_size=(nPixels, nPixels))\n",
    "                x = img_to_array(img)\n",
    "                X.append(x)\n",
    "    X = np.stack(X)\n",
    "    X = preprocesser(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c0755-2bd7-485a-a1da-886e58709894",
   "metadata": {},
   "source": [
    "## Prepare 360 Rocks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a68534-f8b7-418d-ae23-e89a00aec802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image files\n",
    "X = load_images(\"../sanders_2018/360 Rocks\", nPixels, lambda x: resnet50.preprocess_input(np.expand_dims(x, axis=0)).squeeze())\n",
    "\n",
    "# load labels\n",
    "mds_360 = np.loadtxt(\"../sanders_2018/mds_360.txt\") # missing\n",
    "\n",
    "# split data: train vs test\n",
    "(X_train_, X_test, \n",
    " Y_train_, Y_test, \n",
    " categories_train_, categories_test) = train_test_split(X, \n",
    "                                                        mds_360, \n",
    "                                                        categories,\n",
    "                                                        test_size=nTest,\n",
    "                                                        stratify=categories, \n",
    "                                                        random_state=0)\n",
    "\n",
    "# split train set again: train vs validate\n",
    "(X_train, X_validate, \n",
    " Y_train, Y_validate) = train_test_split(X_train_, \n",
    "                                         Y_train_, \n",
    "                                         test_size=nTest,\n",
    "                                         stratify=categories_train_, \n",
    "                                         random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f8eef-b65e-48c5-b32a-80f75ae68a9b",
   "metadata": {},
   "source": [
    "## Prepare 120 Rocks data\n",
    "\n",
    "no train, test, validate splits ...will be later used only for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06f74f0-00e5-4cc4-a8b7-4ee40425ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image files\n",
    "X_120 = load_images(\"../sanders_2018/120 Rocks\", nPixels, lambda x: resnet50.preprocess_input(np.expand_dims(x, axis=0)).squeeze())\n",
    "\n",
    "# load labels\n",
    "Y_120 = np.loadtxt(\"../sanders_2018/mds_120.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b652ff77-60a6-431b-a06d-cf4b09159d2e",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57c64f3a-1982-4868-98ad-382aa70b486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "                    samplewise_center=False,\n",
    "                    featurewise_std_normalization=False,\n",
    "                    samplewise_std_normalization=False,\n",
    "                    zca_whitening=False,\n",
    "                    rotation_range=20,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    shear_range=0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    channel_shift_range=0.,\n",
    "                    fill_mode='nearest',\n",
    "                    cval=0.,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True)\n",
    "\n",
    "nEpochs = 10\n",
    "dropout = 0.5\n",
    "nEnsemble = 2\n",
    "          \n",
    "nDense = 256\n",
    "nLayers = 2\n",
    "loglr = -2.2200654426745987\n",
    "\n",
    "lr = 10 ** loglr\n",
    "nDim = 8\n",
    "batch_size = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d4c08-1998-45ad-9654-17dc5317a921",
   "metadata": {},
   "source": [
    "## Train models and save checkpoints\n",
    "\n",
    "Training performance seems to depend on hardware ... e.g. poor results on laptop, good results on desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "108761a5-9d10-42e7-8ec5-4ea16e3041bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-10 16:18:28.161190: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2022-06-10 16:18:28.161344: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (mink): /proc/driver/nvidia/version does not exist\n",
      "2022-06-10 16:18:28.231396: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 - 22s - loss: 11.0074 - accuracy: 0.1167 - mse: 11.0074 - val_loss: 7.2518 - val_accuracy: 0.1111 - val_mse: 7.2518 - 22s/epoch - 11s/step\n",
      "Epoch 2/10\n",
      "2/2 - 15s - loss: 9.8280 - accuracy: 0.1500 - mse: 9.8280 - val_loss: 8.7458 - val_accuracy: 0.1222 - val_mse: 8.7458 - 15s/epoch - 8s/step\n",
      "Epoch 3/10\n",
      "2/2 - 18s - loss: 8.9972 - accuracy: 0.1500 - mse: 8.9972 - val_loss: 10.1866 - val_accuracy: 0.1000 - val_mse: 10.1866 - 18s/epoch - 9s/step\n",
      "Epoch 4/10\n",
      "2/2 - 18s - loss: 8.4307 - accuracy: 0.1889 - mse: 8.4307 - val_loss: 9.9153 - val_accuracy: 0.1111 - val_mse: 9.9153 - 18s/epoch - 9s/step\n",
      "Epoch 5/10\n",
      "2/2 - 17s - loss: 8.3843 - accuracy: 0.2278 - mse: 8.3843 - val_loss: 10.8136 - val_accuracy: 0.1111 - val_mse: 10.8136 - 17s/epoch - 9s/step\n",
      "Epoch 6/10\n",
      "2/2 - 17s - loss: 7.7838 - accuracy: 0.1611 - mse: 7.7838 - val_loss: 12.2346 - val_accuracy: 0.0556 - val_mse: 12.2346 - 17s/epoch - 9s/step\n",
      "Epoch 7/10\n",
      "2/2 - 17s - loss: 7.4798 - accuracy: 0.1889 - mse: 7.4798 - val_loss: 13.0111 - val_accuracy: 0.1000 - val_mse: 13.0111 - 17s/epoch - 9s/step\n",
      "Epoch 8/10\n",
      "2/2 - 17s - loss: 7.3977 - accuracy: 0.1667 - mse: 7.3977 - val_loss: 12.5119 - val_accuracy: 0.1111 - val_mse: 12.5119 - 17s/epoch - 9s/step\n",
      "Epoch 9/10\n",
      "2/2 - 17s - loss: 7.4051 - accuracy: 0.1778 - mse: 7.4051 - val_loss: 12.2689 - val_accuracy: 0.1556 - val_mse: 12.2689 - 17s/epoch - 9s/step\n",
      "Epoch 10/10\n",
      "2/2 - 17s - loss: 6.9482 - accuracy: 0.2500 - mse: 6.9482 - val_loss: 13.0495 - val_accuracy: 0.1333 - val_mse: 13.0495 - 17s/epoch - 9s/step\n",
      "Epoch 1/10\n",
      "6/6 - 61s - loss: 9.9044 - accuracy: 0.1778 - mse: 9.9044 - val_loss: 7.2405 - val_accuracy: 0.1222 - val_mse: 7.2405 - 61s/epoch - 10s/step\n",
      "Epoch 2/10\n",
      "6/6 - 54s - loss: 9.5990 - accuracy: 0.1500 - mse: 9.5990 - val_loss: 7.4205 - val_accuracy: 0.1000 - val_mse: 7.4205 - 54s/epoch - 9s/step\n",
      "Epoch 3/10\n",
      "6/6 - 54s - loss: 9.1238 - accuracy: 0.1833 - mse: 9.1238 - val_loss: 7.6554 - val_accuracy: 0.0889 - val_mse: 7.6554 - 54s/epoch - 9s/step\n",
      "Epoch 4/10\n",
      "6/6 - 53s - loss: 10.2473 - accuracy: 0.1056 - mse: 10.2473 - val_loss: 7.8092 - val_accuracy: 0.0889 - val_mse: 7.8092 - 53s/epoch - 9s/step\n",
      "Epoch 5/10\n",
      "6/6 - 54s - loss: 10.3047 - accuracy: 0.1056 - mse: 10.3047 - val_loss: 7.8307 - val_accuracy: 0.1222 - val_mse: 7.8307 - 54s/epoch - 9s/step\n",
      "Epoch 6/10\n",
      "6/6 - 55s - loss: 9.6455 - accuracy: 0.1333 - mse: 9.6455 - val_loss: 7.6053 - val_accuracy: 0.1333 - val_mse: 7.6053 - 55s/epoch - 9s/step\n",
      "Epoch 7/10\n",
      "6/6 - 55s - loss: 9.6470 - accuracy: 0.1278 - mse: 9.6470 - val_loss: 7.3991 - val_accuracy: 0.1444 - val_mse: 7.3991 - 55s/epoch - 9s/step\n",
      "Epoch 8/10\n",
      "6/6 - 55s - loss: 10.0093 - accuracy: 0.1333 - mse: 10.0093 - val_loss: 7.2910 - val_accuracy: 0.1444 - val_mse: 7.2910 - 55s/epoch - 9s/step\n",
      "Epoch 9/10\n",
      "6/6 - 56s - loss: 9.8276 - accuracy: 0.1278 - mse: 9.8276 - val_loss: 7.2475 - val_accuracy: 0.1333 - val_mse: 7.2475 - 56s/epoch - 9s/step\n",
      "Epoch 10/10\n",
      "6/6 - 55s - loss: 9.3993 - accuracy: 0.1500 - mse: 9.3993 - val_loss: 7.3319 - val_accuracy: 0.1556 - val_mse: 7.3319 - 55s/epoch - 9s/step\n",
      "Epoch 1/10\n",
      "6/6 - 22s - loss: 10.1529 - accuracy: 0.1389 - mse: 10.1529 - val_loss: 19.6400 - val_accuracy: 0.1222 - val_mse: 19.6400 - 22s/epoch - 4s/step\n",
      "Epoch 2/10\n",
      "6/6 - 18s - loss: 9.2769 - accuracy: 0.1833 - mse: 9.2769 - val_loss: 28.2751 - val_accuracy: 0.1333 - val_mse: 28.2751 - 18s/epoch - 3s/step\n",
      "Epoch 3/10\n",
      "6/6 - 19s - loss: 8.6058 - accuracy: 0.1722 - mse: 8.6058 - val_loss: 13.4884 - val_accuracy: 0.1222 - val_mse: 13.4884 - 19s/epoch - 3s/step\n",
      "Epoch 4/10\n",
      "6/6 - 19s - loss: 7.9494 - accuracy: 0.1833 - mse: 7.9494 - val_loss: 11.5057 - val_accuracy: 0.1556 - val_mse: 11.5057 - 19s/epoch - 3s/step\n",
      "Epoch 5/10\n",
      "6/6 - 18s - loss: 7.5342 - accuracy: 0.2167 - mse: 7.5342 - val_loss: 13.7550 - val_accuracy: 0.0778 - val_mse: 13.7550 - 18s/epoch - 3s/step\n",
      "Epoch 6/10\n",
      "6/6 - 18s - loss: 7.6994 - accuracy: 0.1778 - mse: 7.6994 - val_loss: 14.6527 - val_accuracy: 0.1222 - val_mse: 14.6527 - 18s/epoch - 3s/step\n",
      "Epoch 7/10\n",
      "6/6 - 18s - loss: 7.2144 - accuracy: 0.2111 - mse: 7.2144 - val_loss: 12.1775 - val_accuracy: 0.1556 - val_mse: 12.1775 - 18s/epoch - 3s/step\n",
      "Epoch 8/10\n",
      "6/6 - 18s - loss: 6.8980 - accuracy: 0.2167 - mse: 6.8980 - val_loss: 13.3730 - val_accuracy: 0.1444 - val_mse: 13.3730 - 18s/epoch - 3s/step\n",
      "Epoch 9/10\n",
      "6/6 - 18s - loss: 6.8391 - accuracy: 0.1722 - mse: 6.8391 - val_loss: 11.6863 - val_accuracy: 0.1222 - val_mse: 11.6863 - 18s/epoch - 3s/step\n",
      "Epoch 10/10\n",
      "6/6 - 19s - loss: 6.5037 - accuracy: 0.2500 - mse: 6.5037 - val_loss: 11.3792 - val_accuracy: 0.1444 - val_mse: 11.3792 - 19s/epoch - 3s/step\n",
      "Epoch 1/10\n",
      "6/6 - 61s - loss: 7.0060 - accuracy: 0.2167 - mse: 7.0060 - val_loss: 10.9839 - val_accuracy: 0.1556 - val_mse: 10.9839 - 61s/epoch - 10s/step\n",
      "Epoch 2/10\n",
      "6/6 - 55s - loss: 6.6609 - accuracy: 0.2000 - mse: 6.6609 - val_loss: 10.8939 - val_accuracy: 0.1333 - val_mse: 10.8939 - 55s/epoch - 9s/step\n",
      "Epoch 3/10\n",
      "6/6 - 56s - loss: 6.8041 - accuracy: 0.1778 - mse: 6.8041 - val_loss: 10.6905 - val_accuracy: 0.1333 - val_mse: 10.6905 - 56s/epoch - 9s/step\n",
      "Epoch 4/10\n",
      "6/6 - 56s - loss: 6.5967 - accuracy: 0.2500 - mse: 6.5967 - val_loss: 10.5060 - val_accuracy: 0.1333 - val_mse: 10.5060 - 56s/epoch - 9s/step\n",
      "Epoch 5/10\n",
      "6/6 - 56s - loss: 6.6426 - accuracy: 0.1944 - mse: 6.6426 - val_loss: 10.2239 - val_accuracy: 0.1111 - val_mse: 10.2239 - 56s/epoch - 9s/step\n",
      "Epoch 6/10\n",
      "6/6 - 56s - loss: 6.5195 - accuracy: 0.2333 - mse: 6.5195 - val_loss: 9.9567 - val_accuracy: 0.1111 - val_mse: 9.9567 - 56s/epoch - 9s/step\n",
      "Epoch 7/10\n",
      "6/6 - 56s - loss: 6.3763 - accuracy: 0.2389 - mse: 6.3763 - val_loss: 9.6268 - val_accuracy: 0.1000 - val_mse: 9.6268 - 56s/epoch - 9s/step\n",
      "Epoch 8/10\n",
      "6/6 - 56s - loss: 6.4651 - accuracy: 0.2111 - mse: 6.4651 - val_loss: 9.4899 - val_accuracy: 0.1222 - val_mse: 9.4899 - 56s/epoch - 9s/step\n",
      "Epoch 9/10\n",
      "6/6 - 57s - loss: 6.5451 - accuracy: 0.2333 - mse: 6.5451 - val_loss: 9.4717 - val_accuracy: 0.1111 - val_mse: 9.4717 - 57s/epoch - 9s/step\n",
      "Epoch 10/10\n",
      "6/6 - 56s - loss: 6.5247 - accuracy: 0.2389 - mse: 6.5247 - val_loss: 9.2809 - val_accuracy: 0.1111 - val_mse: 9.2809 - 56s/epoch - 9s/step\n"
     ]
    }
   ],
   "source": [
    "for e in range(nEnsemble):\n",
    "    #Build model\n",
    "    arch = resnet50.ResNet50(include_top=False, pooling='avg')\n",
    "    for layer in arch.layers:\n",
    "        layer.trainable = False    \n",
    "    \n",
    "    x = arch.output\n",
    "    x = Dropout(dropout)(x)\n",
    "    for lyr in range(nLayers):\n",
    "        x = Dense(nDense, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "    x = Dense(nDim)(x)\n",
    "    \n",
    "    model = Model(inputs=arch.input, outputs=x)\n",
    "    \n",
    "    #Initial training\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=lr), metrics=['accuracy', 'mse'])\n",
    "    \n",
    "    checkpoint1 = ModelCheckpoint('intermediate_model.hdf5', save_best_only=True)\n",
    "\n",
    "    hist1 = model.fit(datagen.flow(X_train, Y_train, batch_size), \n",
    "                                steps_per_epoch=len(X_train) / batch_size,\n",
    "                                epochs=nEpochs,\n",
    "                                validation_data=(X_validate, Y_validate),\n",
    "                                callbacks=[checkpoint1],\n",
    "                                verbose=2)\n",
    "    \n",
    "    #Fine tuning\n",
    "    model = load_model(\"intermediate_model.hdf5\")\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    model.compile(optimizer=SGD(learning_rate=0.0001, momentum=0.9), loss='mean_squared_error', metrics=['accuracy', 'mse'])\n",
    "    \n",
    "    batch_size = 30 #reduce the batch size so that the gradients of all layers can fit in memory\n",
    "    \n",
    "    checkpoint2 = ModelCheckpoint('ensemble_{}.hdf5'.format(e), save_best_only=True)\n",
    "    \n",
    "    hist2 = model.fit(datagen.flow(X_train, Y_train, batch_size), \n",
    "                                steps_per_epoch=len(X_train) / batch_size,\n",
    "                                epochs=nEpochs,\n",
    "                                validation_data=(X_validate, Y_validate),\n",
    "                                callbacks=[checkpoint2],\n",
    "                                verbose=2)\n",
    "    \n",
    "    K.clear_session() #Clear tensorflow session to prevent memory issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc080f-6539-4728-b77f-aee7a939f271",
   "metadata": {},
   "source": [
    "## Load checkpoints and get predictions for validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ef99113-b833-466f-9164-1c7f45fd2f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 7s 2s/step\n",
      "3/3 [==============================] - 6s 2s/step\n",
      "4/4 [==============================] - 8s 2s/step\n",
      "3/3 [==============================] - 7s 2s/step\n",
      "3/3 [==============================] - 6s 2s/step\n",
      "4/4 [==============================] - 8s 2s/step\n"
     ]
    }
   ],
   "source": [
    "checkpoints_dir = \"\"\n",
    "\n",
    "validate_pred = np.zeros((nEnsemble, nTest, nDim))\n",
    "test_pred = np.zeros((nEnsemble, nTest, nDim))\n",
    "rocks_120_pred = np.zeros((nEnsemble, 120, nDim))\n",
    "\n",
    "for e in range(nEnsemble):\n",
    "    model = load_model(checkpoints_dir + \"ensemble_{}.hdf5\".format(e))\n",
    "    validate_pred[e,:] = model.predict(X_validate)\n",
    "    test_pred[e,:] = model.predict(X_test)\n",
    "    rocks_120_pred[e,:] = model.predict(X_120)\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "validate_prediction = np.mean(validate_pred, 0)\n",
    "test_prediction = np.mean(test_pred, 0)\n",
    "rocks_120_prediction = np.mean(rocks_120_pred, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c4bd5-1bc7-427f-9712-d68e51cd876b",
   "metadata": {},
   "source": [
    "## Get MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62e0cba5-aaf8-4f28-b742-b69c6fb18bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.453259140003523\n",
      "7.445503120258865\n",
      "3.2852470634053645\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y_validate, validate_prediction))\n",
    "print(mean_squared_error(Y_test, test_prediction))\n",
    "print(mean_squared_error(Y_120, rocks_120_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823a224-d473-404b-9387-c6310cef3ecf",
   "metadata": {},
   "source": [
    "## Get R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7825d48-9155-4d32-b899-45b8f124eb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.13048638343225943\n",
      "-0.13639085863413697\n",
      "-0.5289698101323548\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(Y_validate, validate_prediction))\n",
    "print(r2_score(Y_test, test_prediction))\n",
    "print(r2_score(Y_120, rocks_120_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48874f-92a4-44f6-b563-6c754c3e6682",
   "metadata": {},
   "source": [
    "## Save predictions to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f64fb4a-19dc-4feb-aa3d-5017877c0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pred_file = \"CNN Predictions/MDS Dimensions/cnn_own_predicted_mds_120_laptop.txt\"\n",
    "\n",
    "np.savetxt(cnn_pred_file, rocks_120_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

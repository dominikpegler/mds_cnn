{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac78307-c58c-4f51-9394-72a35b8dc8e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Notebook equivalent of `fit_ensemble.py`\n",
    "---\n",
    "\n",
    "* This program trains the ensemble of CNN models reported in https://link.springer.com/article/10.1007/s42113-020-00073-z (https://osf.io/efjmq)\n",
    "    * It trains a model/ensemble on a 180 images training set of a 360 images dataset\n",
    "    * Makes predictions on\n",
    "        1. the 90 images validation set (part of the same 360 images set)\n",
    "        2. the 90 images test set (part of the same 360 images set)\n",
    "        3. the 120 images set (a different set)\n",
    "* Data required\n",
    "    * File `mds_360.txt` with labels (in `../sanders_2018`)\n",
    "    * Directory `360 Rocks/` with `*.jpg` images (in `../sanders_2018`)\n",
    "    * File `mds_120.txt` with labels (in `../sanders_2018`)\n",
    "    * Directory `120 Rocks/` with `*.jpg` images  (in `../sanders_2018`)\n",
    "* other available data\n",
    "    * Directory `120 Rock Images/` with 120 `*.png` images\n",
    "    * Directory `Similarity Judgements Data/` with similarity labels for the \"120 Rocks\" set as individual textfiles for each of the 85 participants: `rocks_similarity_120_*.txt`\n",
    "    * Directory `Categorization Data/` with category labels (1 = Igneous, 2 = Metamorphic, 4 = Mixed) for the \"120 Rocks\" set as individual textfiles for each of the 85 participants: `rocks_similarity_120_*_*.txt`\n",
    "    * File `MDS/mds_120_supplemental_dims.txt`\n",
    "    \n",
    "    \n",
    "#### **Update 2022/05/31: Additional necessary data added to `../sanders_2018` from here: https://osf.io/d6b9y/**\n",
    "\n",
    "   * Rocks dataset was created in 2017 here: https://link.springer.com/article/10.3758/s13428-017-0884-8 (https://osf.io/w64fv)\n",
    "   * Further work in Sanders' 2018 doctoral thesis https://scholarworks.iu.edu/dspace/handle/2022/22415 (https://osf.io/d6b9y)\n",
    "        * includes the relevant additional data such as the 360 rocks images set\n",
    "        * includes the same script `fit_ensemble.py` (identical version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd9918b7-6593-4b18-a223-605d525501ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "nPixels = 224\n",
    "\n",
    "nTest = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575f4ca-9a76-4c7f-9528-987a98d634f8",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3398881-bcc4-46b5-8699-6cb1f478826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [i for i in range(30) for j in range(12)] # creates 360 list items like so: [0, 0, 0, 0, ... 29, 29, 29, 29]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc824d1-e3b2-4fe8-9b31-507fd2c326e5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b97c899-da5c-4399-ac69-362eae2c303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(directory, nPixels, preprocesser):\n",
    "    \"\"\"\n",
    "    Creates array-like data from a directory with image files for usage with Keras.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                img = load_img(os.path.join(subdir, file), target_size=(nPixels, nPixels))\n",
    "                x = img_to_array(img)\n",
    "                X.append(x)\n",
    "    X = np.stack(X)\n",
    "    X = preprocesser(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c0755-2bd7-485a-a1da-886e58709894",
   "metadata": {},
   "source": [
    "## Prepare 360 Rocks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84a68534-f8b7-418d-ae23-e89a00aec802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image files\n",
    "X = load_images(\"../sanders_2018/360 Rocks\", nPixels, lambda x: resnet50.preprocess_input(np.expand_dims(x, axis=0)).squeeze())\n",
    "\n",
    "# load labels\n",
    "mds_360 = np.loadtxt(\"../sanders_2018/mds_360.txt\")\n",
    "\n",
    "# split data: train vs test\n",
    "(X_train_, X_test, \n",
    " Y_train_, Y_test, \n",
    " categories_train_, categories_test) = train_test_split(X, \n",
    "                                                        mds_360, \n",
    "                                                        categories,\n",
    "                                                        test_size=nTest,\n",
    "                                                        stratify=categories, \n",
    "                                                        random_state=0)\n",
    "\n",
    "# split train set again: train vs validate\n",
    "(X_train, X_validate, \n",
    " Y_train, Y_validate) = train_test_split(X_train_, \n",
    "                                         Y_train_, \n",
    "                                         test_size=nTest,\n",
    "                                         stratify=categories_train_, \n",
    "                                         random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f8eef-b65e-48c5-b32a-80f75ae68a9b",
   "metadata": {},
   "source": [
    "## Prepare 120 Rocks data\n",
    "\n",
    "no train, test, validate splits ...will be later used only for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e06f74f0-00e5-4cc4-a8b7-4ee40425ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image files\n",
    "X_120 = load_images(\"../sanders_2018/120 Rocks\", nPixels, lambda x: resnet50.preprocess_input(np.expand_dims(x, axis=0)).squeeze())\n",
    "\n",
    "# load labels\n",
    "Y_120 = np.loadtxt(\"../sanders_2018/mds_120.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b652ff77-60a6-431b-a06d-cf4b09159d2e",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57c64f3a-1982-4868-98ad-382aa70b486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "                    samplewise_center=False,\n",
    "                    featurewise_std_normalization=False,\n",
    "                    samplewise_std_normalization=False,\n",
    "                    zca_whitening=False,\n",
    "                    rotation_range=20,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    shear_range=0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    channel_shift_range=0.,\n",
    "                    fill_mode='nearest',\n",
    "                    cval=0.,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True)\n",
    "\n",
    "nEpochs = 500\n",
    "dropout = 0.5\n",
    "nEnsemble = 10\n",
    "          \n",
    "nDense = 256\n",
    "nLayers = 2\n",
    "loglr = -2.2200654426745987\n",
    "\n",
    "lr = 10 ** loglr\n",
    "nDim = 8\n",
    "batch_size = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d4c08-1998-45ad-9654-17dc5317a921",
   "metadata": {},
   "source": [
    "## Train models and save checkpoints\n",
    "\n",
    "Training performance seems to depend on hardware ... e.g. poor results on laptop, good results on desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108761a5-9d10-42e7-8ec5-4ea16e3041bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in range(nEnsemble):\n",
    "    #Build model\n",
    "    arch = resnet50.ResNet50(include_top=False, pooling='avg')\n",
    "    for layer in arch.layers:\n",
    "        layer.trainable = False    \n",
    "    \n",
    "    x = arch.output\n",
    "    x = Dropout(dropout)(x)\n",
    "    for lyr in range(nLayers):\n",
    "        x = Dense(nDense, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "    x = Dense(nDim)(x)\n",
    "    \n",
    "    model = Model(inputs=arch.input, outputs=x)\n",
    "    \n",
    "    #Initial training\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=lr), metrics=['accuracy'])\n",
    "    \n",
    "    checkpoint1 = ModelCheckpoint('intermediate_model.hdf5', save_best_only=True)\n",
    "\n",
    "    hist1 = model.fit(datagen.flow(X_train, Y_train, batch_size), \n",
    "                                steps_per_epoch=len(X_train) / batch_size,\n",
    "                                epochs=nEpochs,\n",
    "                                validation_data=(X_validate, Y_validate),\n",
    "                                callbacks=[checkpoint1],\n",
    "                                verbose=2)\n",
    "    \n",
    "    #Fine tuning\n",
    "    model = load_model(\"intermediate_model.hdf5\")\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    model.compile(optimizer=SGD(learning_rate=0.0001, momentum=0.9), loss='mean_squared_error', metrics=['accuracy'])\n",
    "    \n",
    "    batch_size = 30 #reduce the batch size so that the gradients of all layers can fit in memory\n",
    "    \n",
    "    checkpoint2 = ModelCheckpoint('ensemble_{}.hdf5'.format(e), save_best_only=True)\n",
    "    \n",
    "    hist2 = model.fit(datagen.flow(X_train, Y_train, batch_size), \n",
    "                                steps_per_epoch=len(X_train) / batch_size,\n",
    "                                epochs=nEpochs,\n",
    "                                validation_data=(X_validate, Y_validate),\n",
    "                                callbacks=[checkpoint2],\n",
    "                                verbose=2)\n",
    "    \n",
    "    K.clear_session() #Clear tensorflow session to prevent memory issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc080f-6539-4728-b77f-aee7a939f271",
   "metadata": {},
   "source": [
    "## Load checkpoints and get predictions for validation and test sets\n",
    "\n",
    "... before make sure that the checkpoint files are moved to the checkpoints directoy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ef99113-b833-466f-9164-1c7f45fd2f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 4s 1s/step\n",
      "4/4 [==============================] - 6s 1s/step\n",
      "3/3 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 4s 1s/step\n",
      "4/4 [==============================] - 6s 1s/step\n",
      "3/3 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 4s 1s/step\n",
      "4/4 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 4s 1s/step\n",
      "4/4 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 4s 1s/step\n",
      "4/4 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 4s 1s/step\n",
      "4/4 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 4s 1s/step\n",
      "4/4 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 4s 1s/step\n",
      "4/4 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 4s 1s/step\n",
      "4/4 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 4s 1s/step\n",
      "4/4 [==============================] - 5s 1s/step\n"
     ]
    }
   ],
   "source": [
    "checkpoints_dir = \"CNN_checkpoints/\"\n",
    "\n",
    "validate_pred = np.zeros((nEnsemble, nTest, nDim))\n",
    "test_pred = np.zeros((nEnsemble, nTest, nDim))\n",
    "rocks_120_pred = np.zeros((nEnsemble, 120, nDim))\n",
    "\n",
    "for e in range(nEnsemble):\n",
    "    model = load_model(checkpoints_dir + \"ensemble_{}.hdf5\".format(e))\n",
    "    validate_pred[e,:] = model.predict(X_validate)\n",
    "    test_pred[e,:] = model.predict(X_test)\n",
    "    rocks_120_pred[e,:] = model.predict(X_120)\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "validate_prediction = np.mean(validate_pred, 0)\n",
    "test_prediction = np.mean(test_pred, 0)\n",
    "rocks_120_prediction = np.mean(rocks_120_pred, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c4bd5-1bc7-427f-9712-d68e51cd876b",
   "metadata": {},
   "source": [
    "## Get MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "62e0cba5-aaf8-4f28-b742-b69c6fb18bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3980989969178332\n",
      "1.516632216942619\n",
      "2.8975563908325688\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y_validate, validate_prediction))\n",
    "print(mean_squared_error(Y_test, test_prediction))\n",
    "print(mean_squared_error(Y_120, rocks_120_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823a224-d473-404b-9387-c6310cef3ecf",
   "metadata": {},
   "source": [
    "## Get R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7825d48-9155-4d32-b899-45b8f124eb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7593491655793974\n",
      "0.7379470763395782\n",
      "-0.34092653932498296\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(Y_validate, validate_prediction))\n",
    "print(r2_score(Y_test, test_prediction))\n",
    "print(r2_score(Y_120, rocks_120_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48874f-92a4-44f6-b563-6c754c3e6682",
   "metadata": {},
   "source": [
    "## Save predictions to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f64fb4a-19dc-4feb-aa3d-5017877c0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pred_file = \"CNN Predictions/MDS Dimensions/cnn_keras_predicted_mds_120.txt\"\n",
    "\n",
    "np.savetxt(cnn_pred_file, rocks_120_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

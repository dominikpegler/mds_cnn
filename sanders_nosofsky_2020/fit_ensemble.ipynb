{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ac78307-c58c-4f51-9394-72a35b8dc8e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "Notebook equivalent of `fit_ensemble.py`\n",
    "---\n",
    "\n",
    "* This program trains the ensemble of CNN models reported in https://link.springer.com/article/10.1007/s42113-020-00073-z (https://osf.io/efjmq)\n",
    "    * It trains a model/ensemble on a 180 images training set of a 360 images dataset\n",
    "    * Makes predictions on\n",
    "        1. the 90 images validation set (part of the same 360 images set)\n",
    "        2. the 90 images test set (part of the same 360 images set)\n",
    "        3. the 120 images set (a different set)\n",
    "* Data required\n",
    "    * File `mds_360.txt` with labels (in `../sanders_2018`)\n",
    "    * Directory `360 Rocks/` with `*.jpg` images (in `../sanders_2018`)\n",
    "    * File `mds_120.txt` with labels (in `../sanders_2018`)\n",
    "    * Directory `120 Rocks/` with `*.jpg` images  (in `../sanders_2018`)\n",
    "* other available data\n",
    "    * Directory `120 Rock Images/` with 120 `*.png` images\n",
    "    * Directory `Similarity Judgements Data/` with similarity labels for the \"120 Rocks\" set as individual textfiles for each of the 85 participants: `rocks_similarity_120_*.txt`\n",
    "    * Directory `Categorization Data/` with category labels (1 = Igneous, 2 = Metamorphic, 4 = Mixed) for the \"120 Rocks\" set as individual textfiles for each of the 85 participants: `rocks_similarity_120_*_*.txt`\n",
    "    * File `MDS/mds_120_supplemental_dims.txt`\n",
    "    \n",
    "    \n",
    "#### **Update 2022/05/31: Additional necessary data added to `../sanders_2018` from here: https://osf.io/d6b9y/**\n",
    "\n",
    "   * Rocks dataset was created in 2017 here: https://link.springer.com/article/10.3758/s13428-017-0884-8 (https://osf.io/w64fv)\n",
    "   * Further work in Sanders' 2018 doctoral thesis https://scholarworks.iu.edu/dspace/handle/2022/22415 (https://osf.io/d6b9y)\n",
    "        * includes the relevant additional data such as the 360 rocks images set\n",
    "        * includes the same script `fit_ensemble.py` (identical version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fd9918b7-6593-4b18-a223-605d525501ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.applications import resnet50\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "nPixels = 224\n",
    "\n",
    "nTest = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c575f4ca-9a76-4c7f-9528-987a98d634f8",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a3398881-bcc4-46b5-8699-6cb1f478826b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [i for i in range(30) for j in range(12)] # creates 360 list items like so: [0, 0, 0, 0, ... 29, 29, 29, 29]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc824d1-e3b2-4fe8-9b31-507fd2c326e5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0b97c899-da5c-4399-ac69-362eae2c303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(directory, nPixels, preprocesser):\n",
    "    \"\"\"\n",
    "    Creates array-like data from a directory with image files for usage with Keras.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = []\n",
    "    for subdir, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(\".jpg\"):\n",
    "                img = load_img(os.path.join(subdir, file), target_size=(nPixels, nPixels))\n",
    "                x = img_to_array(img)\n",
    "                X.append(x)\n",
    "    X = np.stack(X)\n",
    "    X = preprocesser(X)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442c0755-2bd7-485a-a1da-886e58709894",
   "metadata": {},
   "source": [
    "## Prepare 360 Rocks data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "84a68534-f8b7-418d-ae23-e89a00aec802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image files\n",
    "X = load_images(\"../sanders_2018/360 Rocks\", nPixels, lambda x: resnet50.preprocess_input(np.expand_dims(x, axis=0)).squeeze())\n",
    "\n",
    "# load labels\n",
    "mds_360 = np.loadtxt(\"../sanders_2018/mds_360.txt\") # missing\n",
    "\n",
    "# split data: train vs test\n",
    "(X_train_, X_test, \n",
    " Y_train_, Y_test, \n",
    " categories_train_, categories_test) = train_test_split(X, \n",
    "                                                        mds_360, \n",
    "                                                        categories,\n",
    "                                                        test_size=nTest,\n",
    "                                                        stratify=categories, \n",
    "                                                        random_state=0)\n",
    "\n",
    "# split train set again: train vs validate\n",
    "(X_train, X_validate, \n",
    " Y_train, Y_validate) = train_test_split(X_train_, \n",
    "                                         Y_train_, \n",
    "                                         test_size=nTest,\n",
    "                                         stratify=categories_train_, \n",
    "                                         random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3f8eef-b65e-48c5-b32a-80f75ae68a9b",
   "metadata": {},
   "source": [
    "## Prepare 120 Rocks data\n",
    "\n",
    "no train, test, validate splits ...will be later used only for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e06f74f0-00e5-4cc4-a8b7-4ee40425ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load image files\n",
    "X_120 = load_images(\"../sanders_2018/120 Rocks\", nPixels, lambda x: resnet50.preprocess_input(np.expand_dims(x, axis=0)).squeeze())\n",
    "\n",
    "# load labels\n",
    "Y_120 = np.loadtxt(\"../sanders_2018/mds_120.txt\") # missing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b652ff77-60a6-431b-a06d-cf4b09159d2e",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "57c64f3a-1982-4868-98ad-382aa70b486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(featurewise_center=False,\n",
    "                    samplewise_center=False,\n",
    "                    featurewise_std_normalization=False,\n",
    "                    samplewise_std_normalization=False,\n",
    "                    zca_whitening=False,\n",
    "                    rotation_range=20,\n",
    "                    width_shift_range=0.2,\n",
    "                    height_shift_range=0.2,\n",
    "                    shear_range=0.2,\n",
    "                    zoom_range=0.2,\n",
    "                    channel_shift_range=0.,\n",
    "                    fill_mode='nearest',\n",
    "                    cval=0.,\n",
    "                    horizontal_flip=True,\n",
    "                    vertical_flip=True)\n",
    "\n",
    "nEpochs = 10\n",
    "dropout = 0.5\n",
    "nEnsemble = 2\n",
    "          \n",
    "nDense = 256\n",
    "nLayers = 2\n",
    "loglr = -2.2200654426745987\n",
    "\n",
    "lr = 10 ** loglr\n",
    "nDim = 8\n",
    "batch_size = 90"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58d4c08-1998-45ad-9654-17dc5317a921",
   "metadata": {},
   "source": [
    "## Train models and save checkpoints\n",
    "\n",
    "Training performance seems to depend on hardware ... e.g. poor results on laptop, good results on desktop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "108761a5-9d10-42e7-8ec5-4ea16e3041bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "2/2 - 15s - loss: 10.2266 - accuracy: 0.1444 - mse: 10.2266 - val_loss: 6.7960 - val_accuracy: 0.2778 - val_mse: 6.7960 - 15s/epoch - 8s/step\n",
      "Epoch 2/10\n",
      "2/2 - 12s - loss: 7.2737 - accuracy: 0.2778 - mse: 7.2737 - val_loss: 10.8290 - val_accuracy: 0.2889 - val_mse: 10.8290 - 12s/epoch - 6s/step\n",
      "Epoch 3/10\n",
      "2/2 - 12s - loss: 6.3121 - accuracy: 0.3389 - mse: 6.3121 - val_loss: 17.2369 - val_accuracy: 0.3111 - val_mse: 17.2369 - 12s/epoch - 6s/step\n",
      "Epoch 4/10\n",
      "2/2 - 12s - loss: 5.6272 - accuracy: 0.4111 - mse: 5.6272 - val_loss: 21.7199 - val_accuracy: 0.3000 - val_mse: 21.7199 - 12s/epoch - 6s/step\n",
      "Epoch 5/10\n",
      "2/2 - 12s - loss: 5.5343 - accuracy: 0.3889 - mse: 5.5343 - val_loss: 19.5626 - val_accuracy: 0.3111 - val_mse: 19.5626 - 12s/epoch - 6s/step\n",
      "Epoch 6/10\n",
      "2/2 - 12s - loss: 4.6848 - accuracy: 0.4278 - mse: 4.6848 - val_loss: 16.3416 - val_accuracy: 0.3222 - val_mse: 16.3416 - 12s/epoch - 6s/step\n",
      "Epoch 7/10\n",
      "2/2 - 12s - loss: 4.3022 - accuracy: 0.3889 - mse: 4.3022 - val_loss: 13.4707 - val_accuracy: 0.3778 - val_mse: 13.4707 - 12s/epoch - 6s/step\n",
      "Epoch 8/10\n",
      "2/2 - 12s - loss: 4.3111 - accuracy: 0.4722 - mse: 4.3111 - val_loss: 11.5728 - val_accuracy: 0.3556 - val_mse: 11.5728 - 12s/epoch - 6s/step\n",
      "Epoch 9/10\n",
      "2/2 - 12s - loss: 3.9704 - accuracy: 0.4944 - mse: 3.9704 - val_loss: 10.3830 - val_accuracy: 0.4111 - val_mse: 10.3830 - 12s/epoch - 6s/step\n",
      "Epoch 10/10\n",
      "2/2 - 12s - loss: 3.9016 - accuracy: 0.4778 - mse: 3.9016 - val_loss: 9.7360 - val_accuracy: 0.4333 - val_mse: 9.7360 - 12s/epoch - 6s/step\n",
      "Epoch 1/10\n",
      "6/6 - 47s - loss: 8.2231 - accuracy: 0.2500 - mse: 8.2231 - val_loss: 6.6981 - val_accuracy: 0.2778 - val_mse: 6.6981 - 47s/epoch - 8s/step\n",
      "Epoch 2/10\n",
      "6/6 - 40s - loss: 7.9506 - accuracy: 0.2333 - mse: 7.9506 - val_loss: 7.6557 - val_accuracy: 0.2667 - val_mse: 7.6557 - 40s/epoch - 7s/step\n",
      "Epoch 3/10\n",
      "6/6 - 40s - loss: 7.8942 - accuracy: 0.2556 - mse: 7.8942 - val_loss: 8.6239 - val_accuracy: 0.2889 - val_mse: 8.6239 - 40s/epoch - 7s/step\n",
      "Epoch 4/10\n",
      "6/6 - 41s - loss: 7.4304 - accuracy: 0.2444 - mse: 7.4304 - val_loss: 9.1225 - val_accuracy: 0.3000 - val_mse: 9.1225 - 41s/epoch - 7s/step\n",
      "Epoch 5/10\n",
      "6/6 - 40s - loss: 7.4733 - accuracy: 0.2333 - mse: 7.4733 - val_loss: 10.1425 - val_accuracy: 0.3000 - val_mse: 10.1425 - 40s/epoch - 7s/step\n",
      "Epoch 6/10\n",
      "6/6 - 40s - loss: 8.0263 - accuracy: 0.2444 - mse: 8.0263 - val_loss: 10.1822 - val_accuracy: 0.2889 - val_mse: 10.1822 - 40s/epoch - 7s/step\n",
      "Epoch 7/10\n",
      "6/6 - 40s - loss: 7.4931 - accuracy: 0.2722 - mse: 7.4931 - val_loss: 8.3816 - val_accuracy: 0.3333 - val_mse: 8.3816 - 40s/epoch - 7s/step\n",
      "Epoch 8/10\n",
      "6/6 - 40s - loss: 7.1441 - accuracy: 0.2611 - mse: 7.1441 - val_loss: 7.4331 - val_accuracy: 0.3222 - val_mse: 7.4331 - 40s/epoch - 7s/step\n",
      "Epoch 9/10\n",
      "6/6 - 40s - loss: 7.0179 - accuracy: 0.2722 - mse: 7.0179 - val_loss: 7.3055 - val_accuracy: 0.3444 - val_mse: 7.3055 - 40s/epoch - 7s/step\n",
      "Epoch 10/10\n",
      "6/6 - 40s - loss: 6.5635 - accuracy: 0.2944 - mse: 6.5635 - val_loss: 6.8206 - val_accuracy: 0.3444 - val_mse: 6.8206 - 40s/epoch - 7s/step\n",
      "Epoch 1/10\n",
      "6/6 - 16s - loss: 8.7363 - accuracy: 0.1944 - mse: 8.7363 - val_loss: 24.6267 - val_accuracy: 0.1444 - val_mse: 24.6267 - 16s/epoch - 3s/step\n",
      "Epoch 2/10\n",
      "6/6 - 12s - loss: 6.7547 - accuracy: 0.3500 - mse: 6.7547 - val_loss: 48.4015 - val_accuracy: 0.2000 - val_mse: 48.4015 - 12s/epoch - 2s/step\n",
      "Epoch 3/10\n",
      "6/6 - 12s - loss: 5.6846 - accuracy: 0.4056 - mse: 5.6846 - val_loss: 40.6244 - val_accuracy: 0.2333 - val_mse: 40.6244 - 12s/epoch - 2s/step\n",
      "Epoch 4/10\n",
      "6/6 - 12s - loss: 5.5715 - accuracy: 0.4167 - mse: 5.5715 - val_loss: 25.1866 - val_accuracy: 0.3222 - val_mse: 25.1866 - 12s/epoch - 2s/step\n",
      "Epoch 5/10\n",
      "6/6 - 13s - loss: 4.5305 - accuracy: 0.4333 - mse: 4.5305 - val_loss: 17.8547 - val_accuracy: 0.3667 - val_mse: 17.8547 - 13s/epoch - 2s/step\n",
      "Epoch 6/10\n",
      "6/6 - 13s - loss: 3.8673 - accuracy: 0.4889 - mse: 3.8673 - val_loss: 17.4670 - val_accuracy: 0.3778 - val_mse: 17.4670 - 13s/epoch - 2s/step\n",
      "Epoch 7/10\n",
      "6/6 - 12s - loss: 3.7654 - accuracy: 0.4111 - mse: 3.7654 - val_loss: 14.1672 - val_accuracy: 0.4444 - val_mse: 14.1672 - 12s/epoch - 2s/step\n",
      "Epoch 8/10\n",
      "6/6 - 12s - loss: 3.6051 - accuracy: 0.5278 - mse: 3.6051 - val_loss: 8.1024 - val_accuracy: 0.5333 - val_mse: 8.1024 - 12s/epoch - 2s/step\n",
      "Epoch 9/10\n",
      "6/6 - 12s - loss: 3.3625 - accuracy: 0.4500 - mse: 3.3625 - val_loss: 5.5812 - val_accuracy: 0.5556 - val_mse: 5.5812 - 12s/epoch - 2s/step\n",
      "Epoch 10/10\n",
      "6/6 - 12s - loss: 3.0226 - accuracy: 0.5778 - mse: 3.0226 - val_loss: 5.0415 - val_accuracy: 0.5111 - val_mse: 5.0415 - 12s/epoch - 2s/step\n",
      "Epoch 1/10\n",
      "6/6 - 46s - loss: 3.6791 - accuracy: 0.4167 - mse: 3.6791 - val_loss: 4.5291 - val_accuracy: 0.5333 - val_mse: 4.5291 - 46s/epoch - 8s/step\n",
      "Epoch 2/10\n",
      "6/6 - 41s - loss: 3.7557 - accuracy: 0.4833 - mse: 3.7557 - val_loss: 4.2785 - val_accuracy: 0.5556 - val_mse: 4.2785 - 41s/epoch - 7s/step\n",
      "Epoch 3/10\n",
      "6/6 - 41s - loss: 3.5445 - accuracy: 0.4611 - mse: 3.5445 - val_loss: 4.2577 - val_accuracy: 0.5778 - val_mse: 4.2577 - 41s/epoch - 7s/step\n",
      "Epoch 4/10\n",
      "6/6 - 41s - loss: 3.1940 - accuracy: 0.5278 - mse: 3.1940 - val_loss: 4.3521 - val_accuracy: 0.5778 - val_mse: 4.3521 - 41s/epoch - 7s/step\n",
      "Epoch 5/10\n",
      "6/6 - 42s - loss: 3.2322 - accuracy: 0.4944 - mse: 3.2322 - val_loss: 4.1755 - val_accuracy: 0.5667 - val_mse: 4.1755 - 42s/epoch - 7s/step\n",
      "Epoch 6/10\n",
      "6/6 - 41s - loss: 3.2368 - accuracy: 0.4778 - mse: 3.2368 - val_loss: 4.0889 - val_accuracy: 0.5556 - val_mse: 4.0889 - 41s/epoch - 7s/step\n",
      "Epoch 7/10\n",
      "6/6 - 41s - loss: 3.0180 - accuracy: 0.5722 - mse: 3.0180 - val_loss: 3.9200 - val_accuracy: 0.5556 - val_mse: 3.9200 - 41s/epoch - 7s/step\n",
      "Epoch 8/10\n",
      "6/6 - 41s - loss: 3.0207 - accuracy: 0.5333 - mse: 3.0207 - val_loss: 3.7187 - val_accuracy: 0.5667 - val_mse: 3.7187 - 41s/epoch - 7s/step\n",
      "Epoch 9/10\n",
      "6/6 - 41s - loss: 2.9856 - accuracy: 0.5500 - mse: 2.9856 - val_loss: 3.5878 - val_accuracy: 0.5889 - val_mse: 3.5878 - 41s/epoch - 7s/step\n",
      "Epoch 10/10\n",
      "6/6 - 41s - loss: 2.9381 - accuracy: 0.5667 - mse: 2.9381 - val_loss: 3.4339 - val_accuracy: 0.5778 - val_mse: 3.4339 - 41s/epoch - 7s/step\n"
     ]
    }
   ],
   "source": [
    "for e in range(nEnsemble):\n",
    "    #Build model\n",
    "    arch = resnet50.ResNet50(include_top=False, pooling='avg')\n",
    "    for layer in arch.layers:\n",
    "        layer.trainable = False    \n",
    "    \n",
    "    x = arch.output\n",
    "    x = Dropout(dropout)(x)\n",
    "    for lyr in range(nLayers):\n",
    "        x = Dense(nDense, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(dropout)(x)\n",
    "    x = Dense(nDim)(x)\n",
    "    \n",
    "    model = Model(inputs=arch.input, outputs=x)\n",
    "    \n",
    "    #Initial training\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=lr), metrics=['accuracy', 'mse'])\n",
    "    \n",
    "    checkpoint1 = ModelCheckpoint('intermediate_model.hdf5', save_best_only=True)\n",
    "\n",
    "    hist1 = model.fit(datagen.flow(X_train, Y_train, batch_size), \n",
    "                                steps_per_epoch=len(X_train) / batch_size,\n",
    "                                epochs=nEpochs,\n",
    "                                validation_data=(X_validate, Y_validate),\n",
    "                                callbacks=[checkpoint1],\n",
    "                                verbose=2)\n",
    "    \n",
    "    #Fine tuning\n",
    "    model = load_model(\"intermediate_model.hdf5\")\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        layer.trainable = True\n",
    "    \n",
    "    model.compile(optimizer=SGD(learning_rate=0.0001, momentum=0.9), loss='mean_squared_error', metrics=['accuracy', 'mse'])\n",
    "    \n",
    "    batch_size = 30 #reduce the batch size so that the gradients of all layers can fit in memory\n",
    "    \n",
    "    checkpoint2 = ModelCheckpoint('ensemble_{}.hdf5'.format(e), save_best_only=True)\n",
    "    \n",
    "    hist2 = model.fit(datagen.flow(X_train, Y_train, batch_size), \n",
    "                                steps_per_epoch=len(X_train) / batch_size,\n",
    "                                epochs=nEpochs,\n",
    "                                validation_data=(X_validate, Y_validate),\n",
    "                                callbacks=[checkpoint2],\n",
    "                                verbose=2)\n",
    "    \n",
    "    K.clear_session() #Clear tensorflow session to prevent memory issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88cc080f-6539-4728-b77f-aee7a939f271",
   "metadata": {},
   "source": [
    "## Load checkpoints and get predictions for validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "1ef99113-b833-466f-9164-1c7f45fd2f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 4s 1s/step\n",
      "3/3 [==============================] - 4s 1s/step\n",
      "4/4 [==============================] - 5s 1s/step\n",
      "3/3 [==============================] - 4s 1s/step\n",
      "3/3 [==============================] - 4s 1s/step\n",
      "4/4 [==============================] - 5s 1s/step\n"
     ]
    }
   ],
   "source": [
    "checkpoints_dir = \"\"\n",
    "\n",
    "validate_pred = np.zeros((nEnsemble, nTest, nDim))\n",
    "test_pred = np.zeros((nEnsemble, nTest, nDim))\n",
    "rocks_120_pred = np.zeros((nEnsemble, 120, nDim))\n",
    "\n",
    "for e in range(nEnsemble):\n",
    "    model = load_model(checkpoints_dir + \"ensemble_{}.hdf5\".format(e))\n",
    "    validate_pred[e,:] = model.predict(X_validate)\n",
    "    test_pred[e,:] = model.predict(X_test)\n",
    "    rocks_120_pred[e,:] = model.predict(X_120)\n",
    "    \n",
    "    K.clear_session()\n",
    "\n",
    "validate_prediction = np.mean(validate_pred, 0)\n",
    "test_prediction = np.mean(test_pred, 0)\n",
    "rocks_120_prediction = np.mean(rocks_120_pred, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884c4bd5-1bc7-427f-9712-d68e51cd876b",
   "metadata": {},
   "source": [
    "## Get MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "62e0cba5-aaf8-4f28-b742-b69c6fb18bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5867323262749493\n",
      "3.5055726397772804\n",
      "2.8807225718616616\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(Y_validate, validate_prediction))\n",
    "print(mean_squared_error(Y_test, test_prediction))\n",
    "print(mean_squared_error(Y_120, rocks_120_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0823a224-d473-404b-9387-c6310cef3ecf",
   "metadata": {},
   "source": [
    "## Get R²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a7825d48-9155-4d32-b899-45b8f124eb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43475707357859045\n",
      "0.4353891119304781\n",
      "-0.3620552083355105\n"
     ]
    }
   ],
   "source": [
    "print(r2_score(Y_validate, validate_prediction))\n",
    "print(r2_score(Y_test, test_prediction))\n",
    "print(r2_score(Y_120, rocks_120_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b48874f-92a4-44f6-b563-6c754c3e6682",
   "metadata": {},
   "source": [
    "## Save predictions to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "9f64fb4a-19dc-4feb-aa3d-5017877c0d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_pred_file = \"CNN Predictions/MDS Dimensions/cnn_own_predicted_mds_120_B.txt\"\n",
    "\n",
    "np.savetxt(cnn_pred_file, rocks_120_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
